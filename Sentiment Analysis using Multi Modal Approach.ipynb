{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f69464",
   "metadata": {
    "id": "72f69464"
   },
   "source": [
    "# 1st Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894b0ee",
   "metadata": {
    "id": "9894b0ee"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d61ee2",
   "metadata": {
    "id": "11d61ee2"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01ad63",
   "metadata": {
    "collapsed": true,
    "id": "7d01ad63"
   },
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887e3b7",
   "metadata": {
    "collapsed": true,
    "id": "0887e3b7"
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef4a69",
   "metadata": {
    "collapsed": true,
    "id": "24ef4a69"
   },
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7cdff",
   "metadata": {
    "collapsed": true,
    "id": "e3f7cdff"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "NrHpBDUjMhIA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NrHpBDUjMhIA",
    "outputId": "5ce26957-563d-4dd8-a3cd-34d9b2e5d56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "i1upb_SqMh26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "i1upb_SqMh26",
    "outputId": "31af35bc-a1ee-4887-914e-1ab5e52bb0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.29.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eHK_xfGMiEL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eHK_xfGMiEL",
    "outputId": "3fe4b838-6bbc-42d8-ac31-5eb2bbfbdf84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NQ170kAQhCCg",
   "metadata": {
    "id": "NQ170kAQhCCg"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y_gg5yyVg6GM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_gg5yyVg6GM",
    "outputId": "0c683495-ddd2-4bd5-892a-57fb72e9d4df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rp5QCbghg6WC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp5QCbghg6WC",
    "outputId": "546dd5a5-bd8b-4bd7-d64d-ffb5926fd7fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a97b4ef",
   "metadata": {
    "id": "1a97b4ef"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import torchvision.models as m\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "txtModel = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "woM5XcHIv9Tn",
   "metadata": {
    "id": "woM5XcHIv9Tn"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10400a81",
   "metadata": {
    "id": "10400a81"
   },
   "source": [
    "# Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc4de0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "a3bc4de0",
    "outputId": "43c51d65-cb85-4fb4-d8d0-8f98e891c42d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19802\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-56830fbe-2fe2-4c43-83b6-9ad71e538df9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>raw</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>a plate of delici food includ french fri</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>french fri are not a healthi food but it is an...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>the plate ha one of my favorit food on it fren...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>it wa disgust food not just bad food</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COCO_val2014_000000389081.jpg</td>\n",
       "      <td>a plate of disgust food found at a diner</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39194</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>a dirti bathroom that ha a dirti window made o...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39195</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>a dirti bathroom that ha a window in it</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39196</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>a towel that is on a rack in a dirti bathroom</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39197</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>a dirti bathroom that ha a dirti window made o...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39198</th>\n",
       "      <td>COCO_val2014_000000190705.jpg</td>\n",
       "      <td>a dirti bathroom that ha a window in it</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39109 rows Ã— 4 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56830fbe-2fe2-4c43-83b6-9ad71e538df9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-56830fbe-2fe2-4c43-83b6-9ad71e538df9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-56830fbe-2fe2-4c43-83b6-9ad71e538df9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                            filename  \\\n",
       "0      COCO_val2014_000000389081.jpg   \n",
       "1      COCO_val2014_000000389081.jpg   \n",
       "2      COCO_val2014_000000389081.jpg   \n",
       "3      COCO_val2014_000000389081.jpg   \n",
       "4      COCO_val2014_000000389081.jpg   \n",
       "...                              ...   \n",
       "39194  COCO_val2014_000000190705.jpg   \n",
       "39195  COCO_val2014_000000190705.jpg   \n",
       "39196  COCO_val2014_000000190705.jpg   \n",
       "39197  COCO_val2014_000000190705.jpg   \n",
       "39198  COCO_val2014_000000190705.jpg   \n",
       "\n",
       "                                                     raw  sentiment  split  \n",
       "0               a plate of delici food includ french fri          1  train  \n",
       "1      french fri are not a healthi food but it is an...          1  train  \n",
       "2      the plate ha one of my favorit food on it fren...          1  train  \n",
       "3                   it wa disgust food not just bad food          0  train  \n",
       "4               a plate of disgust food found at a diner          0  train  \n",
       "...                                                  ...        ...    ...  \n",
       "39194  a dirti bathroom that ha a dirti window made o...          0   test  \n",
       "39195            a dirti bathroom that ha a window in it          0   test  \n",
       "39196      a towel that is on a rack in a dirti bathroom          0   test  \n",
       "39197  a dirti bathroom that ha a dirti window made o...          0   test  \n",
       "39198            a dirti bathroom that ha a window in it          0   test  \n",
       "\n",
       "[39109 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/content/gdrive/MyDrive/Project/sentiment/sentiment.csv')\n",
    "df=df[['filename','raw','sentiment','split']]\n",
    "df=df[df['filename']!='COCO_val2014_000000421673.jpg']\n",
    "df=df[df['filename']!='COCO_val2014_000000359276.jpg']\n",
    "df=df[df['filename']!='COCO_val2014_000000130712.jpg']\n",
    "df=df[df['filename']!='COCO_val2014_000000310622.jpg']\n",
    "clean_text = lambda text: text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "df['raw'] = df['raw'].apply(clean_text)\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "#stem the text\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in nltk.word_tokenize(text)])\n",
    "\n",
    "df['raw'] = df['raw'].apply(lemmatize_text)\n",
    "df['raw'] = df['raw'].apply(stem_text)\n",
    "\n",
    "vgg=m.vgg16(pretrained=True)\n",
    "vgg.to(device)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, model, train):\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "        self.train = train\n",
    "        \n",
    "        if self.train == 'train':\n",
    "            self.df = self.df[self.df['split'] == 'train']\n",
    "        elif self.train == 'valid':\n",
    "            self.df = self.df[self.df['split'] == 'val']\n",
    "        else:\n",
    "            self.df = self.df[self.df['split'] == 'test']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        z=torch.tensor(int(self.df.iloc[index,2]))\n",
    "        \n",
    "#         txtModel = SentenceTransformer('all-mpnet-base-v2')\n",
    "        x=self.model.encode(self.df.iloc[index,1])\n",
    "        x = torch.tensor(x)\n",
    "        \n",
    "        x = F.pad(x, (0,100-x.size()[0]))\n",
    "\n",
    "        # tried manually\n",
    "#         txtEmbedings=[]\n",
    "#         for i, row in df.iterrows():\n",
    "#             txtEmbedings.append(generateTextEmbedings(txtModel,row['raw']))\n",
    "#         for i in txtEmbedings:\n",
    "#             while len(i)!=481:    #padding\n",
    "#                 i.append(0)\n",
    "        #if df.shape[0]==len(txtEmbedings):\n",
    "#         df['embedings']=txtEmbedings\n",
    "#         image = Image.imread(r'C:\\Users\\Dell\\sentiment_images'+self.df.iloc[index,0])\n",
    "#         image = Image.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image = Image.open(r'/content/gdrive/MyDrive/Project/sentiment/sentiment_images/'+self.df.iloc[index,0])\n",
    "\n",
    "        image = image.convert('RGB')\n",
    "        transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((100,100)),\n",
    "#         torchvision.transforms.CenterCrop(224),\n",
    "        ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "        y = transform(image).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y=vgg.features(y)\n",
    "\n",
    "        return (y,x,z)\n",
    "counter=0\n",
    "for i,row in df.iterrows():\n",
    "    if row['split']!='train':\n",
    "        counter+=1\n",
    "print(counter)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4e41e",
   "metadata": {
    "id": "e3d4e41e"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea231003",
   "metadata": {
    "id": "ea231003"
   },
   "outputs": [],
   "source": [
    "class model01(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(4608,350)\n",
    "        #self.act_fn = nn.ReLU()\n",
    "        self.act_fn = nn.Tanh()\n",
    "        # self.act_fn2 = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(350,250)\n",
    "        self.linear3 = nn.Linear(250, 5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class model02(nn.Module):\n",
    "\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(100,200)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.act_fn2 = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(200, 80)\n",
    "        self.linear3 = nn.Linear(80, 5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act_fn2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "class modelCombined(nn.Module):\n",
    "\n",
    "    def __init__(self, M1,M2):\n",
    "        super().__init__()\n",
    "        self.M1=M1\n",
    "        self.M2=M2\n",
    "        \n",
    "        self.linear1 = nn.Linear(10, 30)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(30, 20)\n",
    "        self.linear3 = nn.Linear(20, 18)\n",
    "        self.linear4 = nn.Linear(18, 15)\n",
    "        self.linear5 = nn.Linear(15, 12)\n",
    "        self.linear6 = nn.Linear(12, 10)\n",
    "        self.linear7 = nn.Linear(10, 5)\n",
    "        self.linear8 = nn.Linear(5, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, y,z):\n",
    "        y=self.M1(y)\n",
    "        z=self.M2(z)\n",
    "        x = torch.cat((y, z), dim=1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear5(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear6(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear7(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e1d19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c4e1d19",
    "outputId": "4b72eda3-bcad-492c-8f5e-a982576d2835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelCombined(\n",
      "  (M1): model01(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (linear1): Linear(in_features=4608, out_features=350, bias=True)\n",
      "    (act_fn): Tanh()\n",
      "    (linear2): Linear(in_features=350, out_features=250, bias=True)\n",
      "    (linear3): Linear(in_features=250, out_features=5, bias=True)\n",
      "  )\n",
      "  (M2): model02(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (act_fn): ReLU()\n",
      "    (act_fn2): Tanh()\n",
      "    (linear2): Linear(in_features=200, out_features=80, bias=True)\n",
      "    (linear3): Linear(in_features=80, out_features=5, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=10, out_features=30, bias=True)\n",
      "  (act_fn): Tanh()\n",
      "  (linear2): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (linear3): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (linear4): Linear(in_features=15, out_features=10, bias=True)\n",
      "  (linear5): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "M1=model01().to(device)\n",
    "M2=model02().to(device)\n",
    "model = modelCombined(M1, M2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35730aaa",
   "metadata": {
    "id": "35730aaa"
   },
   "source": [
    "# Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cf82c",
   "metadata": {
    "id": "e47cf82c"
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(df, txtModel, train='train')\n",
    "test_dataset = CustomDataset(df, txtModel, train='valid')\n",
    "finaltest_dataset = CustomDataset(df, txtModel, train='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a63a0",
   "metadata": {
    "id": "847a63a0"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "finaltest_loader=DataLoader(finaltest_dataset, batch_size=64)\n",
    "\n",
    "#debuging\n",
    "# for i,x in enumerate(train_loader):\n",
    "#     img,txt,label = x\n",
    "#     print(txt.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7817aea",
   "metadata": {
    "id": "a7817aea"
   },
   "outputs": [],
   "source": [
    "image = Image.open(r'/content/gdrive/MyDrive/Project/sentiment/sentiment_images/'+df.iloc[3,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1954d5",
   "metadata": {
    "id": "cd1954d5"
   },
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d371afe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d371afe",
    "outputId": "fae0b0f6-6031-4860-a4a5-1b5afb0f4ffe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t\t\t Iteration 1 Loss Train: 0.7016758322715759\n",
      "Epoch: 1 \t\t\t Iteration 2 Loss Train: 0.6929707527160645\n",
      "Epoch: 1 \t\t\t Iteration 3 Loss Train: 0.6912208199501038\n",
      "Epoch: 1 \t\t\t Iteration 4 Loss Train: 0.6824958324432373\n",
      "Epoch: 1 \t\t\t Iteration 5 Loss Train: 0.694713830947876\n",
      "Epoch: 1 \t\t\t Iteration 6 Loss Train: 0.7034436464309692\n",
      "Epoch: 1 \t\t\t Iteration 7 Loss Train: 0.7033905982971191\n",
      "Epoch: 1 \t\t\t Iteration 8 Loss Train: 0.7016528844833374\n",
      "Epoch: 1 \t\t\t Iteration 9 Loss Train: 0.7033727765083313\n",
      "Epoch: 1 \t\t\t Iteration 10 Loss Train: 0.7016185522079468\n",
      "Epoch: 1 \t\t\t Iteration 11 Loss Train: 0.6877783536911011\n",
      "Epoch: 1 \t\t\t Iteration 12 Loss Train: 0.7033379077911377\n",
      "Epoch: 1 \t\t\t Iteration 13 Loss Train: 0.703331708908081\n",
      "Epoch: 1 \t\t\t Iteration 14 Loss Train: 0.699857771396637\n",
      "Epoch: 1 \t\t\t Iteration 15 Loss Train: 0.7015633583068848\n",
      "Epoch: 1 \t\t\t Iteration 16 Loss Train: 0.6929572820663452\n",
      "Epoch: 1 \t\t\t Iteration 17 Loss Train: 0.6877619028091431\n",
      "Epoch: 1 \t\t\t Iteration 18 Loss Train: 0.7084369659423828\n",
      "Epoch: 1 \t\t\t Iteration 19 Loss Train: 0.7101093530654907\n",
      "Epoch: 1 \t\t\t Iteration 20 Loss Train: 0.6946483254432678\n",
      "Epoch: 1 \t\t\t Iteration 21 Loss Train: 0.6997894048690796\n",
      "Epoch: 1 \t\t\t Iteration 22 Loss Train: 0.6912011504173279\n",
      "Epoch: 1 \t\t\t Iteration 23 Loss Train: 0.6980745792388916\n",
      "Epoch: 1 \t\t\t Iteration 24 Loss Train: 0.699752688407898\n",
      "Epoch: 1 \t\t\t Iteration 25 Loss Train: 0.7031797766685486\n",
      "Epoch: 1 \t\t\t Iteration 26 Loss Train: 0.7048364877700806\n",
      "Epoch: 1 \t\t\t Iteration 27 Loss Train: 0.6946163773536682\n",
      "Epoch: 1 \t\t\t Iteration 28 Loss Train: 0.6980099678039551\n",
      "Epoch: 1 \t\t\t Iteration 29 Loss Train: 0.7031104564666748\n",
      "Epoch: 1 \t\t\t Iteration 30 Loss Train: 0.7115830183029175\n",
      "Epoch: 1 \t\t\t Iteration 31 Loss Train: 0.6979997158050537\n",
      "Epoch: 1 \t\t\t Iteration 32 Loss Train: 0.6996525526046753\n",
      "Epoch: 1 \t\t\t Iteration 33 Loss Train: 0.6912409067153931\n",
      "Epoch: 1 \t\t\t Iteration 34 Loss Train: 0.6895368099212646\n",
      "Epoch: 1 \t\t\t Iteration 35 Loss Train: 0.6962869763374329\n",
      "Epoch: 1 \t\t\t Iteration 36 Loss Train: 0.684485673904419\n",
      "Epoch: 1 \t\t\t Iteration 37 Loss Train: 0.7063961625099182\n",
      "Epoch: 1 \t\t\t Iteration 38 Loss Train: 0.697927713394165\n",
      "Epoch: 1 \t\t\t Iteration 39 Loss Train: 0.6996545791625977\n",
      "Epoch: 1 \t\t\t Iteration 40 Loss Train: 0.7097021341323853\n",
      "Epoch: 1 \t\t\t Iteration 41 Loss Train: 0.7029296159744263\n",
      "Epoch: 1 \t\t\t Iteration 42 Loss Train: 0.7079563140869141\n",
      "Epoch: 1 \t\t\t Iteration 43 Loss Train: 0.6995532512664795\n",
      "Epoch: 1 \t\t\t Iteration 44 Loss Train: 0.7011987566947937\n",
      "Epoch: 1 \t\t\t Iteration 45 Loss Train: 0.7012110948562622\n",
      "Epoch: 1 \t\t\t Iteration 46 Loss Train: 0.6962313652038574\n",
      "Epoch: 1 \t\t\t Iteration 47 Loss Train: 0.7061774730682373\n",
      "Epoch: 1 \t\t\t Iteration 48 Loss Train: 0.6978634595870972\n",
      "Epoch: 1 \t\t\t Iteration 49 Loss Train: 0.7110582590103149\n",
      "Epoch: 1 \t\t\t Iteration 50 Loss Train: 0.6846619248390198\n",
      "Epoch: 1 \t\t\t Iteration 51 Loss Train: 0.7060777544975281\n",
      "Epoch: 1 \t\t\t Iteration 52 Loss Train: 0.6896060109138489\n",
      "Epoch: 1 \t\t\t Iteration 53 Loss Train: 0.6994614005088806\n",
      "Epoch: 1 \t\t\t Iteration 54 Loss Train: 0.6978393793106079\n",
      "Epoch: 1 \t\t\t Iteration 55 Loss Train: 0.6994305849075317\n",
      "Epoch: 1 \t\t\t Iteration 56 Loss Train: 0.7010964751243591\n",
      "Epoch: 1 \t\t\t Iteration 57 Loss Train: 0.6961702108383179\n",
      "Epoch: 1 \t\t\t Iteration 58 Loss Train: 0.7010602951049805\n",
      "Epoch: 1 \t\t\t Iteration 59 Loss Train: 0.7043287754058838\n",
      "Epoch: 1 \t\t\t Iteration 60 Loss Train: 0.7075529098510742\n",
      "Epoch: 1 \t\t\t Iteration 61 Loss Train: 0.6928761005401611\n",
      "Epoch: 1 \t\t\t Iteration 62 Loss Train: 0.6961305141448975\n",
      "Epoch: 1 \t\t\t Iteration 63 Loss Train: 0.7058781385421753\n",
      "Epoch: 1 \t\t\t Iteration 64 Loss Train: 0.7010049819946289\n",
      "Epoch: 1 \t\t\t Iteration 65 Loss Train: 0.6977211833000183\n",
      "Epoch: 1 \t\t\t Iteration 66 Loss Train: 0.697716236114502\n",
      "Epoch: 1 \t\t\t Iteration 67 Loss Train: 0.6961154341697693\n",
      "Epoch: 1 \t\t\t Iteration 68 Loss Train: 0.6993280053138733\n",
      "Epoch: 1 \t\t\t Iteration 69 Loss Train: 0.700917661190033\n",
      "Epoch: 1 \t\t\t Iteration 70 Loss Train: 0.6848375797271729\n",
      "Epoch: 1 \t\t\t Iteration 71 Loss Train: 0.7057684659957886\n",
      "Epoch: 1 \t\t\t Iteration 72 Loss Train: 0.6977206468582153\n",
      "Epoch: 1 \t\t\t Iteration 73 Loss Train: 0.6944849491119385\n",
      "Epoch: 1 \t\t\t Iteration 74 Loss Train: 0.6992801427841187\n",
      "Epoch: 1 \t\t\t Iteration 75 Loss Train: 0.6944698691368103\n",
      "Epoch: 1 \t\t\t Iteration 76 Loss Train: 0.6992714405059814\n",
      "Epoch: 1 \t\t\t Iteration 77 Loss Train: 0.6880536675453186\n",
      "Epoch: 1 \t\t\t Iteration 78 Loss Train: 0.6944366693496704\n",
      "Epoch: 1 \t\t\t Iteration 79 Loss Train: 0.6912651062011719\n",
      "Epoch: 1 \t\t\t Iteration 80 Loss Train: 0.6976743936538696\n",
      "Epoch: 1 \t\t\t Iteration 81 Loss Train: 0.6896368861198425\n",
      "Epoch: 1 \t\t\t Iteration 82 Loss Train: 0.6960498690605164\n",
      "Epoch: 1 \t\t\t Iteration 83 Loss Train: 0.7072497606277466\n",
      "Epoch: 1 \t\t\t Iteration 84 Loss Train: 0.6976215839385986\n",
      "Epoch: 1 \t\t\t Iteration 85 Loss Train: 0.6976017355918884\n",
      "Epoch: 1 \t\t\t Iteration 86 Loss Train: 0.6912663578987122\n",
      "Epoch: 1 \t\t\t Iteration 87 Loss Train: 0.6992142200469971\n",
      "Epoch: 1 \t\t\t Iteration 88 Loss Train: 0.7103503346443176\n",
      "Epoch: 1 \t\t\t Iteration 89 Loss Train: 0.6944361329078674\n",
      "Epoch: 1 \t\t\t Iteration 90 Loss Train: 0.6991896629333496\n",
      "Epoch: 1 \t\t\t Iteration 91 Loss Train: 0.6959836483001709\n",
      "Epoch: 1 \t\t\t Iteration 92 Loss Train: 0.7023099660873413\n",
      "Epoch: 1 \t\t\t Iteration 93 Loss Train: 0.7007231712341309\n",
      "Epoch: 1 \t\t\t Iteration 94 Loss Train: 0.7007110714912415\n",
      "Epoch: 1 \t\t\t Iteration 95 Loss Train: 0.6959805488586426\n",
      "Epoch: 1 \t\t\t Iteration 96 Loss Train: 0.699146568775177\n",
      "Epoch: 1 \t\t\t Iteration 97 Loss Train: 0.7022486925125122\n",
      "Epoch: 1 \t\t\t Iteration 98 Loss Train: 0.6943891644477844\n",
      "Epoch: 1 \t\t\t Iteration 99 Loss Train: 0.69127357006073\n",
      "Epoch: 1 \t\t\t Iteration 100 Loss Train: 0.695949912071228\n",
      "Epoch: 1 \t\t\t Iteration 101 Loss Train: 0.6897037029266357\n",
      "Epoch: 1 \t\t\t Iteration 102 Loss Train: 0.6928213834762573\n",
      "Epoch: 1 \t\t\t Iteration 103 Loss Train: 0.6959574222564697\n",
      "Epoch: 1 \t\t\t Iteration 104 Loss Train: 0.7006410360336304\n",
      "Epoch: 1 \t\t\t Iteration 105 Loss Train: 0.7068848609924316\n",
      "Epoch: 1 \t\t\t Iteration 106 Loss Train: 0.6975167989730835\n",
      "Epoch: 1 \t\t\t Iteration 107 Loss Train: 0.7021889090538025\n",
      "Epoch: 1 \t\t\t Iteration 108 Loss Train: 0.7005994319915771\n",
      "Epoch: 1 \t\t\t Iteration 109 Loss Train: 0.6944063305854797\n",
      "Epoch: 1 \t\t\t Iteration 110 Loss Train: 0.7052501440048218\n",
      "Epoch: 1 \t\t\t Iteration 111 Loss Train: 0.6989990472793579\n",
      "Epoch: 1 \t\t\t Iteration 112 Loss Train: 0.7067224979400635\n",
      "Epoch: 1 \t\t\t Iteration 113 Loss Train: 0.7005282044410706\n",
      "Epoch: 1 \t\t\t Iteration 114 Loss Train: 0.6974235773086548\n",
      "Epoch: 1 \t\t\t Iteration 115 Loss Train: 0.7112564444541931\n",
      "Epoch: 1 \t\t\t Iteration 116 Loss Train: 0.6881963014602661\n",
      "Epoch: 1 \t\t\t Iteration 117 Loss Train: 0.7004880309104919\n",
      "Epoch: 1 \t\t\t Iteration 118 Loss Train: 0.7019980549812317\n",
      "Epoch: 1 \t\t\t Iteration 119 Loss Train: 0.6989050507545471\n",
      "Epoch: 1 \t\t\t Iteration 120 Loss Train: 0.6882370114326477\n",
      "Epoch: 1 \t\t\t Iteration 121 Loss Train: 0.6928057670593262\n",
      "Epoch: 1 \t\t\t Iteration 122 Loss Train: 0.7050080299377441\n",
      "Epoch: 1 \t\t\t Iteration 123 Loss Train: 0.6882544755935669\n",
      "Epoch: 1 \t\t\t Iteration 124 Loss Train: 0.7049747109413147\n",
      "Epoch: 1 \t\t\t Iteration 125 Loss Train: 0.6958385705947876\n",
      "Epoch: 1 \t\t\t Iteration 126 Loss Train: 0.6897933483123779\n",
      "Epoch: 1 \t\t\t Iteration 127 Loss Train: 0.6973385810852051\n",
      "Epoch: 1 \t\t\t Iteration 128 Loss Train: 0.6928086876869202\n",
      "Epoch: 1 \t\t\t Iteration 129 Loss Train: 0.7004013061523438\n",
      "Epoch: 1 \t\t\t Iteration 130 Loss Train: 0.6928256154060364\n",
      "Epoch: 1 \t\t\t Iteration 131 Loss Train: 0.7033876180648804\n",
      "Epoch: 1 \t\t\t Iteration 132 Loss Train: 0.6988459825515747\n",
      "Epoch: 1 \t\t\t Iteration 133 Loss Train: 0.6897872686386108\n",
      "Epoch: 1 \t\t\t Iteration 134 Loss Train: 0.6882801651954651\n",
      "Epoch: 1 \t\t\t Iteration 135 Loss Train: 0.706402063369751\n",
      "Epoch: 1 \t\t\t Iteration 136 Loss Train: 0.6958147287368774\n",
      "Epoch: 1 \t\t\t Iteration 137 Loss Train: 0.6988143920898438\n",
      "Epoch: 1 \t\t\t Iteration 138 Loss Train: 0.6913235187530518\n",
      "Epoch: 1 \t\t\t Iteration 139 Loss Train: 0.7033184170722961\n",
      "Epoch: 1 \t\t\t Iteration 140 Loss Train: 0.7002865076065063\n",
      "Epoch: 1 \t\t\t Iteration 141 Loss Train: 0.6943085789680481\n",
      "Epoch: 1 \t\t\t Iteration 142 Loss Train: 0.700279712677002\n",
      "Epoch: 1 \t\t\t Iteration 143 Loss Train: 0.6868278980255127\n",
      "Epoch: 1 \t\t\t Iteration 144 Loss Train: 0.6928003430366516\n",
      "Epoch: 1 \t\t\t Iteration 145 Loss Train: 0.700262188911438\n",
      "Epoch: 1 \t\t\t Iteration 146 Loss Train: 0.6943027973175049\n",
      "Epoch: 1 \t\t\t Iteration 147 Loss Train: 0.6972769498825073\n",
      "Epoch: 1 \t\t\t Iteration 148 Loss Train: 0.6987507939338684\n",
      "Epoch: 1 \t\t\t Iteration 149 Loss Train: 0.7032088041305542\n",
      "Epoch: 1 \t\t\t Iteration 150 Loss Train: 0.7031780481338501\n",
      "Epoch: 1 \t\t\t Iteration 151 Loss Train: 0.6987429857254028\n",
      "Epoch: 1 \t\t\t Iteration 152 Loss Train: 0.6957581043243408\n",
      "Epoch: 1 \t\t\t Iteration 153 Loss Train: 0.6987001299858093\n",
      "Epoch: 1 \t\t\t Iteration 154 Loss Train: 0.6986987590789795\n",
      "Epoch: 1 \t\t\t Iteration 155 Loss Train: 0.697211503982544\n",
      "Epoch: 1 \t\t\t Iteration 156 Loss Train: 0.7045642733573914\n",
      "Epoch: 1 \t\t\t Iteration 157 Loss Train: 0.7016019821166992\n",
      "Epoch: 1 \t\t\t Iteration 158 Loss Train: 0.7088741064071655\n",
      "Epoch: 1 \t\t\t Iteration 159 Loss Train: 0.6927869319915771\n",
      "Epoch: 1 \t\t\t Iteration 160 Loss Train: 0.7015625238418579\n",
      "Epoch: 1 \t\t\t Iteration 161 Loss Train: 0.701541006565094\n",
      "Epoch: 1 \t\t\t Iteration 162 Loss Train: 0.6971641778945923\n",
      "Epoch: 1 \t\t\t Iteration 163 Loss Train: 0.6942332983016968\n",
      "Epoch: 1 \t\t\t Iteration 164 Loss Train: 0.7014781832695007\n",
      "Epoch: 1 \t\t\t Iteration 165 Loss Train: 0.7014880776405334\n",
      "Epoch: 1 \t\t\t Iteration 166 Loss Train: 0.6956580877304077\n",
      "Epoch: 1 \t\t\t Iteration 167 Loss Train: 0.7057965993881226\n",
      "Epoch: 1 \t\t\t Iteration 168 Loss Train: 0.6927956938743591\n",
      "Epoch: 1 \t\t\t Iteration 169 Loss Train: 0.6985224485397339\n",
      "Epoch: 1 \t\t\t Iteration 170 Loss Train: 0.699976921081543\n",
      "Epoch: 1 \t\t\t Iteration 171 Loss Train: 0.697084903717041\n",
      "Epoch: 1 \t\t\t Iteration 172 Loss Train: 0.698543131351471\n",
      "Epoch: 1 \t\t\t Iteration 173 Loss Train: 0.6870163679122925\n",
      "Epoch: 1 \t\t\t Iteration 174 Loss Train: 0.7013874053955078\n",
      "Epoch: 1 \t\t\t Iteration 175 Loss Train: 0.6999247074127197\n",
      "Epoch: 1 \t\t\t Iteration 176 Loss Train: 0.6984989047050476\n",
      "Epoch: 1 \t\t\t Iteration 177 Loss Train: 0.6942106485366821\n",
      "Epoch: 1 \t\t\t Iteration 178 Loss Train: 0.7070437669754028\n",
      "Epoch: 1 \t\t\t Iteration 179 Loss Train: 0.6984671354293823\n",
      "Epoch: 1 \t\t\t Iteration 180 Loss Train: 0.692773163318634\n",
      "Epoch: 1 \t\t\t Iteration 181 Loss Train: 0.7027262449264526\n",
      "Epoch: 1 \t\t\t Iteration 182 Loss Train: 0.6998496651649475\n",
      "Epoch: 1 \t\t\t Iteration 183 Loss Train: 0.6913366317749023\n",
      "Epoch: 1 \t\t\t Iteration 184 Loss Train: 0.6899310350418091\n",
      "Epoch: 1 \t\t\t Iteration 185 Loss Train: 0.6899474263191223\n",
      "Epoch: 1 \t\t\t Iteration 186 Loss Train: 0.7069261074066162\n",
      "Epoch: 1 \t\t\t Iteration 187 Loss Train: 0.6955591440200806\n",
      "Epoch: 1 \t\t\t Iteration 188 Loss Train: 0.7012224197387695\n",
      "Epoch: 1 \t\t\t Iteration 189 Loss Train: 0.6927391290664673\n",
      "Epoch: 1 \t\t\t Iteration 190 Loss Train: 0.6983950734138489\n",
      "Epoch: 1 \t\t\t Iteration 191 Loss Train: 0.7025865912437439\n",
      "Epoch: 1 \t\t\t Iteration 192 Loss Train: 0.6997733116149902\n",
      "Epoch: 1 \t\t\t Iteration 193 Loss Train: 0.7067506313323975\n",
      "Epoch: 1 \t\t\t Iteration 194 Loss Train: 0.6927118301391602\n",
      "Epoch: 1 \t\t\t Iteration 195 Loss Train: 0.6983227133750916\n",
      "Epoch: 1 \t\t\t Iteration 196 Loss Train: 0.7038824558258057\n",
      "Epoch: 1 \t\t\t Iteration 197 Loss Train: 0.7010952234268188\n",
      "Epoch: 1 \t\t\t Iteration 198 Loss Train: 0.695498526096344\n",
      "Epoch: 1 \t\t\t Iteration 199 Loss Train: 0.6969033479690552\n",
      "Epoch: 1 \t\t\t Iteration 200 Loss Train: 0.6913467645645142\n",
      "Epoch: 1 \t\t\t Iteration 201 Loss Train: 0.7024478316307068\n",
      "Epoch: 1 \t\t\t Iteration 202 Loss Train: 0.7092939615249634\n",
      "Epoch: 1 \t\t\t Iteration 203 Loss Train: 0.7010048627853394\n",
      "Epoch: 1 \t\t\t Iteration 204 Loss Train: 0.6982293128967285\n",
      "Epoch: 1 \t\t\t Iteration 205 Loss Train: 0.6955033540725708\n",
      "Epoch: 1 \t\t\t Iteration 206 Loss Train: 0.6913752555847168\n",
      "Epoch: 1 \t\t\t Iteration 207 Loss Train: 0.6955043077468872\n",
      "Epoch: 1 \t\t\t Iteration 208 Loss Train: 0.7009507417678833\n",
      "Epoch: 1 \t\t\t Iteration 209 Loss Train: 0.698218822479248\n",
      "Epoch: 1 \t\t\t Iteration 210 Loss Train: 0.6981889009475708\n",
      "Epoch: 1 \t\t\t Iteration 211 Loss Train: 0.6981700658798218\n",
      "Epoch: 1 \t\t\t Iteration 212 Loss Train: 0.6968119740486145\n",
      "Epoch: 1 \t\t\t Iteration 213 Loss Train: 0.700896143913269\n",
      "Epoch: 1 \t\t\t Iteration 214 Loss Train: 0.6873233318328857\n",
      "Epoch: 1 \t\t\t Iteration 215 Loss Train: 0.6954314708709717\n",
      "Epoch: 1 \t\t\t Iteration 216 Loss Train: 0.692745566368103\n",
      "Epoch: 1 \t\t\t Iteration 217 Loss Train: 0.6981573700904846\n",
      "Epoch: 1 \t\t\t Iteration 218 Loss Train: 0.6981470584869385\n",
      "Epoch: 1 \t\t\t Iteration 219 Loss Train: 0.7035685777664185\n",
      "Epoch: 1 \t\t\t Iteration 220 Loss Train: 0.6954396963119507\n",
      "Epoch: 1 \t\t\t Iteration 221 Loss Train: 0.7021660804748535\n",
      "Epoch: 1 \t\t\t Iteration 222 Loss Train: 0.6954123377799988\n",
      "Epoch: 1 \t\t\t Iteration 223 Loss Train: 0.7021326422691345\n",
      "Epoch: 1 \t\t\t Iteration 224 Loss Train: 0.7021249532699585\n",
      "Epoch: 1 \t\t\t Iteration 225 Loss Train: 0.6994190216064453\n",
      "Epoch: 1 \t\t\t Iteration 226 Loss Train: 0.7100870609283447\n",
      "Epoch: 1 \t\t\t Iteration 227 Loss Train: 0.6887363195419312\n",
      "Epoch: 1 \t\t\t Iteration 228 Loss Train: 0.6940374970436096\n",
      "Epoch: 1 \t\t\t Iteration 229 Loss Train: 0.6940712332725525\n",
      "Epoch: 1 \t\t\t Iteration 230 Loss Train: 0.6900559663772583\n",
      "Epoch: 1 \t\t\t Iteration 231 Loss Train: 0.6967031359672546\n",
      "Epoch: 1 \t\t\t Iteration 232 Loss Train: 0.6953938603401184\n",
      "Epoch: 1 \t\t\t Iteration 233 Loss Train: 0.6927183866500854\n",
      "Epoch: 1 \t\t\t Iteration 234 Loss Train: 0.698036253452301\n",
      "Epoch: 1 \t\t\t Iteration 235 Loss Train: 0.6874170303344727\n",
      "Epoch: 1 \t\t\t Iteration 236 Loss Train: 0.7046799659729004\n",
      "Epoch: 1 \t\t\t Iteration 237 Loss Train: 0.6927268505096436\n",
      "Epoch: 1 \t\t\t Iteration 238 Loss Train: 0.6940323114395142\n",
      "Epoch: 1 \t\t\t Iteration 239 Loss Train: 0.6940475702285767\n",
      "Epoch: 1 \t\t\t Iteration 240 Loss Train: 0.6887549161911011\n",
      "Epoch: 1 \t\t\t Iteration 241 Loss Train: 0.6980118751525879\n",
      "Epoch: 1 \t\t\t Iteration 242 Loss Train: 0.7059150338172913\n",
      "Epoch: 1 \t\t\t Iteration 243 Loss Train: 0.7045688033103943\n",
      "Epoch: 1 \t\t\t Iteration 244 Loss Train: 0.6992485523223877\n",
      "Epoch: 1 \t\t\t Iteration 245 Loss Train: 0.700598955154419\n",
      "Epoch: 1 \t\t\t Iteration 246 Loss Train: 0.6992498636245728\n",
      "Epoch: 1 \t\t\t Iteration 247 Loss Train: 0.7031564712524414\n",
      "Epoch: 1 \t\t\t Iteration 248 Loss Train: 0.6940090656280518\n",
      "Epoch: 1 \t\t\t Iteration 249 Loss Train: 0.695289671421051\n",
      "Epoch: 1 \t\t\t Iteration 250 Loss Train: 0.700525164604187\n",
      "Epoch: 1 \t\t\t Iteration 251 Loss Train: 0.7004622220993042\n",
      "Epoch: 1 \t\t\t Iteration 252 Loss Train: 0.692715048789978\n",
      "Epoch: 1 \t\t\t Iteration 253 Loss Train: 0.69141685962677\n",
      "Epoch: 1 \t\t\t Iteration 254 Loss Train: 0.7031000852584839\n",
      "Epoch: 1 \t\t\t Iteration 255 Loss Train: 0.7004497647285461\n",
      "Epoch: 1 \t\t\t Iteration 256 Loss Train: 0.7017104625701904\n",
      "Epoch: 1 \t\t\t Iteration 257 Loss Train: 0.700396716594696\n",
      "Epoch: 1 \t\t\t Iteration 258 Loss Train: 0.6914170980453491\n",
      "Epoch: 1 \t\t\t Iteration 259 Loss Train: 0.6952584385871887\n",
      "Epoch: 1 \t\t\t Iteration 260 Loss Train: 0.7055147886276245\n",
      "Epoch: 1 \t\t\t Iteration 261 Loss Train: 0.6965469121932983\n",
      "Epoch: 1 \t\t\t Iteration 262 Loss Train: 0.7003360986709595\n",
      "Epoch: 1 \t\t\t Iteration 263 Loss Train: 0.7016353011131287\n",
      "Epoch: 1 \t\t\t Iteration 264 Loss Train: 0.7066605091094971\n",
      "Epoch: 1 \t\t\t Iteration 265 Loss Train: 0.6952387094497681\n",
      "Epoch: 1 \t\t\t Iteration 266 Loss Train: 0.6939729452133179\n",
      "Epoch: 1 \t\t\t Iteration 267 Loss Train: 0.6964834928512573\n",
      "Epoch: 1 \t\t\t Iteration 268 Loss Train: 0.7015462517738342\n",
      "Epoch: 1 \t\t\t Iteration 269 Loss Train: 0.6964786052703857\n",
      "Epoch: 1 \t\t\t Iteration 270 Loss Train: 0.6989774703979492\n",
      "Epoch: 1 \t\t\t Iteration 271 Loss Train: 0.6964432001113892\n",
      "Epoch: 1 \t\t\t Iteration 272 Loss Train: 0.690204381942749\n",
      "Epoch: 1 \t\t\t Iteration 273 Loss Train: 0.690177857875824\n",
      "Epoch: 1 \t\t\t Iteration 274 Loss Train: 0.6939677000045776\n",
      "Epoch: 1 \t\t\t Iteration 275 Loss Train: 0.6989723443984985\n",
      "Epoch: 1 \t\t\t Iteration 276 Loss Train: 0.6977040767669678\n",
      "Epoch: 1 \t\t\t Iteration 277 Loss Train: 0.6939355134963989\n",
      "Epoch: 1 \t\t\t Iteration 278 Loss Train: 0.7051769495010376\n",
      "Epoch: 1 \t\t\t Iteration 279 Loss Train: 0.6989407539367676\n",
      "Epoch: 1 \t\t\t Iteration 280 Loss Train: 0.6964114904403687\n",
      "Epoch: 1 \t\t\t Iteration 281 Loss Train: 0.6877250671386719\n",
      "Epoch: 1 \t\t\t Iteration 282 Loss Train: 0.7038928866386414\n",
      "Epoch: 1 \t\t\t Iteration 283 Loss Train: 0.6939473152160645\n",
      "Epoch: 1 \t\t\t Iteration 284 Loss Train: 0.6852625012397766\n",
      "Epoch: 1 \t\t\t Iteration 285 Loss Train: 0.7001323103904724\n",
      "Epoch: 1 \t\t\t Iteration 286 Loss Train: 0.6914502382278442\n",
      "Epoch: 1 \t\t\t Iteration 287 Loss Train: 0.7025734186172485\n",
      "Epoch: 1 \t\t\t Iteration 288 Loss Train: 0.6951763033866882\n",
      "Epoch: 1 \t\t\t Iteration 289 Loss Train: 0.7013548612594604\n",
      "Epoch: 1 \t\t\t Iteration 290 Loss Train: 0.6975773572921753\n",
      "Epoch: 1 \t\t\t Iteration 291 Loss Train: 0.7000719308853149\n",
      "Epoch: 1 \t\t\t Iteration 292 Loss Train: 0.6963750720024109\n",
      "Epoch: 1 \t\t\t Iteration 293 Loss Train: 0.6938862800598145\n",
      "Epoch: 1 \t\t\t Iteration 294 Loss Train: 0.7049295902252197\n",
      "Epoch: 1 \t\t\t Iteration 295 Loss Train: 0.6975684762001038\n",
      "Epoch: 1 \t\t\t Iteration 296 Loss Train: 0.6902692317962646\n",
      "Epoch: 1 \t\t\t Iteration 297 Loss Train: 0.6963576674461365\n",
      "Epoch: 1 \t\t\t Iteration 298 Loss Train: 0.6902437210083008\n",
      "Epoch: 1 \t\t\t Iteration 299 Loss Train: 0.6963503360748291\n",
      "Epoch: 1 \t\t\t Iteration 300 Loss Train: 0.6951140761375427\n",
      "Epoch: 1 \t\t\t Iteration 301 Loss Train: 0.699959397315979\n",
      "Epoch: 1 \t\t\t Iteration 302 Loss Train: 0.7074265480041504\n",
      "Epoch: 1 \t\t\t Iteration 1 Loss Valid: 0.695111870765686\n",
      "Epoch: 1 \t\t\t Iteration 2 Loss Valid: 0.6938782930374146\n",
      "Epoch: 1 \t\t\t Iteration 3 Loss Valid: 0.692690372467041\n",
      "Epoch: 1 \t\t\t Iteration 4 Loss Valid: 0.6950918436050415\n",
      "Epoch: 1 \t\t\t Iteration 5 Loss Valid: 0.6938952207565308\n",
      "Epoch: 1 \t\t\t Iteration 6 Loss Valid: 0.6902644634246826\n",
      "Epoch: 1 \t\t\t Iteration 7 Loss Valid: 0.6950962543487549\n",
      "Epoch: 1 \t\t\t Iteration 8 Loss Valid: 0.692690908908844\n",
      "Epoch: 1 \t\t\t Iteration 9 Loss Valid: 0.695094645023346\n",
      "Epoch: 1 \t\t\t Iteration 10 Loss Valid: 0.696308970451355\n",
      "Epoch: 1 \t\t\t Iteration 11 Loss Valid: 0.6890847682952881\n",
      "Epoch: 1 \t\t\t Iteration 12 Loss Valid: 0.6926911473274231\n",
      "Epoch: 1 \t\t\t Iteration 13 Loss Valid: 0.6938977837562561\n",
      "Epoch: 1 \t\t\t Iteration 14 Loss Valid: 0.6951040029525757\n",
      "Epoch: 1 \t\t\t Iteration 15 Loss Valid: 0.6951001286506653\n",
      "Epoch: 1 \t\t\t Iteration 16 Loss Valid: 0.6987175941467285\n",
      "Epoch: 1 \t\t\t Iteration 17 Loss Valid: 0.6963099241256714\n",
      "Epoch: 1 \t\t\t Iteration 18 Loss Valid: 0.6950986385345459\n",
      "Epoch: 1 \t\t\t Iteration 19 Loss Valid: 0.6938894987106323\n",
      "Epoch: 1 \t\t\t Iteration 20 Loss Valid: 0.6951037645339966\n",
      "Epoch: 1 \t\t\t Iteration 21 Loss Valid: 0.6975188851356506\n",
      "Epoch: 1 \t\t\t Iteration 22 Loss Valid: 0.7035409808158875\n",
      "Epoch: 1 \t\t\t Iteration 23 Loss Valid: 0.6951056122779846\n",
      "Epoch: 1 \t\t\t Iteration 24 Loss Valid: 0.6926860809326172\n",
      "Epoch: 1 \t\t\t Iteration 25 Loss Valid: 0.6938884258270264\n",
      "Epoch: 1 \t\t\t Iteration 26 Loss Valid: 0.6963086128234863\n",
      "Epoch: 1 \t\t\t Iteration 27 Loss Valid: 0.693881630897522\n",
      "Epoch: 1 \t\t\t Iteration 28 Loss Valid: 0.6926851272583008\n",
      "Epoch: 1 \t\t\t Iteration 29 Loss Valid: 0.6950949430465698\n",
      "Epoch: 1 \t\t\t Iteration 30 Loss Valid: 0.6950973272323608\n",
      "Epoch: 1 \t\t\t Iteration 31 Loss Valid: 0.6975076198577881\n",
      "Epoch: 1 \t\t\t Iteration 32 Loss Valid: 0.6975376009941101\n",
      "Epoch: 1 \t\t\t Iteration 33 Loss Valid: 0.6938847303390503\n",
      "Epoch: 1 \t\t\t Iteration 34 Loss Valid: 0.6950912475585938\n",
      "Epoch: 1 \t\t\t Iteration 35 Loss Valid: 0.6926867961883545\n",
      "Epoch: 1 \t\t\t Iteration 36 Loss Valid: 0.69388747215271\n",
      "Epoch: 1 \t\t\t Iteration 37 Loss Valid: 0.6963149309158325\n",
      "Epoch: 1 \t\t\t Iteration 38 Loss Valid: 0.6950988173484802\n",
      "Epoch: 1 \t\t\t Iteration 39 Loss Valid: 0.696302056312561\n",
      "Epoch: 1 \t\t\t Iteration 40 Loss Valid: 0.6999311447143555\n",
      "Epoch: 1 \t\t\t Iteration 41 Loss Valid: 0.6939005255699158\n",
      "Epoch: 1 \t\t\t Iteration 42 Loss Valid: 0.6987323760986328\n",
      "Epoch: 1 \t\t\t Iteration 43 Loss Valid: 0.7168101072311401\n",
      "Epoch: 1 \t\t\t Iteration 44 Loss Valid: 0.7324380278587341\n",
      "Epoch: 1 \t\t\t Iteration 45 Loss Valid: 0.7324489951133728\n",
      "Epoch: 1 \t\t\t Iteration 46 Loss Valid: 0.732449471950531\n",
      "Epoch: 1 \t\t\t Iteration 47 Loss Valid: 0.7325506210327148\n",
      "Epoch: 1 \t\t\t Iteration 48 Loss Valid: 0.7325394749641418\n",
      "Epoch: 1 \t\t\t Iteration 49 Loss Valid: 0.732490062713623\n",
      "Epoch: 1 \t\t\t Iteration 50 Loss Valid: 0.7325294017791748\n",
      "Epoch: 1 \t\t\t Iteration 51 Loss Valid: 0.7324210405349731\n",
      "Epoch: 1 \t\t\t Iteration 52 Loss Valid: 0.7324742078781128\n",
      "Epoch: 1 \t\t\t Iteration 53 Loss Valid: 0.7325034141540527\n",
      "Epoch: 1 \t\t\t Iteration 54 Loss Valid: 0.6745947599411011\n",
      "Epoch: 1 \t\t\t Iteration 55 Loss Valid: 0.6552813649177551\n",
      "Epoch: 1 \t\t\t Iteration 56 Loss Valid: 0.6552984714508057\n",
      "Epoch: 1 \t\t\t Iteration 57 Loss Valid: 0.6553339958190918\n",
      "Epoch: 1 \t\t\t Iteration 58 Loss Valid: 0.6552784442901611\n",
      "Epoch: 1 \t\t\t Iteration 59 Loss Valid: 0.6553063988685608\n",
      "Epoch: 1 \t\t\t Iteration 60 Loss Valid: 0.6553325653076172\n",
      "Epoch: 1 \t\t\t Iteration 61 Loss Valid: 0.6552804708480835\n",
      "Epoch: 1 / 5 \t\t\t Training Loss:0.8387177539187551\n",
      "Epoch: 2 \t\t\t Iteration 1 Loss Train: 0.6987277269363403\n",
      "Epoch: 2 \t\t\t Iteration 2 Loss Train: 0.6974947452545166\n",
      "Epoch: 2 \t\t\t Iteration 3 Loss Train: 0.6975067853927612\n",
      "Epoch: 2 \t\t\t Iteration 4 Loss Train: 0.6962520480155945\n",
      "Epoch: 2 \t\t\t Iteration 5 Loss Train: 0.6962588429450989\n",
      "Epoch: 2 \t\t\t Iteration 6 Loss Train: 0.6974674463272095\n",
      "Epoch: 2 \t\t\t Iteration 7 Loss Train: 0.6950523257255554\n",
      "Epoch: 2 \t\t\t Iteration 8 Loss Train: 0.699850857257843\n",
      "Epoch: 2 \t\t\t Iteration 9 Loss Train: 0.6962776184082031\n",
      "Epoch: 2 \t\t\t Iteration 10 Loss Train: 0.6938788890838623\n",
      "Epoch: 2 \t\t\t Iteration 11 Loss Train: 0.6926895380020142\n",
      "Epoch: 2 \t\t\t Iteration 12 Loss Train: 0.6974555253982544\n",
      "Epoch: 2 \t\t\t Iteration 13 Loss Train: 0.6974273324012756\n",
      "Epoch: 2 \t\t\t Iteration 14 Loss Train: 0.6997932195663452\n",
      "Epoch: 2 \t\t\t Iteration 15 Loss Train: 0.706890344619751\n",
      "Epoch: 2 \t\t\t Iteration 16 Loss Train: 0.695042073726654\n",
      "Epoch: 2 \t\t\t Iteration 17 Loss Train: 0.6926695704460144\n",
      "Epoch: 2 \t\t\t Iteration 18 Loss Train: 0.6903165578842163\n",
      "Epoch: 2 \t\t\t Iteration 19 Loss Train: 0.6938859224319458\n",
      "Epoch: 2 \t\t\t Iteration 20 Loss Train: 0.7079809308052063\n",
      "Epoch: 2 \t\t\t Iteration 21 Loss Train: 0.6973522901535034\n",
      "Epoch: 2 \t\t\t Iteration 22 Loss Train: 0.692659854888916\n",
      "Epoch: 2 \t\t\t Iteration 23 Loss Train: 0.6950172781944275\n",
      "Epoch: 2 \t\t\t Iteration 24 Loss Train: 0.7020574808120728\n",
      "Epoch: 2 \t\t\t Iteration 25 Loss Train: 0.6903481483459473\n",
      "Epoch: 2 \t\t\t Iteration 26 Loss Train: 0.6845070123672485\n",
      "Epoch: 2 \t\t\t Iteration 27 Loss Train: 0.6985353231430054\n",
      "Epoch: 2 \t\t\t Iteration 28 Loss Train: 0.7031756639480591\n",
      "Epoch: 2 \t\t\t Iteration 29 Loss Train: 0.6961578130722046\n",
      "Epoch: 2 \t\t\t Iteration 30 Loss Train: 0.6950113773345947\n",
      "Epoch: 2 \t\t\t Iteration 31 Loss Train: 0.6973264217376709\n",
      "Epoch: 2 \t\t\t Iteration 32 Loss Train: 0.6950179934501648\n",
      "Epoch: 2 \t\t\t Iteration 33 Loss Train: 0.6880608797073364\n",
      "Epoch: 2 \t\t\t Iteration 34 Loss Train: 0.6903762817382812\n",
      "Epoch: 2 \t\t\t Iteration 35 Loss Train: 0.6961662769317627\n",
      "Epoch: 2 \t\t\t Iteration 36 Loss Train: 0.7031028866767883\n",
      "Epoch: 2 \t\t\t Iteration 37 Loss Train: 0.7018958926200867\n",
      "Epoch: 2 \t\t\t Iteration 38 Loss Train: 0.6938300132751465\n",
      "Epoch: 2 \t\t\t Iteration 39 Loss Train: 0.7030376195907593\n",
      "Epoch: 2 \t\t\t Iteration 40 Loss Train: 0.6961179971694946\n",
      "Epoch: 2 \t\t\t Iteration 41 Loss Train: 0.6961084008216858\n",
      "Epoch: 2 \t\t\t Iteration 42 Loss Train: 0.6915463209152222\n",
      "Epoch: 2 \t\t\t Iteration 43 Loss Train: 0.6949654817581177\n",
      "Epoch: 2 \t\t\t Iteration 44 Loss Train: 0.6983662843704224\n",
      "Epoch: 2 \t\t\t Iteration 45 Loss Train: 0.6949613094329834\n",
      "Epoch: 2 \t\t\t Iteration 46 Loss Train: 0.6949645280838013\n",
      "Epoch: 2 \t\t\t Iteration 47 Loss Train: 0.7017793655395508\n",
      "Epoch: 2 \t\t\t Iteration 48 Loss Train: 0.7062805891036987\n",
      "Epoch: 2 \t\t\t Iteration 49 Loss Train: 0.6892826557159424\n",
      "Epoch: 2 \t\t\t Iteration 50 Loss Train: 0.6915568709373474\n",
      "Epoch: 2 \t\t\t Iteration 51 Loss Train: 0.7005919814109802\n",
      "Epoch: 2 \t\t\t Iteration 52 Loss Train: 0.6982986927032471\n",
      "Epoch: 2 \t\t\t Iteration 53 Loss Train: 0.6893114447593689\n",
      "Epoch: 2 \t\t\t Iteration 54 Loss Train: 0.6994244456291199\n",
      "Epoch: 2 \t\t\t Iteration 55 Loss Train: 0.6915669441223145\n",
      "Epoch: 2 \t\t\t Iteration 56 Loss Train: 0.7005301713943481\n",
      "Epoch: 2 \t\t\t Iteration 57 Loss Train: 0.6926664113998413\n",
      "Epoch: 2 \t\t\t Iteration 58 Loss Train: 0.6926833391189575\n",
      "Epoch: 2 \t\t\t Iteration 59 Loss Train: 0.6994158029556274\n",
      "Epoch: 2 \t\t\t Iteration 60 Loss Train: 0.696029543876648\n",
      "Epoch: 2 \t\t\t Iteration 61 Loss Train: 0.6960130929946899\n",
      "Epoch: 2 \t\t\t Iteration 62 Loss Train: 0.6937993764877319\n",
      "Epoch: 2 \t\t\t Iteration 63 Loss Train: 0.6971263885498047\n",
      "Epoch: 2 \t\t\t Iteration 64 Loss Train: 0.7004814743995667\n",
      "Epoch: 2 \t\t\t Iteration 65 Loss Train: 0.6959981322288513\n",
      "Epoch: 2 \t\t\t Iteration 66 Loss Train: 0.701509952545166\n",
      "Epoch: 2 \t\t\t Iteration 67 Loss Train: 0.6992993354797363\n",
      "Epoch: 2 \t\t\t Iteration 68 Loss Train: 0.6970542669296265\n",
      "Epoch: 2 \t\t\t Iteration 69 Loss Train: 0.7003716826438904\n",
      "Epoch: 2 \t\t\t Iteration 70 Loss Train: 0.6938023567199707\n",
      "Epoch: 2 \t\t\t Iteration 71 Loss Train: 0.6981486678123474\n",
      "Epoch: 2 \t\t\t Iteration 72 Loss Train: 0.6937558054924011\n",
      "Epoch: 2 \t\t\t Iteration 73 Loss Train: 0.7003183364868164\n",
      "Epoch: 2 \t\t\t Iteration 74 Loss Train: 0.6948575973510742\n",
      "Epoch: 2 \t\t\t Iteration 75 Loss Train: 0.6937479972839355\n",
      "Epoch: 2 \t\t\t Iteration 76 Loss Train: 0.6948418021202087\n",
      "Epoch: 2 \t\t\t Iteration 77 Loss Train: 0.6959288120269775\n",
      "Epoch: 2 \t\t\t Iteration 78 Loss Train: 0.6948559284210205\n",
      "Epoch: 2 \t\t\t Iteration 79 Loss Train: 0.7002760171890259\n",
      "Epoch: 2 \t\t\t Iteration 80 Loss Train: 0.6959138512611389\n",
      "Epoch: 2 \t\t\t Iteration 81 Loss Train: 0.7002255916595459\n",
      "Epoch: 2 \t\t\t Iteration 82 Loss Train: 0.7034464478492737\n",
      "Epoch: 2 \t\t\t Iteration 83 Loss Train: 0.6937609910964966\n",
      "Epoch: 2 \t\t\t Iteration 84 Loss Train: 0.6851357221603394\n",
      "Epoch: 2 \t\t\t Iteration 85 Loss Train: 0.6991361975669861\n",
      "Epoch: 2 \t\t\t Iteration 86 Loss Train: 0.693764328956604\n",
      "Epoch: 2 \t\t\t Iteration 87 Loss Train: 0.6937271952629089\n",
      "Epoch: 2 \t\t\t Iteration 88 Loss Train: 0.6958723068237305\n",
      "Epoch: 2 \t\t\t Iteration 89 Loss Train: 0.7022992968559265\n",
      "Epoch: 2 \t\t\t Iteration 90 Loss Train: 0.6958694458007812\n",
      "Epoch: 2 \t\t\t Iteration 91 Loss Train: 0.6947964429855347\n",
      "Epoch: 2 \t\t\t Iteration 92 Loss Train: 0.6916004419326782\n",
      "Epoch: 2 \t\t\t Iteration 93 Loss Train: 0.6990309357643127\n",
      "Epoch: 2 \t\t\t Iteration 94 Loss Train: 0.6947987079620361\n",
      "Epoch: 2 \t\t\t Iteration 95 Loss Train: 0.6926518678665161\n",
      "Epoch: 2 \t\t\t Iteration 96 Loss Train: 0.6926494836807251\n",
      "Epoch: 2 \t\t\t Iteration 97 Loss Train: 0.6948094367980957\n",
      "Epoch: 2 \t\t\t Iteration 98 Loss Train: 0.6990448236465454\n",
      "Epoch: 2 \t\t\t Iteration 99 Loss Train: 0.695813775062561\n",
      "Epoch: 2 \t\t\t Iteration 100 Loss Train: 0.6937252879142761\n",
      "Epoch: 2 \t\t\t Iteration 101 Loss Train: 0.6937240362167358\n",
      "Epoch: 2 \t\t\t Iteration 102 Loss Train: 0.6895115971565247\n",
      "Epoch: 2 \t\t\t Iteration 103 Loss Train: 0.6979491710662842\n",
      "Epoch: 2 \t\t\t Iteration 104 Loss Train: 0.6895002126693726\n",
      "Epoch: 2 \t\t\t Iteration 105 Loss Train: 0.6915831565856934\n",
      "Epoch: 2 \t\t\t Iteration 106 Loss Train: 0.7000715136528015\n",
      "Epoch: 2 \t\t\t Iteration 107 Loss Train: 0.702151358127594\n",
      "Epoch: 2 \t\t\t Iteration 108 Loss Train: 0.6905924081802368\n",
      "Epoch: 2 \t\t\t Iteration 109 Loss Train: 0.6916146874427795\n",
      "Epoch: 2 \t\t\t Iteration 110 Loss Train: 0.6926673650741577\n",
      "Epoch: 2 \t\t\t Iteration 111 Loss Train: 0.7031774520874023\n",
      "Epoch: 2 \t\t\t Iteration 112 Loss Train: 0.6937094330787659\n",
      "Epoch: 2 \t\t\t Iteration 113 Loss Train: 0.6926661729812622\n",
      "Epoch: 2 \t\t\t Iteration 114 Loss Train: 0.6926886439323425\n",
      "Epoch: 2 \t\t\t Iteration 115 Loss Train: 0.7031504511833191\n",
      "Epoch: 2 \t\t\t Iteration 116 Loss Train: 0.6947146654129028\n",
      "Epoch: 2 \t\t\t Iteration 117 Loss Train: 0.6968224048614502\n",
      "Epoch: 2 \t\t\t Iteration 118 Loss Train: 0.6926455497741699\n",
      "Epoch: 2 \t\t\t Iteration 119 Loss Train: 0.7020111680030823\n",
      "Epoch: 2 \t\t\t Iteration 120 Loss Train: 0.6968023777008057\n",
      "Epoch: 2 \t\t\t Iteration 121 Loss Train: 0.6967947483062744\n",
      "Epoch: 2 \t\t\t Iteration 122 Loss Train: 0.6957586407661438\n",
      "Epoch: 2 \t\t\t Iteration 123 Loss Train: 0.6802784204483032\n",
      "Epoch: 2 \t\t\t Iteration 124 Loss Train: 0.6957664489746094\n",
      "Epoch: 2 \t\t\t Iteration 125 Loss Train: 0.6957435011863708\n",
      "Epoch: 2 \t\t\t Iteration 126 Loss Train: 0.6926445960998535\n",
      "Epoch: 2 \t\t\t Iteration 127 Loss Train: 0.6978263258934021\n",
      "Epoch: 2 \t\t\t Iteration 128 Loss Train: 0.694717288017273\n",
      "Epoch: 2 \t\t\t Iteration 129 Loss Train: 0.6916337013244629\n",
      "Epoch: 2 \t\t\t Iteration 130 Loss Train: 0.6968189477920532\n",
      "Epoch: 2 \t\t\t Iteration 131 Loss Train: 0.7019188404083252\n",
      "Epoch: 2 \t\t\t Iteration 132 Loss Train: 0.69986891746521\n",
      "Epoch: 2 \t\t\t Iteration 133 Loss Train: 0.6967539191246033\n",
      "Epoch: 2 \t\t\t Iteration 134 Loss Train: 0.6947023868560791\n",
      "Epoch: 2 \t\t\t Iteration 135 Loss Train: 0.6926746964454651\n",
      "Epoch: 2 \t\t\t Iteration 136 Loss Train: 0.6977710723876953\n",
      "Epoch: 2 \t\t\t Iteration 137 Loss Train: 0.7038548588752747\n",
      "Epoch: 2 \t\t\t Iteration 138 Loss Train: 0.7038087844848633\n",
      "Epoch: 2 \t\t\t Iteration 139 Loss Train: 0.6966853737831116\n",
      "Epoch: 2 \t\t\t Iteration 140 Loss Train: 0.6946702003479004\n",
      "Epoch: 2 \t\t\t Iteration 141 Loss Train: 0.6986861824989319\n",
      "Epoch: 2 \t\t\t Iteration 142 Loss Train: 0.6996721029281616\n",
      "Epoch: 2 \t\t\t Iteration 143 Loss Train: 0.6946823596954346\n",
      "Epoch: 2 \t\t\t Iteration 144 Loss Train: 0.6946585178375244\n",
      "Epoch: 2 \t\t\t Iteration 145 Loss Train: 0.7006362676620483\n",
      "Epoch: 2 \t\t\t Iteration 146 Loss Train: 0.6916554570198059\n",
      "Epoch: 2 \t\t\t Iteration 147 Loss Train: 0.6946439743041992\n",
      "Epoch: 2 \t\t\t Iteration 148 Loss Train: 0.700613260269165\n",
      "Epoch: 2 \t\t\t Iteration 149 Loss Train: 0.6936669945716858\n",
      "Epoch: 2 \t\t\t Iteration 150 Loss Train: 0.690680742263794\n",
      "Epoch: 2 \t\t\t Iteration 151 Loss Train: 0.6926432847976685\n",
      "Epoch: 2 \t\t\t Iteration 152 Loss Train: 0.6926482319831848\n",
      "Epoch: 2 \t\t\t Iteration 153 Loss Train: 0.6906839609146118\n",
      "Epoch: 2 \t\t\t Iteration 154 Loss Train: 0.693652868270874\n",
      "Epoch: 2 \t\t\t Iteration 155 Loss Train: 0.6936397552490234\n",
      "Epoch: 2 \t\t\t Iteration 156 Loss Train: 0.6936432123184204\n",
      "Epoch: 2 \t\t\t Iteration 157 Loss Train: 0.701575517654419\n",
      "Epoch: 2 \t\t\t Iteration 158 Loss Train: 0.6916635632514954\n",
      "Epoch: 2 \t\t\t Iteration 159 Loss Train: 0.7005405426025391\n",
      "Epoch: 2 \t\t\t Iteration 160 Loss Train: 0.7005542516708374\n",
      "Epoch: 2 \t\t\t Iteration 161 Loss Train: 0.7044190168380737\n",
      "Epoch: 2 \t\t\t Iteration 162 Loss Train: 0.6907024383544922\n",
      "Epoch: 2 \t\t\t Iteration 163 Loss Train: 0.6897384524345398\n",
      "Epoch: 2 \t\t\t Iteration 164 Loss Train: 0.6985316276550293\n",
      "Epoch: 2 \t\t\t Iteration 165 Loss Train: 0.6945947408676147\n",
      "Epoch: 2 \t\t\t Iteration 166 Loss Train: 0.6916927099227905\n",
      "Epoch: 2 \t\t\t Iteration 167 Loss Train: 0.6994730234146118\n",
      "Epoch: 2 \t\t\t Iteration 168 Loss Train: 0.7023721933364868\n",
      "Epoch: 2 \t\t\t Iteration 169 Loss Train: 0.7003697156906128\n",
      "Epoch: 2 \t\t\t Iteration 170 Loss Train: 0.6916934251785278\n",
      "Epoch: 2 \t\t\t Iteration 171 Loss Train: 0.6984319686889648\n",
      "Epoch: 2 \t\t\t Iteration 172 Loss Train: 0.6945942640304565\n",
      "Epoch: 2 \t\t\t Iteration 173 Loss Train: 0.6993669867515564\n",
      "Epoch: 2 \t\t\t Iteration 174 Loss Train: 0.6974406242370605\n",
      "Epoch: 2 \t\t\t Iteration 175 Loss Train: 0.6878962516784668\n",
      "Epoch: 2 \t\t\t Iteration 176 Loss Train: 0.7012751698493958\n",
      "Epoch: 2 \t\t\t Iteration 177 Loss Train: 0.6955167055130005\n",
      "Epoch: 2 \t\t\t Iteration 178 Loss Train: 0.6936225891113281\n",
      "Epoch: 2 \t\t\t Iteration 179 Loss Train: 0.6945702433586121\n",
      "Epoch: 2 \t\t\t Iteration 180 Loss Train: 0.69267338514328\n",
      "Epoch: 2 \t\t\t Iteration 181 Loss Train: 0.6974048614501953\n",
      "Epoch: 2 \t\t\t Iteration 182 Loss Train: 0.6898390054702759\n",
      "Epoch: 2 \t\t\t Iteration 183 Loss Train: 0.6945580244064331\n",
      "Epoch: 2 \t\t\t Iteration 184 Loss Train: 0.6964401006698608\n",
      "Epoch: 2 \t\t\t Iteration 185 Loss Train: 0.693600594997406\n",
      "Epoch: 2 \t\t\t Iteration 186 Loss Train: 0.6954737901687622\n",
      "Epoch: 2 \t\t\t Iteration 187 Loss Train: 0.7011629343032837\n",
      "Epoch: 2 \t\t\t Iteration 188 Loss Train: 0.6917245388031006\n",
      "Epoch: 2 \t\t\t Iteration 189 Loss Train: 0.7020740509033203\n",
      "Epoch: 2 \t\t\t Iteration 190 Loss Train: 0.6917209625244141\n",
      "Epoch: 2 \t\t\t Iteration 191 Loss Train: 0.7020171284675598\n",
      "Epoch: 2 \t\t\t Iteration 192 Loss Train: 0.69172602891922\n",
      "Epoch: 2 \t\t\t Iteration 193 Loss Train: 0.6963875889778137\n",
      "Epoch: 2 \t\t\t Iteration 194 Loss Train: 0.6982119083404541\n",
      "Epoch: 2 \t\t\t Iteration 195 Loss Train: 0.7038004398345947\n",
      "Epoch: 2 \t\t\t Iteration 196 Loss Train: 0.69450843334198\n",
      "Epoch: 2 \t\t\t Iteration 197 Loss Train: 0.7018837928771973\n",
      "Epoch: 2 \t\t\t Iteration 198 Loss Train: 0.6899197697639465\n",
      "Epoch: 2 \t\t\t Iteration 199 Loss Train: 0.6981744766235352\n",
      "Epoch: 2 \t\t\t Iteration 200 Loss Train: 0.69542396068573\n",
      "Epoch: 2 \t\t\t Iteration 201 Loss Train: 0.7008892297744751\n",
      "Epoch: 2 \t\t\t Iteration 202 Loss Train: 0.6972144842147827\n",
      "Epoch: 2 \t\t\t Iteration 203 Loss Train: 0.6990185976028442\n",
      "Epoch: 2 \t\t\t Iteration 204 Loss Train: 0.6972004175186157\n",
      "Epoch: 2 \t\t\t Iteration 205 Loss Train: 0.6980936527252197\n",
      "Epoch: 2 \t\t\t Iteration 206 Loss Train: 0.6998728513717651\n",
      "Epoch: 2 \t\t\t Iteration 207 Loss Train: 0.6953656673431396\n",
      "Epoch: 2 \t\t\t Iteration 208 Loss Train: 0.6926700472831726\n",
      "Epoch: 2 \t\t\t Iteration 209 Loss Train: 0.7007525563240051\n",
      "Epoch: 2 \t\t\t Iteration 210 Loss Train: 0.7042664885520935\n",
      "Epoch: 2 \t\t\t Iteration 211 Loss Train: 0.6882224082946777\n",
      "Epoch: 2 \t\t\t Iteration 212 Loss Train: 0.7024586796760559\n",
      "Epoch: 2 \t\t\t Iteration 213 Loss Train: 0.694427490234375\n",
      "Epoch: 2 \t\t\t Iteration 214 Loss Train: 0.6953266859054565\n",
      "Epoch: 2 \t\t\t Iteration 215 Loss Train: 0.6944198608398438\n",
      "Epoch: 2 \t\t\t Iteration 216 Loss Train: 0.697952151298523\n",
      "Epoch: 2 \t\t\t Iteration 217 Loss Train: 0.7023298740386963\n",
      "Epoch: 2 \t\t\t Iteration 218 Loss Train: 0.698800265789032\n",
      "Epoch: 2 \t\t\t Iteration 219 Loss Train: 0.6987607479095459\n",
      "Epoch: 2 \t\t\t Iteration 220 Loss Train: 0.7013698220252991\n",
      "Epoch: 2 \t\t\t Iteration 221 Loss Train: 0.7004526853561401\n",
      "Epoch: 2 \t\t\t Iteration 222 Loss Train: 0.6926833391189575\n",
      "Epoch: 2 \t\t\t Iteration 223 Loss Train: 0.6943966746330261\n",
      "Epoch: 2 \t\t\t Iteration 224 Loss Train: 0.6986849308013916\n",
      "Epoch: 2 \t\t\t Iteration 225 Loss Train: 0.6986672878265381\n",
      "Epoch: 2 \t\t\t Iteration 226 Loss Train: 0.6943917274475098\n",
      "Epoch: 2 \t\t\t Iteration 227 Loss Train: 0.6952309608459473\n",
      "Epoch: 2 \t\t\t Iteration 228 Loss Train: 0.6986115574836731\n",
      "Epoch: 2 \t\t\t Iteration 229 Loss Train: 0.6977581977844238\n",
      "Epoch: 2 \t\t\t Iteration 230 Loss Train: 0.6935087442398071\n",
      "Epoch: 2 \t\t\t Iteration 231 Loss Train: 0.6968849301338196\n",
      "Epoch: 2 \t\t\t Iteration 232 Loss Train: 0.6985747814178467\n",
      "Epoch: 2 \t\t\t Iteration 233 Loss Train: 0.692652702331543\n",
      "Epoch: 2 \t\t\t Iteration 234 Loss Train: 0.7002770900726318\n",
      "Epoch: 2 \t\t\t Iteration 235 Loss Train: 0.6968608498573303\n",
      "Epoch: 2 \t\t\t Iteration 236 Loss Train: 0.6935049295425415\n",
      "Epoch: 2 \t\t\t Iteration 237 Loss Train: 0.6993309259414673\n",
      "Epoch: 2 \t\t\t Iteration 238 Loss Train: 0.6943434476852417\n",
      "Epoch: 2 \t\t\t Iteration 239 Loss Train: 0.6918542385101318\n",
      "Epoch: 2 \t\t\t Iteration 240 Loss Train: 0.6968191862106323\n",
      "Epoch: 2 \t\t\t Iteration 241 Loss Train: 0.6918361186981201\n",
      "Epoch: 2 \t\t\t Iteration 242 Loss Train: 0.6885294318199158\n",
      "Epoch: 2 \t\t\t Iteration 243 Loss Train: 0.6959725618362427\n",
      "Epoch: 2 \t\t\t Iteration 244 Loss Train: 0.6951741576194763\n",
      "Epoch: 2 \t\t\t Iteration 245 Loss Train: 0.6918424367904663\n",
      "Epoch: 2 \t\t\t Iteration 246 Loss Train: 0.6968594193458557\n",
      "Epoch: 2 \t\t\t Iteration 247 Loss Train: 0.694340169429779\n",
      "Epoch: 2 \t\t\t Iteration 248 Loss Train: 0.6984573602676392\n",
      "Epoch: 2 \t\t\t Iteration 249 Loss Train: 0.6918529272079468\n",
      "Epoch: 2 \t\t\t Iteration 250 Loss Train: 0.6992805004119873\n",
      "Epoch: 2 \t\t\t Iteration 251 Loss Train: 0.69511479139328\n",
      "Epoch: 2 \t\t\t Iteration 252 Loss Train: 0.6926544308662415\n",
      "Epoch: 2 \t\t\t Iteration 253 Loss Train: 0.6959546804428101\n",
      "Epoch: 2 \t\t\t Iteration 254 Loss Train: 0.6984121799468994\n",
      "Epoch: 2 \t\t\t Iteration 255 Loss Train: 0.6934978365898132\n",
      "Epoch: 2 \t\t\t Iteration 256 Loss Train: 0.6991878747940063\n",
      "Epoch: 2 \t\t\t Iteration 257 Loss Train: 0.6999725103378296\n",
      "Epoch: 2 \t\t\t Iteration 258 Loss Train: 0.696720540523529\n",
      "Epoch: 2 \t\t\t Iteration 259 Loss Train: 0.6959055066108704\n",
      "Epoch: 2 \t\t\t Iteration 260 Loss Train: 0.6958825588226318\n",
      "Epoch: 2 \t\t\t Iteration 261 Loss Train: 0.694266676902771\n",
      "Epoch: 2 \t\t\t Iteration 262 Loss Train: 0.7006974220275879\n",
      "Epoch: 2 \t\t\t Iteration 263 Loss Train: 0.6974684596061707\n",
      "Epoch: 2 \t\t\t Iteration 264 Loss Train: 0.6974319219589233\n",
      "Epoch: 2 \t\t\t Iteration 265 Loss Train: 0.6950523853302002\n",
      "Epoch: 2 \t\t\t Iteration 266 Loss Train: 0.6974157691001892\n",
      "Epoch: 2 \t\t\t Iteration 267 Loss Train: 0.6950308084487915\n",
      "Epoch: 2 \t\t\t Iteration 268 Loss Train: 0.6958118677139282\n",
      "Epoch: 2 \t\t\t Iteration 269 Loss Train: 0.6958190202713013\n",
      "Epoch: 2 \t\t\t Iteration 270 Loss Train: 0.6966128349304199\n",
      "Epoch: 2 \t\t\t Iteration 271 Loss Train: 0.6958009004592896\n",
      "Epoch: 2 \t\t\t Iteration 272 Loss Train: 0.6926684975624084\n",
      "Epoch: 2 \t\t\t Iteration 273 Loss Train: 0.6973723769187927\n",
      "Epoch: 2 \t\t\t Iteration 274 Loss Train: 0.6957739591598511\n",
      "Epoch: 2 \t\t\t Iteration 275 Loss Train: 0.6934516429901123\n",
      "Epoch: 2 \t\t\t Iteration 276 Loss Train: 0.7004508972167969\n",
      "Epoch: 2 \t\t\t Iteration 277 Loss Train: 0.6973142623901367\n",
      "Epoch: 2 \t\t\t Iteration 278 Loss Train: 0.6988418698310852\n",
      "Epoch: 2 \t\t\t Iteration 279 Loss Train: 0.6973006725311279\n",
      "Epoch: 2 \t\t\t Iteration 280 Loss Train: 0.6881204843521118\n",
      "Epoch: 2 \t\t\t Iteration 281 Loss Train: 0.6873352527618408\n",
      "Epoch: 2 \t\t\t Iteration 282 Loss Train: 0.6973023414611816\n",
      "Epoch: 2 \t\t\t Iteration 283 Loss Train: 0.6942281126976013\n",
      "Epoch: 2 \t\t\t Iteration 284 Loss Train: 0.6949665546417236\n",
      "Epoch: 2 \t\t\t Iteration 285 Loss Train: 0.6919091939926147\n",
      "Epoch: 2 \t\t\t Iteration 286 Loss Train: 0.6980268955230713\n",
      "Epoch: 2 \t\t\t Iteration 287 Loss Train: 0.6949433088302612\n",
      "Epoch: 2 \t\t\t Iteration 288 Loss Train: 0.6934397220611572\n",
      "Epoch: 2 \t\t\t Iteration 289 Loss Train: 0.6858652830123901\n",
      "Epoch: 2 \t\t\t Iteration 290 Loss Train: 0.6988106369972229\n",
      "Epoch: 2 \t\t\t Iteration 291 Loss Train: 0.6979858875274658\n",
      "Epoch: 2 \t\t\t Iteration 292 Loss Train: 0.6903907060623169\n",
      "Epoch: 2 \t\t\t Iteration 293 Loss Train: 0.7002686858177185\n",
      "Epoch: 2 \t\t\t Iteration 294 Loss Train: 0.6949628591537476\n",
      "Epoch: 2 \t\t\t Iteration 295 Loss Train: 0.6941808462142944\n",
      "Epoch: 2 \t\t\t Iteration 296 Loss Train: 0.6941753029823303\n",
      "Epoch: 2 \t\t\t Iteration 297 Loss Train: 0.7002084255218506\n",
      "Epoch: 2 \t\t\t Iteration 298 Loss Train: 0.6949360370635986\n",
      "Epoch: 2 \t\t\t Iteration 299 Loss Train: 0.6934135556221008\n",
      "Epoch: 2 \t\t\t Iteration 300 Loss Train: 0.6971540451049805\n",
      "Epoch: 2 \t\t\t Iteration 301 Loss Train: 0.6964130997657776\n",
      "Epoch: 2 \t\t\t Iteration 302 Loss Train: 0.6961988210678101\n",
      "Epoch: 2 \t\t\t Iteration 1 Loss Valid: 0.6941787600517273\n",
      "Epoch: 2 \t\t\t Iteration 2 Loss Valid: 0.6934137344360352\n",
      "Epoch: 2 \t\t\t Iteration 3 Loss Valid: 0.6926934719085693\n",
      "Epoch: 2 \t\t\t Iteration 4 Loss Valid: 0.6941602230072021\n",
      "Epoch: 2 \t\t\t Iteration 5 Loss Valid: 0.6934311389923096\n",
      "Epoch: 2 \t\t\t Iteration 6 Loss Valid: 0.6911978721618652\n",
      "Epoch: 2 \t\t\t Iteration 7 Loss Valid: 0.6941654086112976\n",
      "Epoch: 2 \t\t\t Iteration 8 Loss Valid: 0.6926920413970947\n",
      "Epoch: 2 \t\t\t Iteration 9 Loss Valid: 0.6941631436347961\n",
      "Epoch: 2 \t\t\t Iteration 10 Loss Valid: 0.6949092149734497\n",
      "Epoch: 2 \t\t\t Iteration 11 Loss Valid: 0.6904869079589844\n",
      "Epoch: 2 \t\t\t Iteration 12 Loss Valid: 0.6926947832107544\n",
      "Epoch: 2 \t\t\t Iteration 13 Loss Valid: 0.6934328079223633\n",
      "Epoch: 2 \t\t\t Iteration 14 Loss Valid: 0.6941711902618408\n",
      "Epoch: 2 \t\t\t Iteration 15 Loss Valid: 0.694169819355011\n",
      "Epoch: 2 \t\t\t Iteration 16 Loss Valid: 0.6963867545127869\n",
      "Epoch: 2 \t\t\t Iteration 17 Loss Valid: 0.6949118375778198\n",
      "Epoch: 2 \t\t\t Iteration 18 Loss Valid: 0.6941673755645752\n",
      "Epoch: 2 \t\t\t Iteration 19 Loss Valid: 0.6934265494346619\n",
      "Epoch: 2 \t\t\t Iteration 20 Loss Valid: 0.694172203540802\n",
      "Epoch: 2 \t\t\t Iteration 21 Loss Valid: 0.6956534385681152\n",
      "Epoch: 2 \t\t\t Iteration 22 Loss Valid: 0.6993433237075806\n",
      "Epoch: 2 \t\t\t Iteration 23 Loss Valid: 0.6941748261451721\n",
      "Epoch: 2 \t\t\t Iteration 24 Loss Valid: 0.6926902532577515\n",
      "Epoch: 2 \t\t\t Iteration 25 Loss Valid: 0.6934250593185425\n",
      "Epoch: 2 \t\t\t Iteration 26 Loss Valid: 0.6949114799499512\n",
      "Epoch: 2 \t\t\t Iteration 27 Loss Valid: 0.6934176087379456\n",
      "Epoch: 2 \t\t\t Iteration 28 Loss Valid: 0.6926887035369873\n",
      "Epoch: 2 \t\t\t Iteration 29 Loss Valid: 0.6941643953323364\n",
      "Epoch: 2 \t\t\t Iteration 30 Loss Valid: 0.6941659450531006\n",
      "Epoch: 2 \t\t\t Iteration 31 Loss Valid: 0.6956444382667542\n",
      "Epoch: 2 \t\t\t Iteration 32 Loss Valid: 0.6956710815429688\n",
      "Epoch: 2 \t\t\t Iteration 33 Loss Valid: 0.693421483039856\n",
      "Epoch: 2 \t\t\t Iteration 34 Loss Valid: 0.694161057472229\n",
      "Epoch: 2 \t\t\t Iteration 35 Loss Valid: 0.6926888227462769\n",
      "Epoch: 2 \t\t\t Iteration 36 Loss Valid: 0.6934241652488708\n",
      "Epoch: 2 \t\t\t Iteration 37 Loss Valid: 0.6949164867401123\n",
      "Epoch: 2 \t\t\t Iteration 38 Loss Valid: 0.69416743516922\n",
      "Epoch: 2 \t\t\t Iteration 39 Loss Valid: 0.6949042081832886\n",
      "Epoch: 2 \t\t\t Iteration 40 Loss Valid: 0.697134256362915\n",
      "Epoch: 2 \t\t\t Iteration 41 Loss Valid: 0.6934337615966797\n",
      "Epoch: 2 \t\t\t Iteration 42 Loss Valid: 0.6964031457901001\n",
      "Epoch: 2 \t\t\t Iteration 43 Loss Valid: 0.7074825763702393\n",
      "Epoch: 2 \t\t\t Iteration 44 Loss Valid: 0.7170482873916626\n",
      "Epoch: 2 \t\t\t Iteration 45 Loss Valid: 0.7170581817626953\n",
      "Epoch: 2 \t\t\t Iteration 46 Loss Valid: 0.7170577049255371\n",
      "Epoch: 2 \t\t\t Iteration 47 Loss Valid: 0.7171580791473389\n",
      "Epoch: 2 \t\t\t Iteration 48 Loss Valid: 0.7171474099159241\n",
      "Epoch: 2 \t\t\t Iteration 49 Loss Valid: 0.7170999646186829\n",
      "Epoch: 2 \t\t\t Iteration 50 Loss Valid: 0.7171381711959839\n",
      "Epoch: 2 \t\t\t Iteration 51 Loss Valid: 0.7170311212539673\n",
      "Epoch: 2 \t\t\t Iteration 52 Loss Valid: 0.7170832753181458\n",
      "Epoch: 2 \t\t\t Iteration 53 Loss Valid: 0.717111349105835\n",
      "Epoch: 2 \t\t\t Iteration 54 Loss Valid: 0.6815928816795349\n",
      "Epoch: 2 \t\t\t Iteration 55 Loss Valid: 0.6697419881820679\n",
      "Epoch: 2 \t\t\t Iteration 56 Loss Valid: 0.6697617173194885\n",
      "Epoch: 2 \t\t\t Iteration 57 Loss Valid: 0.6697972416877747\n",
      "Epoch: 2 \t\t\t Iteration 58 Loss Valid: 0.6697397232055664\n",
      "Epoch: 2 \t\t\t Iteration 59 Loss Valid: 0.6697695255279541\n",
      "Epoch: 2 \t\t\t Iteration 60 Loss Valid: 0.6697960495948792\n",
      "Epoch: 2 \t\t\t Iteration 61 Loss Valid: 0.6697419881820679\n",
      "Epoch: 2 / 5 \t\t\t Training Loss:0.8364958038787969\n",
      "Epoch: 3 \t\t\t Iteration 1 Loss Train: 0.6978856325149536\n",
      "Epoch: 3 \t\t\t Iteration 2 Loss Train: 0.6948949098587036\n",
      "Epoch: 3 \t\t\t Iteration 3 Loss Train: 0.6934422254562378\n",
      "Epoch: 3 \t\t\t Iteration 4 Loss Train: 0.7014952898025513\n",
      "Epoch: 3 \t\t\t Iteration 5 Loss Train: 0.6963341236114502\n",
      "Epoch: 3 \t\t\t Iteration 6 Loss Train: 0.6919648051261902\n",
      "Epoch: 3 \t\t\t Iteration 7 Loss Train: 0.6963325142860413\n",
      "Epoch: 3 \t\t\t Iteration 8 Loss Train: 0.6956236362457275\n",
      "Epoch: 3 \t\t\t Iteration 9 Loss Train: 0.6948844194412231\n",
      "Epoch: 3 \t\t\t Iteration 10 Loss Train: 0.6948646903038025\n",
      "Epoch: 3 \t\t\t Iteration 11 Loss Train: 0.6927051544189453\n",
      "Epoch: 3 \t\t\t Iteration 12 Loss Train: 0.6984667778015137\n",
      "Epoch: 3 \t\t\t Iteration 13 Loss Train: 0.6927013397216797\n",
      "Epoch: 3 \t\t\t Iteration 14 Loss Train: 0.6948515772819519\n",
      "Epoch: 3 \t\t\t Iteration 15 Loss Train: 0.6883949041366577\n",
      "Epoch: 3 \t\t\t Iteration 16 Loss Train: 0.6991991996765137\n",
      "Epoch: 3 \t\t\t Iteration 17 Loss Train: 0.694144606590271\n",
      "Epoch: 3 \t\t\t Iteration 18 Loss Train: 0.6926589608192444\n",
      "Epoch: 3 \t\t\t Iteration 19 Loss Train: 0.7012858390808105\n",
      "Epoch: 3 \t\t\t Iteration 20 Loss Train: 0.6962687373161316\n",
      "Epoch: 3 \t\t\t Iteration 21 Loss Train: 0.6990879774093628\n",
      "Epoch: 3 \t\t\t Iteration 22 Loss Train: 0.692005455493927\n",
      "Epoch: 3 \t\t\t Iteration 23 Loss Train: 0.7018656134605408\n",
      "Epoch: 3 \t\t\t Iteration 24 Loss Train: 0.6927004456520081\n",
      "Epoch: 3 \t\t\t Iteration 25 Loss Train: 0.6927129030227661\n",
      "Epoch: 3 \t\t\t Iteration 26 Loss Train: 0.6990187168121338\n",
      "Epoch: 3 \t\t\t Iteration 27 Loss Train: 0.6975666284561157\n",
      "Epoch: 3 \t\t\t Iteration 28 Loss Train: 0.6941107511520386\n",
      "Epoch: 3 \t\t\t Iteration 29 Loss Train: 0.6947997212409973\n",
      "Epoch: 3 \t\t\t Iteration 30 Loss Train: 0.6996206045150757\n",
      "Epoch: 3 \t\t\t Iteration 31 Loss Train: 0.6933907270431519\n",
      "Epoch: 3 \t\t\t Iteration 32 Loss Train: 0.6982089281082153\n",
      "Epoch: 3 \t\t\t Iteration 33 Loss Train: 0.7016217708587646\n",
      "Epoch: 3 \t\t\t Iteration 34 Loss Train: 0.6920456886291504\n",
      "Epoch: 3 \t\t\t Iteration 35 Loss Train: 0.6906902194023132\n",
      "Epoch: 3 \t\t\t Iteration 36 Loss Train: 0.6995015144348145\n",
      "Epoch: 3 \t\t\t Iteration 37 Loss Train: 0.6967525482177734\n",
      "Epoch: 3 \t\t\t Iteration 38 Loss Train: 0.6960799098014832\n",
      "Epoch: 3 \t\t\t Iteration 39 Loss Train: 0.6940150260925293\n",
      "Epoch: 3 \t\t\t Iteration 40 Loss Train: 0.6933906078338623\n",
      "Epoch: 3 \t\t\t Iteration 41 Loss Train: 0.6960683465003967\n",
      "Epoch: 3 \t\t\t Iteration 42 Loss Train: 0.6940276622772217\n",
      "Epoch: 3 \t\t\t Iteration 43 Loss Train: 0.6927089691162109\n",
      "Epoch: 3 \t\t\t Iteration 44 Loss Train: 0.6947089433670044\n",
      "Epoch: 3 \t\t\t Iteration 45 Loss Train: 0.694033682346344\n",
      "Epoch: 3 \t\t\t Iteration 46 Loss Train: 0.6927101612091064\n",
      "Epoch: 3 \t\t\t Iteration 47 Loss Train: 0.6967136859893799\n",
      "Epoch: 3 \t\t\t Iteration 48 Loss Train: 0.6953569054603577\n",
      "Epoch: 3 \t\t\t Iteration 49 Loss Train: 0.6953526735305786\n",
      "Epoch: 3 \t\t\t Iteration 50 Loss Train: 0.6927049160003662\n",
      "Epoch: 3 \t\t\t Iteration 51 Loss Train: 0.6979994773864746\n",
      "Epoch: 3 \t\t\t Iteration 52 Loss Train: 0.695344090461731\n",
      "Epoch: 3 \t\t\t Iteration 53 Loss Train: 0.692714512348175\n",
      "Epoch: 3 \t\t\t Iteration 54 Loss Train: 0.6992616057395935\n",
      "Epoch: 3 \t\t\t Iteration 55 Loss Train: 0.6914116144180298\n",
      "Epoch: 3 \t\t\t Iteration 56 Loss Train: 0.695297122001648\n",
      "Epoch: 3 \t\t\t Iteration 57 Loss Train: 0.6901274919509888\n",
      "Epoch: 3 \t\t\t Iteration 58 Loss Train: 0.6959866285324097\n",
      "Epoch: 3 \t\t\t Iteration 59 Loss Train: 0.7005312442779541\n",
      "Epoch: 3 \t\t\t Iteration 60 Loss Train: 0.695293664932251\n",
      "Epoch: 3 \t\t\t Iteration 61 Loss Train: 0.692076563835144\n",
      "Epoch: 3 \t\t\t Iteration 62 Loss Train: 0.6985181570053101\n",
      "Epoch: 3 \t\t\t Iteration 63 Loss Train: 0.6991242170333862\n",
      "Epoch: 3 \t\t\t Iteration 64 Loss Train: 0.6997223496437073\n",
      "Epoch: 3 \t\t\t Iteration 65 Loss Train: 0.6958849430084229\n",
      "Epoch: 3 \t\t\t Iteration 66 Loss Train: 0.692078173160553\n",
      "Epoch: 3 \t\t\t Iteration 67 Loss Train: 0.6952454447746277\n",
      "Epoch: 3 \t\t\t Iteration 68 Loss Train: 0.6952612400054932\n",
      "Epoch: 3 \t\t\t Iteration 69 Loss Train: 0.689563512802124\n",
      "Epoch: 3 \t\t\t Iteration 70 Loss Train: 0.6914491653442383\n",
      "Epoch: 3 \t\t\t Iteration 71 Loss Train: 0.6952475309371948\n",
      "Epoch: 3 \t\t\t Iteration 72 Loss Train: 0.6952162981033325\n",
      "Epoch: 3 \t\t\t Iteration 73 Loss Train: 0.6945968866348267\n",
      "Epoch: 3 \t\t\t Iteration 74 Loss Train: 0.6939961910247803\n",
      "Epoch: 3 \t\t\t Iteration 75 Loss Train: 0.6902337074279785\n",
      "Epoch: 3 \t\t\t Iteration 76 Loss Train: 0.6964704990386963\n",
      "Epoch: 3 \t\t\t Iteration 77 Loss Train: 0.6952359080314636\n",
      "Epoch: 3 \t\t\t Iteration 78 Loss Train: 0.6952067613601685\n",
      "Epoch: 3 \t\t\t Iteration 79 Loss Train: 0.695198655128479\n",
      "Epoch: 3 \t\t\t Iteration 80 Loss Train: 0.6933696269989014\n",
      "Epoch: 3 \t\t\t Iteration 81 Loss Train: 0.6964411735534668\n",
      "Epoch: 3 \t\t\t Iteration 82 Loss Train: 0.6933608651161194\n",
      "Epoch: 3 \t\t\t Iteration 83 Loss Train: 0.6945683360099792\n",
      "Epoch: 3 \t\t\t Iteration 84 Loss Train: 0.6957890391349792\n",
      "Epoch: 3 \t\t\t Iteration 85 Loss Train: 0.6982442140579224\n",
      "Epoch: 3 \t\t\t Iteration 86 Loss Train: 0.6939512491226196\n",
      "Epoch: 3 \t\t\t Iteration 87 Loss Train: 0.6896852254867554\n",
      "Epoch: 3 \t\t\t Iteration 88 Loss Train: 0.6964223384857178\n",
      "Epoch: 3 \t\t\t Iteration 89 Loss Train: 0.6982247829437256\n",
      "Epoch: 3 \t\t\t Iteration 90 Loss Train: 0.6976019144058228\n",
      "Epoch: 3 \t\t\t Iteration 91 Loss Train: 0.6921291947364807\n",
      "Epoch: 3 \t\t\t Iteration 92 Loss Train: 0.6945297718048096\n",
      "Epoch: 3 \t\t\t Iteration 93 Loss Train: 0.6969647407531738\n",
      "Epoch: 3 \t\t\t Iteration 94 Loss Train: 0.6903358697891235\n",
      "Epoch: 3 \t\t\t Iteration 95 Loss Train: 0.6897181868553162\n",
      "Epoch: 3 \t\t\t Iteration 96 Loss Train: 0.6939594745635986\n",
      "Epoch: 3 \t\t\t Iteration 97 Loss Train: 0.6963435411453247\n",
      "Epoch: 3 \t\t\t Iteration 98 Loss Train: 0.6957278251647949\n",
      "Epoch: 3 \t\t\t Iteration 99 Loss Train: 0.699320912361145\n",
      "Epoch: 3 \t\t\t Iteration 100 Loss Train: 0.6992683410644531\n",
      "Epoch: 3 \t\t\t Iteration 101 Loss Train: 0.6939223408699036\n",
      "Epoch: 3 \t\t\t Iteration 102 Loss Train: 0.6938989758491516\n",
      "Epoch: 3 \t\t\t Iteration 103 Loss Train: 0.6974723935127258\n",
      "Epoch: 3 \t\t\t Iteration 104 Loss Train: 0.6973932981491089\n",
      "Epoch: 3 \t\t\t Iteration 105 Loss Train: 0.6962062120437622\n",
      "Epoch: 3 \t\t\t Iteration 106 Loss Train: 0.6967960000038147\n",
      "Epoch: 3 \t\t\t Iteration 107 Loss Train: 0.6962136030197144\n",
      "Epoch: 3 \t\t\t Iteration 108 Loss Train: 0.6938895583152771\n",
      "Epoch: 3 \t\t\t Iteration 109 Loss Train: 0.6978930234909058\n",
      "Epoch: 3 \t\t\t Iteration 110 Loss Train: 0.6915974020957947\n",
      "Epoch: 3 \t\t\t Iteration 111 Loss Train: 0.6961702108383179\n",
      "Epoch: 3 \t\t\t Iteration 112 Loss Train: 0.6938844323158264\n",
      "Epoch: 3 \t\t\t Iteration 113 Loss Train: 0.6921876072883606\n",
      "Epoch: 3 \t\t\t Iteration 114 Loss Train: 0.6938748359680176\n",
      "Epoch: 3 \t\t\t Iteration 115 Loss Train: 0.6950039863586426\n",
      "Epoch: 3 \t\t\t Iteration 116 Loss Train: 0.6916234493255615\n",
      "Epoch: 3 \t\t\t Iteration 117 Loss Train: 0.6944546699523926\n",
      "Epoch: 3 \t\t\t Iteration 118 Loss Train: 0.6938772201538086\n",
      "Epoch: 3 \t\t\t Iteration 119 Loss Train: 0.6927489042282104\n",
      "Epoch: 3 \t\t\t Iteration 120 Loss Train: 0.6961323618888855\n",
      "Epoch: 3 \t\t\t Iteration 121 Loss Train: 0.6905088424682617\n",
      "Epoch: 3 \t\t\t Iteration 122 Loss Train: 0.6961389780044556\n",
      "Epoch: 3 \t\t\t Iteration 123 Loss Train: 0.693297803401947\n",
      "Epoch: 3 \t\t\t Iteration 124 Loss Train: 0.6927496194839478\n",
      "Epoch: 3 \t\t\t Iteration 125 Loss Train: 0.6921921968460083\n",
      "Epoch: 3 \t\t\t Iteration 126 Loss Train: 0.6955488324165344\n",
      "Epoch: 3 \t\t\t Iteration 127 Loss Train: 0.6961257457733154\n",
      "Epoch: 3 \t\t\t Iteration 128 Loss Train: 0.6944063901901245\n",
      "Epoch: 3 \t\t\t Iteration 129 Loss Train: 0.6949796676635742\n",
      "Epoch: 3 \t\t\t Iteration 130 Loss Train: 0.6955446004867554\n",
      "Epoch: 3 \t\t\t Iteration 131 Loss Train: 0.6949895620346069\n",
      "Epoch: 3 \t\t\t Iteration 132 Loss Train: 0.6943999528884888\n",
      "Epoch: 3 \t\t\t Iteration 133 Loss Train: 0.6977370977401733\n",
      "Epoch: 3 \t\t\t Iteration 134 Loss Train: 0.6938633918762207\n",
      "Epoch: 3 \t\t\t Iteration 135 Loss Train: 0.6911314129829407\n",
      "Epoch: 3 \t\t\t Iteration 136 Loss Train: 0.6944078803062439\n",
      "Epoch: 3 \t\t\t Iteration 137 Loss Train: 0.6938718557357788\n",
      "Epoch: 3 \t\t\t Iteration 138 Loss Train: 0.6943902373313904\n",
      "Epoch: 3 \t\t\t Iteration 139 Loss Train: 0.6971076726913452\n",
      "Epoch: 3 \t\t\t Iteration 140 Loss Train: 0.6970828175544739\n",
      "Epoch: 3 \t\t\t Iteration 141 Loss Train: 0.7002972960472107\n",
      "Epoch: 3 \t\t\t Iteration 142 Loss Train: 0.6938192844390869\n",
      "Epoch: 3 \t\t\t Iteration 143 Loss Train: 0.6954330205917358\n",
      "Epoch: 3 \t\t\t Iteration 144 Loss Train: 0.6927729845046997\n",
      "Epoch: 3 \t\t\t Iteration 145 Loss Train: 0.6959488391876221\n",
      "Epoch: 3 \t\t\t Iteration 146 Loss Train: 0.6927790641784668\n",
      "Epoch: 3 \t\t\t Iteration 147 Loss Train: 0.6943684816360474\n",
      "Epoch: 3 \t\t\t Iteration 148 Loss Train: 0.6932897567749023\n",
      "Epoch: 3 \t\t\t Iteration 149 Loss Train: 0.6932879686355591\n",
      "Epoch: 3 \t\t\t Iteration 150 Loss Train: 0.6954037547111511\n",
      "Epoch: 3 \t\t\t Iteration 151 Loss Train: 0.6959149837493896\n",
      "Epoch: 3 \t\t\t Iteration 152 Loss Train: 0.6974700689315796\n",
      "Epoch: 3 \t\t\t Iteration 153 Loss Train: 0.6896505355834961\n",
      "Epoch: 3 \t\t\t Iteration 154 Loss Train: 0.6922582983970642\n",
      "Epoch: 3 \t\t\t Iteration 155 Loss Train: 0.693810224533081\n",
      "Epoch: 3 \t\t\t Iteration 156 Loss Train: 0.6985095143318176\n",
      "Epoch: 3 \t\t\t Iteration 157 Loss Train: 0.6953533291816711\n",
      "Epoch: 3 \t\t\t Iteration 158 Loss Train: 0.6933064460754395\n",
      "Epoch: 3 \t\t\t Iteration 159 Loss Train: 0.6968814134597778\n",
      "Epoch: 3 \t\t\t Iteration 160 Loss Train: 0.6963770389556885\n",
      "Epoch: 3 \t\t\t Iteration 161 Loss Train: 0.6947944164276123\n",
      "Epoch: 3 \t\t\t Iteration 162 Loss Train: 0.691240668296814\n",
      "Epoch: 3 \t\t\t Iteration 163 Loss Train: 0.6932787299156189\n",
      "Epoch: 3 \t\t\t Iteration 164 Loss Train: 0.6958406567573547\n",
      "Epoch: 3 \t\t\t Iteration 165 Loss Train: 0.6942892670631409\n",
      "Epoch: 3 \t\t\t Iteration 166 Loss Train: 0.6952950954437256\n",
      "Epoch: 3 \t\t\t Iteration 167 Loss Train: 0.6942929625511169\n",
      "Epoch: 3 \t\t\t Iteration 168 Loss Train: 0.6922791600227356\n",
      "Epoch: 3 \t\t\t Iteration 169 Loss Train: 0.6927675008773804\n",
      "Epoch: 3 \t\t\t Iteration 170 Loss Train: 0.6963027715682983\n",
      "Epoch: 3 \t\t\t Iteration 171 Loss Train: 0.6957482099533081\n",
      "Epoch: 3 \t\t\t Iteration 172 Loss Train: 0.6972675323486328\n",
      "Epoch: 3 \t\t\t Iteration 173 Loss Train: 0.6942615509033203\n",
      "Epoch: 3 \t\t\t Iteration 174 Loss Train: 0.6942610144615173\n",
      "Epoch: 3 \t\t\t Iteration 175 Loss Train: 0.6942524313926697\n",
      "Epoch: 3 \t\t\t Iteration 176 Loss Train: 0.6937786340713501\n",
      "Epoch: 3 \t\t\t Iteration 177 Loss Train: 0.694263219833374\n",
      "Epoch: 3 \t\t\t Iteration 178 Loss Train: 0.6961966753005981\n",
      "Epoch: 3 \t\t\t Iteration 179 Loss Train: 0.6903454065322876\n",
      "Epoch: 3 \t\t\t Iteration 180 Loss Train: 0.6981366872787476\n",
      "Epoch: 3 \t\t\t Iteration 181 Loss Train: 0.6976390480995178\n",
      "Epoch: 3 \t\t\t Iteration 182 Loss Train: 0.6956769227981567\n",
      "Epoch: 3 \t\t\t Iteration 183 Loss Train: 0.6980184316635132\n",
      "Epoch: 3 \t\t\t Iteration 184 Loss Train: 0.6894741058349609\n",
      "Epoch: 3 \t\t\t Iteration 185 Loss Train: 0.6961172819137573\n",
      "Epoch: 3 \t\t\t Iteration 186 Loss Train: 0.692792534828186\n",
      "Epoch: 3 \t\t\t Iteration 187 Loss Train: 0.6960943937301636\n",
      "Epoch: 3 \t\t\t Iteration 188 Loss Train: 0.6946643590927124\n",
      "Epoch: 3 \t\t\t Iteration 189 Loss Train: 0.6923359036445618\n",
      "Epoch: 3 \t\t\t Iteration 190 Loss Train: 0.6937272548675537\n",
      "Epoch: 3 \t\t\t Iteration 191 Loss Train: 0.6946636438369751\n",
      "Epoch: 3 \t\t\t Iteration 192 Loss Train: 0.6927847862243652\n",
      "Epoch: 3 \t\t\t Iteration 193 Loss Train: 0.6927857398986816\n",
      "Epoch: 3 \t\t\t Iteration 194 Loss Train: 0.692298412322998\n",
      "Epoch: 3 \t\t\t Iteration 195 Loss Train: 0.6923219561576843\n",
      "Epoch: 3 \t\t\t Iteration 196 Loss Train: 0.6970023512840271\n",
      "Epoch: 3 \t\t\t Iteration 197 Loss Train: 0.6913795471191406\n",
      "Epoch: 3 \t\t\t Iteration 198 Loss Train: 0.6951439380645752\n",
      "Epoch: 3 \t\t\t Iteration 199 Loss Train: 0.694189190864563\n",
      "Epoch: 3 \t\t\t Iteration 200 Loss Train: 0.6937307119369507\n",
      "Epoch: 3 \t\t\t Iteration 201 Loss Train: 0.6937018632888794\n",
      "Epoch: 3 \t\t\t Iteration 202 Loss Train: 0.6941709518432617\n",
      "Epoch: 3 \t\t\t Iteration 203 Loss Train: 0.6955711841583252\n",
      "Epoch: 3 \t\t\t Iteration 204 Loss Train: 0.6946306824684143\n",
      "Epoch: 3 \t\t\t Iteration 205 Loss Train: 0.6950988173484802\n",
      "Epoch: 3 \t\t\t Iteration 206 Loss Train: 0.6914314031600952\n",
      "Epoch: 3 \t\t\t Iteration 207 Loss Train: 0.6900597214698792\n",
      "Epoch: 3 \t\t\t Iteration 208 Loss Train: 0.6946353912353516\n",
      "Epoch: 3 \t\t\t Iteration 209 Loss Train: 0.6937034726142883\n",
      "Epoch: 3 \t\t\t Iteration 210 Loss Train: 0.6973893642425537\n",
      "Epoch: 3 \t\t\t Iteration 211 Loss Train: 0.6914361715316772\n",
      "Epoch: 3 \t\t\t Iteration 212 Loss Train: 0.6946225166320801\n",
      "Epoch: 3 \t\t\t Iteration 213 Loss Train: 0.6964151859283447\n",
      "Epoch: 3 \t\t\t Iteration 214 Loss Train: 0.6950520873069763\n",
      "Epoch: 3 \t\t\t Iteration 215 Loss Train: 0.6950492858886719\n",
      "Epoch: 3 \t\t\t Iteration 216 Loss Train: 0.6950496435165405\n",
      "Epoch: 3 \t\t\t Iteration 217 Loss Train: 0.6959085464477539\n",
      "Epoch: 3 \t\t\t Iteration 218 Loss Train: 0.6941351890563965\n",
      "Epoch: 3 \t\t\t Iteration 219 Loss Train: 0.696365475654602\n",
      "Epoch: 3 \t\t\t Iteration 220 Loss Train: 0.6989399790763855\n",
      "Epoch: 3 \t\t\t Iteration 221 Loss Train: 0.6923646330833435\n",
      "Epoch: 3 \t\t\t Iteration 222 Loss Train: 0.69322669506073\n",
      "Epoch: 3 \t\t\t Iteration 223 Loss Train: 0.6945701241493225\n",
      "Epoch: 3 \t\t\t Iteration 224 Loss Train: 0.6949788331985474\n",
      "Epoch: 3 \t\t\t Iteration 225 Loss Train: 0.6919571757316589\n",
      "Epoch: 3 \t\t\t Iteration 226 Loss Train: 0.6941033601760864\n",
      "Epoch: 3 \t\t\t Iteration 227 Loss Train: 0.6975374817848206\n",
      "Epoch: 3 \t\t\t Iteration 228 Loss Train: 0.6953544616699219\n",
      "Epoch: 3 \t\t\t Iteration 229 Loss Train: 0.6928367614746094\n",
      "Epoch: 3 \t\t\t Iteration 230 Loss Train: 0.6941024661064148\n",
      "Epoch: 3 \t\t\t Iteration 231 Loss Train: 0.6949294805526733\n",
      "Epoch: 3 \t\t\t Iteration 232 Loss Train: 0.6940912008285522\n",
      "Epoch: 3 \t\t\t Iteration 233 Loss Train: 0.695767879486084\n",
      "Epoch: 3 \t\t\t Iteration 234 Loss Train: 0.6907331943511963\n",
      "Epoch: 3 \t\t\t Iteration 235 Loss Train: 0.6936444044113159\n",
      "Epoch: 3 \t\t\t Iteration 236 Loss Train: 0.6961675882339478\n",
      "Epoch: 3 \t\t\t Iteration 237 Loss Train: 0.692399799823761\n",
      "Epoch: 3 \t\t\t Iteration 238 Loss Train: 0.6940864324569702\n",
      "Epoch: 3 \t\t\t Iteration 239 Loss Train: 0.6924315094947815\n",
      "Epoch: 3 \t\t\t Iteration 240 Loss Train: 0.6940879821777344\n",
      "Epoch: 3 \t\t\t Iteration 241 Loss Train: 0.6961314082145691\n",
      "Epoch: 3 \t\t\t Iteration 242 Loss Train: 0.6919911503791809\n",
      "Epoch: 3 \t\t\t Iteration 243 Loss Train: 0.6911841630935669\n",
      "Epoch: 3 \t\t\t Iteration 244 Loss Train: 0.6940736770629883\n",
      "Epoch: 3 \t\t\t Iteration 245 Loss Train: 0.6957266926765442\n",
      "Epoch: 3 \t\t\t Iteration 246 Loss Train: 0.6936612725257874\n",
      "Epoch: 3 \t\t\t Iteration 247 Loss Train: 0.6957025527954102\n",
      "Epoch: 3 \t\t\t Iteration 248 Loss Train: 0.6952694654464722\n",
      "Epoch: 3 \t\t\t Iteration 249 Loss Train: 0.6948692798614502\n",
      "Epoch: 3 \t\t\t Iteration 250 Loss Train: 0.6956334114074707\n",
      "Epoch: 3 \t\t\t Iteration 251 Loss Train: 0.6940260529518127\n",
      "Epoch: 3 \t\t\t Iteration 252 Loss Train: 0.695222020149231\n",
      "Epoch: 3 \t\t\t Iteration 253 Loss Train: 0.6979960203170776\n",
      "Epoch: 3 \t\t\t Iteration 254 Loss Train: 0.6932326555252075\n",
      "Epoch: 3 \t\t\t Iteration 255 Loss Train: 0.6952136158943176\n",
      "Epoch: 3 \t\t\t Iteration 256 Loss Train: 0.6959229707717896\n",
      "Epoch: 3 \t\t\t Iteration 257 Loss Train: 0.695168137550354\n",
      "Epoch: 3 \t\t\t Iteration 258 Loss Train: 0.6939570903778076\n",
      "Epoch: 3 \t\t\t Iteration 259 Loss Train: 0.6901609301567078\n",
      "Epoch: 3 \t\t\t Iteration 260 Loss Train: 0.6943731307983398\n",
      "Epoch: 3 \t\t\t Iteration 261 Loss Train: 0.6951242685317993\n",
      "Epoch: 3 \t\t\t Iteration 262 Loss Train: 0.6935931444168091\n",
      "Epoch: 3 \t\t\t Iteration 263 Loss Train: 0.694756031036377\n",
      "Epoch: 3 \t\t\t Iteration 264 Loss Train: 0.6935985088348389\n",
      "Epoch: 3 \t\t\t Iteration 265 Loss Train: 0.6939949989318848\n",
      "Epoch: 3 \t\t\t Iteration 266 Loss Train: 0.6947240829467773\n",
      "Epoch: 3 \t\t\t Iteration 267 Loss Train: 0.6935766935348511\n",
      "Epoch: 3 \t\t\t Iteration 268 Loss Train: 0.6951078176498413\n",
      "Epoch: 3 \t\t\t Iteration 269 Loss Train: 0.6935814619064331\n",
      "Epoch: 3 \t\t\t Iteration 270 Loss Train: 0.694718599319458\n",
      "Epoch: 3 \t\t\t Iteration 271 Loss Train: 0.6946781873703003\n",
      "Epoch: 3 \t\t\t Iteration 272 Loss Train: 0.6947026252746582\n",
      "Epoch: 3 \t\t\t Iteration 273 Loss Train: 0.6939370632171631\n",
      "Epoch: 3 \t\t\t Iteration 274 Loss Train: 0.6939393281936646\n",
      "Epoch: 3 \t\t\t Iteration 275 Loss Train: 0.6928491592407227\n",
      "Epoch: 3 \t\t\t Iteration 276 Loss Train: 0.6921384930610657\n",
      "Epoch: 3 \t\t\t Iteration 277 Loss Train: 0.6946797370910645\n",
      "Epoch: 3 \t\t\t Iteration 278 Loss Train: 0.6935999393463135\n",
      "Epoch: 3 \t\t\t Iteration 279 Loss Train: 0.6961196660995483\n",
      "Epoch: 3 \t\t\t Iteration 280 Loss Train: 0.694269597530365\n",
      "Epoch: 3 \t\t\t Iteration 281 Loss Train: 0.6935585737228394\n",
      "Epoch: 3 \t\t\t Iteration 282 Loss Train: 0.6921229958534241\n",
      "Epoch: 3 \t\t\t Iteration 283 Loss Train: 0.6921330094337463\n",
      "Epoch: 3 \t\t\t Iteration 284 Loss Train: 0.6953669786453247\n",
      "Epoch: 3 \t\t\t Iteration 285 Loss Train: 0.6942806243896484\n",
      "Epoch: 3 \t\t\t Iteration 286 Loss Train: 0.6935746669769287\n",
      "Epoch: 3 \t\t\t Iteration 287 Loss Train: 0.6953604221343994\n",
      "Epoch: 3 \t\t\t Iteration 288 Loss Train: 0.695341944694519\n",
      "Epoch: 3 \t\t\t Iteration 289 Loss Train: 0.693886399269104\n",
      "Epoch: 3 \t\t\t Iteration 290 Loss Train: 0.6959879994392395\n",
      "Epoch: 3 \t\t\t Iteration 291 Loss Train: 0.6952905058860779\n",
      "Epoch: 3 \t\t\t Iteration 292 Loss Train: 0.6942336559295654\n",
      "Epoch: 3 \t\t\t Iteration 293 Loss Train: 0.6935625672340393\n",
      "Epoch: 3 \t\t\t Iteration 294 Loss Train: 0.693873405456543\n",
      "Epoch: 3 \t\t\t Iteration 295 Loss Train: 0.6945787668228149\n",
      "Epoch: 3 \t\t\t Iteration 296 Loss Train: 0.6932023763656616\n",
      "Epoch: 3 \t\t\t Iteration 297 Loss Train: 0.6945570707321167\n",
      "Epoch: 3 \t\t\t Iteration 298 Loss Train: 0.6931906342506409\n",
      "Epoch: 3 \t\t\t Iteration 299 Loss Train: 0.6938642263412476\n",
      "Epoch: 3 \t\t\t Iteration 300 Loss Train: 0.6912040710449219\n",
      "Epoch: 3 \t\t\t Iteration 301 Loss Train: 0.6911672353744507\n",
      "Epoch: 3 \t\t\t Iteration 302 Loss Train: 0.6939660310745239\n",
      "Epoch: 3 \t\t\t Iteration 1 Loss Valid: 0.6935536861419678\n",
      "Epoch: 3 \t\t\t Iteration 2 Loss Valid: 0.6931921243667603\n",
      "Epoch: 3 \t\t\t Iteration 3 Loss Valid: 0.692874550819397\n",
      "Epoch: 3 \t\t\t Iteration 4 Loss Valid: 0.6935365200042725\n",
      "Epoch: 3 \t\t\t Iteration 5 Loss Valid: 0.6932101249694824\n",
      "Epoch: 3 \t\t\t Iteration 6 Loss Valid: 0.6921800374984741\n",
      "Epoch: 3 \t\t\t Iteration 7 Loss Valid: 0.6935423016548157\n",
      "Epoch: 3 \t\t\t Iteration 8 Loss Valid: 0.6928714513778687\n",
      "Epoch: 3 \t\t\t Iteration 9 Loss Valid: 0.6935394406318665\n",
      "Epoch: 3 \t\t\t Iteration 10 Loss Valid: 0.6938822269439697\n",
      "Epoch: 3 \t\t\t Iteration 11 Loss Valid: 0.6918727159500122\n",
      "Epoch: 3 \t\t\t Iteration 12 Loss Valid: 0.6928763389587402\n",
      "Epoch: 3 \t\t\t Iteration 13 Loss Valid: 0.6932108402252197\n",
      "Epoch: 3 \t\t\t Iteration 14 Loss Valid: 0.6935464143753052\n",
      "Epoch: 3 \t\t\t Iteration 15 Loss Valid: 0.6935470700263977\n",
      "Epoch: 3 \t\t\t Iteration 16 Loss Valid: 0.6945581436157227\n",
      "Epoch: 3 \t\t\t Iteration 17 Loss Valid: 0.6938865184783936\n",
      "Epoch: 3 \t\t\t Iteration 18 Loss Valid: 0.6935437917709351\n",
      "Epoch: 3 \t\t\t Iteration 19 Loss Valid: 0.6932063698768616\n",
      "Epoch: 3 \t\t\t Iteration 20 Loss Valid: 0.6935484409332275\n",
      "Epoch: 3 \t\t\t Iteration 21 Loss Valid: 0.6942256689071655\n",
      "Epoch: 3 \t\t\t Iteration 22 Loss Valid: 0.6959069967269897\n",
      "Epoch: 3 \t\t\t Iteration 23 Loss Valid: 0.6935516595840454\n",
      "Epoch: 3 \t\t\t Iteration 24 Loss Valid: 0.6928722262382507\n",
      "Epoch: 3 \t\t\t Iteration 25 Loss Valid: 0.6932045221328735\n",
      "Epoch: 3 \t\t\t Iteration 26 Loss Valid: 0.6938867568969727\n",
      "Epoch: 3 \t\t\t Iteration 27 Loss Valid: 0.6931964755058289\n",
      "Epoch: 3 \t\t\t Iteration 28 Loss Valid: 0.6928702592849731\n",
      "Epoch: 3 \t\t\t Iteration 29 Loss Valid: 0.6935414671897888\n",
      "Epoch: 3 \t\t\t Iteration 30 Loss Valid: 0.6935423016548157\n",
      "Epoch: 3 \t\t\t Iteration 31 Loss Valid: 0.6942185163497925\n",
      "Epoch: 3 \t\t\t Iteration 32 Loss Valid: 0.6942422389984131\n",
      "Epoch: 3 \t\t\t Iteration 33 Loss Valid: 0.693200945854187\n",
      "Epoch: 3 \t\t\t Iteration 34 Loss Valid: 0.69353848695755\n",
      "Epoch: 3 \t\t\t Iteration 35 Loss Valid: 0.6928690671920776\n",
      "Epoch: 3 \t\t\t Iteration 36 Loss Valid: 0.6932036280632019\n",
      "Epoch: 3 \t\t\t Iteration 37 Loss Valid: 0.6938906908035278\n",
      "Epoch: 3 \t\t\t Iteration 38 Loss Valid: 0.6935438513755798\n",
      "Epoch: 3 \t\t\t Iteration 39 Loss Valid: 0.6938788890838623\n",
      "Epoch: 3 \t\t\t Iteration 40 Loss Valid: 0.694904088973999\n",
      "Epoch: 3 \t\t\t Iteration 41 Loss Valid: 0.693210244178772\n",
      "Epoch: 3 \t\t\t Iteration 42 Loss Valid: 0.6945756673812866\n",
      "Epoch: 3 \t\t\t Iteration 43 Loss Valid: 0.699629008769989\n",
      "Epoch: 3 \t\t\t Iteration 44 Loss Valid: 0.7039744853973389\n",
      "Epoch: 3 \t\t\t Iteration 45 Loss Valid: 0.7039835453033447\n",
      "Epoch: 3 \t\t\t Iteration 46 Loss Valid: 0.7039821743965149\n",
      "Epoch: 3 \t\t\t Iteration 47 Loss Valid: 0.7040818929672241\n",
      "Epoch: 3 \t\t\t Iteration 48 Loss Valid: 0.704071581363678\n",
      "Epoch: 3 \t\t\t Iteration 49 Loss Valid: 0.7040258646011353\n",
      "Epoch: 3 \t\t\t Iteration 50 Loss Valid: 0.7040630578994751\n",
      "Epoch: 3 \t\t\t Iteration 51 Loss Valid: 0.7039571404457092\n",
      "Epoch: 3 \t\t\t Iteration 52 Loss Valid: 0.7040084004402161\n",
      "Epoch: 3 \t\t\t Iteration 53 Loss Valid: 0.7040356397628784\n",
      "Epoch: 3 \t\t\t Iteration 54 Loss Valid: 0.6877974271774292\n",
      "Epoch: 3 \t\t\t Iteration 55 Loss Valid: 0.6823725700378418\n",
      "Epoch: 3 \t\t\t Iteration 56 Loss Valid: 0.6823945045471191\n",
      "Epoch: 3 \t\t\t Iteration 57 Loss Valid: 0.6824300289154053\n",
      "Epoch: 3 \t\t\t Iteration 58 Loss Valid: 0.6823709011077881\n",
      "Epoch: 3 \t\t\t Iteration 59 Loss Valid: 0.6824021935462952\n",
      "Epoch: 3 \t\t\t Iteration 60 Loss Valid: 0.6824290752410889\n",
      "Epoch: 3 \t\t\t Iteration 61 Loss Valid: 0.6823732852935791\n",
      "Epoch: 3 / 5 \t\t\t Training Loss:0.8348327350142776\n",
      "Epoch: 4 \t\t\t Iteration 1 Loss Train: 0.696240246295929\n",
      "Epoch: 4 \t\t\t Iteration 2 Loss Train: 0.693558931350708\n",
      "Epoch: 4 \t\t\t Iteration 3 Loss Train: 0.6945324540138245\n",
      "Epoch: 4 \t\t\t Iteration 4 Loss Train: 0.6951916813850403\n",
      "Epoch: 4 \t\t\t Iteration 5 Loss Train: 0.6912384033203125\n",
      "Epoch: 4 \t\t\t Iteration 6 Loss Train: 0.6938393115997314\n",
      "Epoch: 4 \t\t\t Iteration 7 Loss Train: 0.6945241689682007\n",
      "Epoch: 4 \t\t\t Iteration 8 Loss Train: 0.6925463080406189\n",
      "Epoch: 4 \t\t\t Iteration 9 Loss Train: 0.6941912770271301\n",
      "Epoch: 4 \t\t\t Iteration 10 Loss Train: 0.6935495734214783\n",
      "Epoch: 4 \t\t\t Iteration 11 Loss Train: 0.692874550819397\n",
      "Epoch: 4 \t\t\t Iteration 12 Loss Train: 0.6932055354118347\n",
      "Epoch: 4 \t\t\t Iteration 13 Loss Train: 0.6951727867126465\n",
      "Epoch: 4 \t\t\t Iteration 14 Loss Train: 0.6941527128219604\n",
      "Epoch: 4 \t\t\t Iteration 15 Loss Train: 0.6944604516029358\n",
      "Epoch: 4 \t\t\t Iteration 16 Loss Train: 0.6951249837875366\n",
      "Epoch: 4 \t\t\t Iteration 17 Loss Train: 0.6919348239898682\n",
      "Epoch: 4 \t\t\t Iteration 18 Loss Train: 0.6960799694061279\n",
      "Epoch: 4 \t\t\t Iteration 19 Loss Train: 0.694769024848938\n",
      "Epoch: 4 \t\t\t Iteration 20 Loss Train: 0.6950687170028687\n",
      "Epoch: 4 \t\t\t Iteration 21 Loss Train: 0.6931993961334229\n",
      "Epoch: 4 \t\t\t Iteration 22 Loss Train: 0.6938364505767822\n",
      "Epoch: 4 \t\t\t Iteration 23 Loss Train: 0.6938127279281616\n",
      "Epoch: 4 \t\t\t Iteration 24 Loss Train: 0.6950459480285645\n",
      "Epoch: 4 \t\t\t Iteration 25 Loss Train: 0.6935096979141235\n",
      "Epoch: 4 \t\t\t Iteration 26 Loss Train: 0.6947256326675415\n",
      "Epoch: 4 \t\t\t Iteration 27 Loss Train: 0.6938077211380005\n",
      "Epoch: 4 \t\t\t Iteration 28 Loss Train: 0.6935110688209534\n",
      "Epoch: 4 \t\t\t Iteration 29 Loss Train: 0.6928936243057251\n",
      "Epoch: 4 \t\t\t Iteration 30 Loss Train: 0.6922925114631653\n",
      "Epoch: 4 \t\t\t Iteration 31 Loss Train: 0.6944060325622559\n",
      "Epoch: 4 \t\t\t Iteration 32 Loss Train: 0.6956278681755066\n",
      "Epoch: 4 \t\t\t Iteration 33 Loss Train: 0.6932065486907959\n",
      "Epoch: 4 \t\t\t Iteration 34 Loss Train: 0.6934854388237\n",
      "Epoch: 4 \t\t\t Iteration 35 Loss Train: 0.6949858665466309\n",
      "Epoch: 4 \t\t\t Iteration 36 Loss Train: 0.6931939721107483\n",
      "Epoch: 4 \t\t\t Iteration 37 Loss Train: 0.6952521800994873\n",
      "Epoch: 4 \t\t\t Iteration 38 Loss Train: 0.6920236945152283\n",
      "Epoch: 4 \t\t\t Iteration 39 Loss Train: 0.6946406364440918\n",
      "Epoch: 4 \t\t\t Iteration 40 Loss Train: 0.6931723356246948\n",
      "Epoch: 4 \t\t\t Iteration 41 Loss Train: 0.6958045959472656\n",
      "Epoch: 4 \t\t\t Iteration 42 Loss Train: 0.6937772035598755\n",
      "Epoch: 4 \t\t\t Iteration 43 Loss Train: 0.6940453052520752\n",
      "Epoch: 4 \t\t\t Iteration 44 Loss Train: 0.6914972066879272\n",
      "Epoch: 4 \t\t\t Iteration 45 Loss Train: 0.6934396624565125\n",
      "Epoch: 4 \t\t\t Iteration 46 Loss Train: 0.6920263767242432\n",
      "Epoch: 4 \t\t\t Iteration 47 Loss Train: 0.6931933164596558\n",
      "Epoch: 4 \t\t\t Iteration 48 Loss Train: 0.6900323629379272\n",
      "Epoch: 4 \t\t\t Iteration 49 Loss Train: 0.6961281299591064\n",
      "Epoch: 4 \t\t\t Iteration 50 Loss Train: 0.6940556764602661\n",
      "Epoch: 4 \t\t\t Iteration 51 Loss Train: 0.6934698224067688\n",
      "Epoch: 4 \t\t\t Iteration 52 Loss Train: 0.6937544345855713\n",
      "Epoch: 4 \t\t\t Iteration 53 Loss Train: 0.6943409442901611\n",
      "Epoch: 4 \t\t\t Iteration 54 Loss Train: 0.6940327882766724\n",
      "Epoch: 4 \t\t\t Iteration 55 Loss Train: 0.695155143737793\n",
      "Epoch: 4 \t\t\t Iteration 56 Loss Train: 0.6934558153152466\n",
      "Epoch: 4 \t\t\t Iteration 57 Loss Train: 0.6929070949554443\n",
      "Epoch: 4 \t\t\t Iteration 58 Loss Train: 0.6959658861160278\n",
      "Epoch: 4 \t\t\t Iteration 59 Loss Train: 0.6950986385345459\n",
      "Epoch: 4 \t\t\t Iteration 60 Loss Train: 0.6923545598983765\n",
      "Epoch: 4 \t\t\t Iteration 61 Loss Train: 0.694536030292511\n",
      "Epoch: 4 \t\t\t Iteration 62 Loss Train: 0.692930281162262\n",
      "Epoch: 4 \t\t\t Iteration 63 Loss Train: 0.6942638158798218\n",
      "Epoch: 4 \t\t\t Iteration 64 Loss Train: 0.6926369667053223\n",
      "Epoch: 4 \t\t\t Iteration 65 Loss Train: 0.6945398449897766\n",
      "Epoch: 4 \t\t\t Iteration 66 Loss Train: 0.6928994655609131\n",
      "Epoch: 4 \t\t\t Iteration 67 Loss Train: 0.6952969431877136\n",
      "Epoch: 4 \t\t\t Iteration 68 Loss Train: 0.6947435736656189\n",
      "Epoch: 4 \t\t\t Iteration 69 Loss Train: 0.6929121017456055\n",
      "Epoch: 4 \t\t\t Iteration 70 Loss Train: 0.6929217576980591\n",
      "Epoch: 4 \t\t\t Iteration 71 Loss Train: 0.6932021975517273\n",
      "Epoch: 4 \t\t\t Iteration 72 Loss Train: 0.6926636695861816\n",
      "Epoch: 4 \t\t\t Iteration 73 Loss Train: 0.69395911693573\n",
      "Epoch: 4 \t\t\t Iteration 74 Loss Train: 0.693189799785614\n",
      "Epoch: 4 \t\t\t Iteration 75 Loss Train: 0.6944799423217773\n",
      "Epoch: 4 \t\t\t Iteration 76 Loss Train: 0.6947106122970581\n",
      "Epoch: 4 \t\t\t Iteration 77 Loss Train: 0.6934428215026855\n",
      "Epoch: 4 \t\t\t Iteration 78 Loss Train: 0.6944342255592346\n",
      "Epoch: 4 \t\t\t Iteration 79 Loss Train: 0.6936883330345154\n",
      "Epoch: 4 \t\t\t Iteration 80 Loss Train: 0.694706916809082\n",
      "Epoch: 4 \t\t\t Iteration 81 Loss Train: 0.6934297680854797\n",
      "Epoch: 4 \t\t\t Iteration 82 Loss Train: 0.6956356167793274\n",
      "Epoch: 4 \t\t\t Iteration 83 Loss Train: 0.6936644315719604\n",
      "Epoch: 4 \t\t\t Iteration 84 Loss Train: 0.6948651075363159\n",
      "Epoch: 4 \t\t\t Iteration 85 Loss Train: 0.6922509074211121\n",
      "Epoch: 4 \t\t\t Iteration 86 Loss Train: 0.6953340768814087\n",
      "Epoch: 4 \t\t\t Iteration 87 Loss Train: 0.6927001476287842\n",
      "Epoch: 4 \t\t\t Iteration 88 Loss Train: 0.6948425769805908\n",
      "Epoch: 4 \t\t\t Iteration 89 Loss Train: 0.6929458379745483\n",
      "Epoch: 4 \t\t\t Iteration 90 Loss Train: 0.6931842565536499\n",
      "Epoch: 4 \t\t\t Iteration 91 Loss Train: 0.6934318542480469\n",
      "Epoch: 4 \t\t\t Iteration 92 Loss Train: 0.6929478645324707\n",
      "Epoch: 4 \t\t\t Iteration 93 Loss Train: 0.6931888461112976\n",
      "Epoch: 4 \t\t\t Iteration 94 Loss Train: 0.6943553686141968\n",
      "Epoch: 4 \t\t\t Iteration 95 Loss Train: 0.6950110793113708\n",
      "Epoch: 4 \t\t\t Iteration 96 Loss Train: 0.693656861782074\n",
      "Epoch: 4 \t\t\t Iteration 97 Loss Train: 0.6956593990325928\n",
      "Epoch: 4 \t\t\t Iteration 98 Loss Train: 0.6923092603683472\n",
      "Epoch: 4 \t\t\t Iteration 99 Loss Train: 0.6936466693878174\n",
      "Epoch: 4 \t\t\t Iteration 100 Loss Train: 0.6947274804115295\n",
      "Epoch: 4 \t\t\t Iteration 101 Loss Train: 0.6942949891090393\n",
      "Epoch: 4 \t\t\t Iteration 102 Loss Train: 0.6942301988601685\n",
      "Epoch: 4 \t\t\t Iteration 103 Loss Train: 0.6938068866729736\n",
      "Epoch: 4 \t\t\t Iteration 104 Loss Train: 0.6940288543701172\n",
      "Epoch: 4 \t\t\t Iteration 105 Loss Train: 0.6921054124832153\n",
      "Epoch: 4 \t\t\t Iteration 106 Loss Train: 0.6936041712760925\n",
      "Epoch: 4 \t\t\t Iteration 107 Loss Train: 0.6940187215805054\n",
      "Epoch: 4 \t\t\t Iteration 108 Loss Train: 0.6938046216964722\n",
      "Epoch: 4 \t\t\t Iteration 109 Loss Train: 0.6948634386062622\n",
      "Epoch: 4 \t\t\t Iteration 110 Loss Train: 0.6940017938613892\n",
      "Epoch: 4 \t\t\t Iteration 111 Loss Train: 0.6956301927566528\n",
      "Epoch: 4 \t\t\t Iteration 112 Loss Train: 0.6943464279174805\n",
      "Epoch: 4 \t\t\t Iteration 113 Loss Train: 0.6933681964874268\n",
      "Epoch: 4 \t\t\t Iteration 114 Loss Train: 0.6927716732025146\n",
      "Epoch: 4 \t\t\t Iteration 115 Loss Train: 0.6935470104217529\n",
      "Epoch: 4 \t\t\t Iteration 116 Loss Train: 0.6929734945297241\n",
      "Epoch: 4 \t\t\t Iteration 117 Loss Train: 0.6935662627220154\n",
      "Epoch: 4 \t\t\t Iteration 118 Loss Train: 0.6931571960449219\n",
      "Epoch: 4 \t\t\t Iteration 119 Loss Train: 0.6931540966033936\n",
      "Epoch: 4 \t\t\t Iteration 120 Loss Train: 0.6941683888435364\n",
      "Epoch: 4 \t\t\t Iteration 121 Loss Train: 0.6920243501663208\n",
      "Epoch: 4 \t\t\t Iteration 122 Loss Train: 0.6925721168518066\n",
      "Epoch: 4 \t\t\t Iteration 123 Loss Train: 0.6919702887535095\n",
      "Epoch: 4 \t\t\t Iteration 124 Loss Train: 0.6943531036376953\n",
      "Epoch: 4 \t\t\t Iteration 125 Loss Train: 0.6933687925338745\n",
      "Epoch: 4 \t\t\t Iteration 126 Loss Train: 0.6943560838699341\n",
      "Epoch: 4 \t\t\t Iteration 127 Loss Train: 0.6951130628585815\n",
      "Epoch: 4 \t\t\t Iteration 128 Loss Train: 0.6924046277999878\n",
      "Epoch: 4 \t\t\t Iteration 129 Loss Train: 0.6937258243560791\n",
      "Epoch: 4 \t\t\t Iteration 130 Loss Train: 0.6933679580688477\n",
      "Epoch: 4 \t\t\t Iteration 131 Loss Train: 0.6939364671707153\n",
      "Epoch: 4 \t\t\t Iteration 132 Loss Train: 0.6952358484268188\n",
      "Epoch: 4 \t\t\t Iteration 133 Loss Train: 0.6937072277069092\n",
      "Epoch: 4 \t\t\t Iteration 134 Loss Train: 0.6924461722373962\n",
      "Epoch: 4 \t\t\t Iteration 135 Loss Train: 0.6942572593688965\n",
      "Epoch: 4 \t\t\t Iteration 136 Loss Train: 0.6928380727767944\n",
      "Epoch: 4 \t\t\t Iteration 137 Loss Train: 0.6938748359680176\n",
      "Epoch: 4 \t\t\t Iteration 138 Loss Train: 0.6931487321853638\n",
      "Epoch: 4 \t\t\t Iteration 139 Loss Train: 0.6919008493423462\n",
      "Epoch: 4 \t\t\t Iteration 140 Loss Train: 0.6926129460334778\n",
      "Epoch: 4 \t\t\t Iteration 141 Loss Train: 0.6933755278587341\n",
      "Epoch: 4 \t\t\t Iteration 142 Loss Train: 0.6938919425010681\n",
      "Epoch: 4 \t\t\t Iteration 143 Loss Train: 0.6931717395782471\n",
      "Epoch: 4 \t\t\t Iteration 144 Loss Train: 0.6931734085083008\n",
      "Epoch: 4 \t\t\t Iteration 145 Loss Train: 0.6931813955307007\n",
      "Epoch: 4 \t\t\t Iteration 146 Loss Train: 0.6955211758613586\n",
      "Epoch: 4 \t\t\t Iteration 147 Loss Train: 0.6940606236457825\n",
      "Epoch: 4 \t\t\t Iteration 148 Loss Train: 0.6934958696365356\n",
      "Epoch: 4 \t\t\t Iteration 149 Loss Train: 0.6929938197135925\n",
      "Epoch: 4 \t\t\t Iteration 150 Loss Train: 0.6933519244194031\n",
      "Epoch: 4 \t\t\t Iteration 151 Loss Train: 0.6948696374893188\n",
      "Epoch: 4 \t\t\t Iteration 152 Loss Train: 0.6931657791137695\n",
      "Epoch: 4 \t\t\t Iteration 153 Loss Train: 0.692648708820343\n",
      "Epoch: 4 \t\t\t Iteration 154 Loss Train: 0.6933470964431763\n",
      "Epoch: 4 \t\t\t Iteration 155 Loss Train: 0.6919610500335693\n",
      "Epoch: 4 \t\t\t Iteration 156 Loss Train: 0.6929974555969238\n",
      "Epoch: 4 \t\t\t Iteration 157 Loss Train: 0.6923113465309143\n",
      "Epoch: 4 \t\t\t Iteration 158 Loss Train: 0.6938551664352417\n",
      "Epoch: 4 \t\t\t Iteration 159 Loss Train: 0.6938492059707642\n",
      "Epoch: 4 \t\t\t Iteration 160 Loss Train: 0.693837583065033\n",
      "Epoch: 4 \t\t\t Iteration 161 Loss Train: 0.6941928863525391\n",
      "Epoch: 4 \t\t\t Iteration 162 Loss Train: 0.6949818134307861\n",
      "Epoch: 4 \t\t\t Iteration 163 Loss Train: 0.6935031414031982\n",
      "Epoch: 4 \t\t\t Iteration 164 Loss Train: 0.6934792995452881\n",
      "Epoch: 4 \t\t\t Iteration 165 Loss Train: 0.6939821839332581\n",
      "Epoch: 4 \t\t\t Iteration 166 Loss Train: 0.6941245198249817\n",
      "Epoch: 4 \t\t\t Iteration 167 Loss Train: 0.6939220428466797\n",
      "Epoch: 4 \t\t\t Iteration 168 Loss Train: 0.6937746405601501\n",
      "Epoch: 4 \t\t\t Iteration 169 Loss Train: 0.692870557308197\n",
      "Epoch: 4 \t\t\t Iteration 170 Loss Train: 0.693926215171814\n",
      "Epoch: 4 \t\t\t Iteration 171 Loss Train: 0.693146824836731\n",
      "Epoch: 4 \t\t\t Iteration 172 Loss Train: 0.6936162710189819\n",
      "Epoch: 4 \t\t\t Iteration 173 Loss Train: 0.6940397024154663\n",
      "Epoch: 4 \t\t\t Iteration 174 Loss Train: 0.693732500076294\n",
      "Epoch: 4 \t\t\t Iteration 175 Loss Train: 0.6944401264190674\n",
      "Epoch: 4 \t\t\t Iteration 176 Loss Train: 0.693442702293396\n",
      "Epoch: 4 \t\t\t Iteration 177 Loss Train: 0.6931527853012085\n",
      "Epoch: 4 \t\t\t Iteration 178 Loss Train: 0.6937004923820496\n",
      "Epoch: 4 \t\t\t Iteration 179 Loss Train: 0.6935645341873169\n",
      "Epoch: 4 \t\t\t Iteration 180 Loss Train: 0.6934394836425781\n",
      "Epoch: 4 \t\t\t Iteration 181 Loss Train: 0.6938347220420837\n",
      "Epoch: 4 \t\t\t Iteration 182 Loss Train: 0.6938028931617737\n",
      "Epoch: 4 \t\t\t Iteration 183 Loss Train: 0.6940964460372925\n",
      "Epoch: 4 \t\t\t Iteration 184 Loss Train: 0.693548321723938\n",
      "Epoch: 4 \t\t\t Iteration 185 Loss Train: 0.6932874917984009\n",
      "Epoch: 4 \t\t\t Iteration 186 Loss Train: 0.6945328116416931\n",
      "Epoch: 4 \t\t\t Iteration 187 Loss Train: 0.6932873129844666\n",
      "Epoch: 4 \t\t\t Iteration 188 Loss Train: 0.6934047937393188\n",
      "Epoch: 4 \t\t\t Iteration 189 Loss Train: 0.6932673454284668\n",
      "Epoch: 4 \t\t\t Iteration 190 Loss Train: 0.6935120820999146\n",
      "Epoch: 4 \t\t\t Iteration 191 Loss Train: 0.6939839124679565\n",
      "Epoch: 4 \t\t\t Iteration 192 Loss Train: 0.69327312707901\n",
      "Epoch: 4 \t\t\t Iteration 193 Loss Train: 0.6935139298439026\n",
      "Epoch: 4 \t\t\t Iteration 194 Loss Train: 0.6938515305519104\n",
      "Epoch: 4 \t\t\t Iteration 195 Loss Train: 0.6938296556472778\n",
      "Epoch: 4 \t\t\t Iteration 196 Loss Train: 0.6925187706947327\n",
      "Epoch: 4 \t\t\t Iteration 197 Loss Train: 0.6929343938827515\n",
      "Epoch: 4 \t\t\t Iteration 198 Loss Train: 0.6933838725090027\n",
      "Epoch: 4 \t\t\t Iteration 199 Loss Train: 0.6933813095092773\n",
      "Epoch: 4 \t\t\t Iteration 200 Loss Train: 0.6931484341621399\n",
      "Epoch: 4 \t\t\t Iteration 201 Loss Train: 0.6928131580352783\n",
      "Epoch: 4 \t\t\t Iteration 202 Loss Train: 0.6925791501998901\n",
      "Epoch: 4 \t\t\t Iteration 203 Loss Train: 0.6938520073890686\n",
      "Epoch: 4 \t\t\t Iteration 204 Loss Train: 0.6932493448257446\n",
      "Epoch: 4 \t\t\t Iteration 205 Loss Train: 0.6937062740325928\n",
      "Epoch: 4 \t\t\t Iteration 206 Loss Train: 0.6930317282676697\n",
      "Epoch: 4 \t\t\t Iteration 207 Loss Train: 0.6933687329292297\n",
      "Epoch: 4 \t\t\t Iteration 208 Loss Train: 0.6939308047294617\n",
      "Epoch: 4 \t\t\t Iteration 209 Loss Train: 0.692916750907898\n",
      "Epoch: 4 \t\t\t Iteration 210 Loss Train: 0.6940089464187622\n",
      "Epoch: 4 \t\t\t Iteration 211 Loss Train: 0.6921082735061646\n",
      "Epoch: 4 \t\t\t Iteration 212 Loss Train: 0.6933643817901611\n",
      "Epoch: 4 \t\t\t Iteration 213 Loss Train: 0.693144679069519\n",
      "Epoch: 4 \t\t\t Iteration 214 Loss Train: 0.6932612061500549\n",
      "Epoch: 4 \t\t\t Iteration 215 Loss Train: 0.6939883232116699\n",
      "Epoch: 4 \t\t\t Iteration 216 Loss Train: 0.6931689977645874\n",
      "Epoch: 4 \t\t\t Iteration 217 Loss Train: 0.6932491660118103\n",
      "Epoch: 4 \t\t\t Iteration 218 Loss Train: 0.6934596300125122\n",
      "Epoch: 4 \t\t\t Iteration 219 Loss Train: 0.6939769983291626\n",
      "Epoch: 4 \t\t\t Iteration 220 Loss Train: 0.6937447190284729\n",
      "Epoch: 4 \t\t\t Iteration 221 Loss Train: 0.6935303211212158\n",
      "Epoch: 4 \t\t\t Iteration 222 Loss Train: 0.6931507587432861\n",
      "Epoch: 4 \t\t\t Iteration 223 Loss Train: 0.6933385729789734\n",
      "Epoch: 4 \t\t\t Iteration 224 Loss Train: 0.6930633187294006\n",
      "Epoch: 4 \t\t\t Iteration 225 Loss Train: 0.6934478878974915\n",
      "Epoch: 4 \t\t\t Iteration 226 Loss Train: 0.6936265230178833\n",
      "Epoch: 4 \t\t\t Iteration 227 Loss Train: 0.693246066570282\n",
      "Epoch: 4 \t\t\t Iteration 228 Loss Train: 0.6940573453903198\n",
      "Epoch: 4 \t\t\t Iteration 229 Loss Train: 0.6930635571479797\n",
      "Epoch: 4 \t\t\t Iteration 230 Loss Train: 0.6935106515884399\n",
      "Epoch: 4 \t\t\t Iteration 231 Loss Train: 0.6930581331253052\n",
      "Epoch: 4 \t\t\t Iteration 232 Loss Train: 0.6934105753898621\n",
      "Epoch: 4 \t\t\t Iteration 233 Loss Train: 0.6929886341094971\n",
      "Epoch: 4 \t\t\t Iteration 234 Loss Train: 0.6936600804328918\n",
      "Epoch: 4 \t\t\t Iteration 235 Loss Train: 0.6931230425834656\n",
      "Epoch: 4 \t\t\t Iteration 236 Loss Train: 0.6930773854255676\n",
      "Epoch: 4 \t\t\t Iteration 237 Loss Train: 0.69380122423172\n",
      "Epoch: 4 \t\t\t Iteration 238 Loss Train: 0.6933059692382812\n",
      "Epoch: 4 \t\t\t Iteration 239 Loss Train: 0.6934510469436646\n",
      "Epoch: 4 \t\t\t Iteration 240 Loss Train: 0.6930631399154663\n",
      "Epoch: 4 \t\t\t Iteration 241 Loss Train: 0.6932294368743896\n",
      "Epoch: 4 \t\t\t Iteration 242 Loss Train: 0.6934492588043213\n",
      "Epoch: 4 \t\t\t Iteration 243 Loss Train: 0.6928608417510986\n",
      "Epoch: 4 \t\t\t Iteration 244 Loss Train: 0.6932277679443359\n",
      "Epoch: 4 \t\t\t Iteration 245 Loss Train: 0.6932191252708435\n",
      "Epoch: 4 \t\t\t Iteration 246 Loss Train: 0.6936831474304199\n",
      "Epoch: 4 \t\t\t Iteration 247 Loss Train: 0.693082332611084\n",
      "Epoch: 4 \t\t\t Iteration 248 Loss Train: 0.6938053965568542\n",
      "Epoch: 4 \t\t\t Iteration 249 Loss Train: 0.6937927603721619\n",
      "Epoch: 4 \t\t\t Iteration 250 Loss Train: 0.6932361721992493\n",
      "Epoch: 4 \t\t\t Iteration 251 Loss Train: 0.6934851408004761\n",
      "Epoch: 4 \t\t\t Iteration 252 Loss Train: 0.6935303211212158\n",
      "Epoch: 4 \t\t\t Iteration 253 Loss Train: 0.6931576728820801\n",
      "Epoch: 4 \t\t\t Iteration 254 Loss Train: 0.6935063600540161\n",
      "Epoch: 4 \t\t\t Iteration 255 Loss Train: 0.6934523582458496\n",
      "Epoch: 4 \t\t\t Iteration 256 Loss Train: 0.6934847235679626\n",
      "Epoch: 4 \t\t\t Iteration 257 Loss Train: 0.693145751953125\n",
      "Epoch: 4 \t\t\t Iteration 258 Loss Train: 0.6931917667388916\n",
      "Epoch: 4 \t\t\t Iteration 259 Loss Train: 0.6936428546905518\n",
      "Epoch: 4 \t\t\t Iteration 260 Loss Train: 0.6932940483093262\n",
      "Epoch: 4 \t\t\t Iteration 261 Loss Train: 0.6931554675102234\n",
      "Epoch: 4 \t\t\t Iteration 262 Loss Train: 0.6931013464927673\n",
      "Epoch: 4 \t\t\t Iteration 263 Loss Train: 0.6934080123901367\n",
      "Epoch: 4 \t\t\t Iteration 264 Loss Train: 0.6935034394264221\n",
      "Epoch: 4 \t\t\t Iteration 265 Loss Train: 0.6930482387542725\n",
      "Epoch: 4 \t\t\t Iteration 266 Loss Train: 0.6934247016906738\n",
      "Epoch: 4 \t\t\t Iteration 267 Loss Train: 0.6931471824645996\n",
      "Epoch: 4 \t\t\t Iteration 268 Loss Train: 0.6933709383010864\n",
      "Epoch: 4 \t\t\t Iteration 269 Loss Train: 0.6931970119476318\n",
      "Epoch: 4 \t\t\t Iteration 270 Loss Train: 0.693305492401123\n",
      "Epoch: 4 \t\t\t Iteration 271 Loss Train: 0.6934669017791748\n",
      "Epoch: 4 \t\t\t Iteration 272 Loss Train: 0.6933774948120117\n",
      "Epoch: 4 \t\t\t Iteration 273 Loss Train: 0.693293035030365\n",
      "Epoch: 4 \t\t\t Iteration 274 Loss Train: 0.6933220028877258\n",
      "Epoch: 4 \t\t\t Iteration 275 Loss Train: 0.6932998895645142\n",
      "Epoch: 4 \t\t\t Iteration 276 Loss Train: 0.6930938363075256\n",
      "Epoch: 4 \t\t\t Iteration 277 Loss Train: 0.6930091381072998\n",
      "Epoch: 4 \t\t\t Iteration 278 Loss Train: 0.6934199333190918\n",
      "Epoch: 4 \t\t\t Iteration 279 Loss Train: 0.693267822265625\n",
      "Epoch: 4 \t\t\t Iteration 280 Loss Train: 0.6934394836425781\n",
      "Epoch: 4 \t\t\t Iteration 281 Loss Train: 0.693202793598175\n",
      "Epoch: 4 \t\t\t Iteration 282 Loss Train: 0.6931142807006836\n",
      "Epoch: 4 \t\t\t Iteration 283 Loss Train: 0.6931977272033691\n",
      "Epoch: 4 \t\t\t Iteration 284 Loss Train: 0.6933916211128235\n",
      "Epoch: 4 \t\t\t Iteration 285 Loss Train: 0.693276047706604\n",
      "Epoch: 4 \t\t\t Iteration 286 Loss Train: 0.6932049989700317\n",
      "Epoch: 4 \t\t\t Iteration 287 Loss Train: 0.6931747198104858\n",
      "Epoch: 4 \t\t\t Iteration 288 Loss Train: 0.6931638717651367\n",
      "Epoch: 4 \t\t\t Iteration 289 Loss Train: 0.6931961178779602\n",
      "Epoch: 4 \t\t\t Iteration 290 Loss Train: 0.6930891275405884\n",
      "Epoch: 4 \t\t\t Iteration 291 Loss Train: 0.6931784152984619\n",
      "Epoch: 4 \t\t\t Iteration 292 Loss Train: 0.6931772828102112\n",
      "Epoch: 4 \t\t\t Iteration 293 Loss Train: 0.6932077407836914\n",
      "Epoch: 4 \t\t\t Iteration 294 Loss Train: 0.6931337714195251\n",
      "Epoch: 4 \t\t\t Iteration 295 Loss Train: 0.6931366920471191\n",
      "Epoch: 4 \t\t\t Iteration 296 Loss Train: 0.6932089328765869\n",
      "Epoch: 4 \t\t\t Iteration 297 Loss Train: 0.6931489109992981\n",
      "Epoch: 4 \t\t\t Iteration 298 Loss Train: 0.6931865811347961\n",
      "Epoch: 4 \t\t\t Iteration 299 Loss Train: 0.6931465268135071\n",
      "Epoch: 4 \t\t\t Iteration 300 Loss Train: 0.693138599395752\n",
      "Epoch: 4 \t\t\t Iteration 301 Loss Train: 0.6931684613227844\n",
      "Epoch: 4 \t\t\t Iteration 302 Loss Train: 0.6931633353233337\n",
      "Epoch: 4 \t\t\t Iteration 1 Loss Valid: 0.6931546926498413\n",
      "Epoch: 4 \t\t\t Iteration 2 Loss Valid: 0.6931383609771729\n",
      "Epoch: 4 \t\t\t Iteration 3 Loss Valid: 0.6931508183479309\n",
      "Epoch: 4 \t\t\t Iteration 4 Loss Valid: 0.6931435465812683\n",
      "Epoch: 4 \t\t\t Iteration 5 Loss Valid: 0.6931519508361816\n",
      "Epoch: 4 \t\t\t Iteration 6 Loss Valid: 0.6931332945823669\n",
      "Epoch: 4 \t\t\t Iteration 7 Loss Valid: 0.6931474804878235\n",
      "Epoch: 4 \t\t\t Iteration 8 Loss Valid: 0.693148672580719\n",
      "Epoch: 4 \t\t\t Iteration 9 Loss Valid: 0.6931449174880981\n",
      "Epoch: 4 \t\t\t Iteration 10 Loss Valid: 0.6931487917900085\n",
      "Epoch: 4 \t\t\t Iteration 11 Loss Valid: 0.6931542158126831\n",
      "Epoch: 4 \t\t\t Iteration 12 Loss Valid: 0.6931525468826294\n",
      "Epoch: 4 \t\t\t Iteration 13 Loss Valid: 0.6931504607200623\n",
      "Epoch: 4 \t\t\t Iteration 14 Loss Valid: 0.693151593208313\n",
      "Epoch: 4 \t\t\t Iteration 15 Loss Valid: 0.6931525468826294\n",
      "Epoch: 4 \t\t\t Iteration 16 Loss Valid: 0.6931558847427368\n",
      "Epoch: 4 \t\t\t Iteration 17 Loss Valid: 0.6931502223014832\n",
      "Epoch: 4 \t\t\t Iteration 18 Loss Valid: 0.6931479573249817\n",
      "Epoch: 4 \t\t\t Iteration 19 Loss Valid: 0.693148136138916\n",
      "Epoch: 4 \t\t\t Iteration 20 Loss Valid: 0.6931517720222473\n",
      "Epoch: 4 \t\t\t Iteration 21 Loss Valid: 0.6931553483009338\n",
      "Epoch: 4 \t\t\t Iteration 22 Loss Valid: 0.6931601762771606\n",
      "Epoch: 4 \t\t\t Iteration 23 Loss Valid: 0.6931525468826294\n",
      "Epoch: 4 \t\t\t Iteration 24 Loss Valid: 0.6931517124176025\n",
      "Epoch: 4 \t\t\t Iteration 25 Loss Valid: 0.693145751953125\n",
      "Epoch: 4 \t\t\t Iteration 26 Loss Valid: 0.6931532025337219\n",
      "Epoch: 4 \t\t\t Iteration 27 Loss Valid: 0.6931406259536743\n",
      "Epoch: 4 \t\t\t Iteration 28 Loss Valid: 0.6931491494178772\n",
      "Epoch: 4 \t\t\t Iteration 29 Loss Valid: 0.693146824836731\n",
      "Epoch: 4 \t\t\t Iteration 30 Loss Valid: 0.693149745464325\n",
      "Epoch: 4 \t\t\t Iteration 31 Loss Valid: 0.6931518912315369\n",
      "Epoch: 4 \t\t\t Iteration 32 Loss Valid: 0.6931648254394531\n",
      "Epoch: 4 \t\t\t Iteration 33 Loss Valid: 0.6931449174880981\n",
      "Epoch: 4 \t\t\t Iteration 34 Loss Valid: 0.6931438446044922\n",
      "Epoch: 4 \t\t\t Iteration 35 Loss Valid: 0.693148136138916\n",
      "Epoch: 4 \t\t\t Iteration 36 Loss Valid: 0.6931459903717041\n",
      "Epoch: 4 \t\t\t Iteration 37 Loss Valid: 0.6931544542312622\n",
      "Epoch: 4 \t\t\t Iteration 38 Loss Valid: 0.6931490898132324\n",
      "Epoch: 4 \t\t\t Iteration 39 Loss Valid: 0.693145751953125\n",
      "Epoch: 4 \t\t\t Iteration 40 Loss Valid: 0.6931560039520264\n",
      "Epoch: 4 \t\t\t Iteration 41 Loss Valid: 0.6931486129760742\n",
      "Epoch: 4 \t\t\t Iteration 42 Loss Valid: 0.6931658387184143\n",
      "Epoch: 4 \t\t\t Iteration 43 Loss Valid: 0.6931898593902588\n",
      "Epoch: 4 \t\t\t Iteration 44 Loss Valid: 0.6931517124176025\n",
      "Epoch: 4 \t\t\t Iteration 45 Loss Valid: 0.6931695938110352\n",
      "Epoch: 4 \t\t\t Iteration 46 Loss Valid: 0.6931793689727783\n",
      "Epoch: 4 \t\t\t Iteration 47 Loss Valid: 0.6932436227798462\n",
      "Epoch: 4 \t\t\t Iteration 48 Loss Valid: 0.6932342052459717\n",
      "Epoch: 4 \t\t\t Iteration 49 Loss Valid: 0.6931884288787842\n",
      "Epoch: 4 \t\t\t Iteration 50 Loss Valid: 0.6932241916656494\n",
      "Epoch: 4 \t\t\t Iteration 51 Loss Valid: 0.693145751953125\n",
      "Epoch: 4 \t\t\t Iteration 52 Loss Valid: 0.6931867599487305\n",
      "Epoch: 4 \t\t\t Iteration 53 Loss Valid: 0.6932222843170166\n",
      "Epoch: 4 \t\t\t Iteration 54 Loss Valid: 0.6931256055831909\n",
      "Epoch: 4 \t\t\t Iteration 55 Loss Valid: 0.6930949091911316\n",
      "Epoch: 4 \t\t\t Iteration 56 Loss Valid: 0.6930854916572571\n",
      "Epoch: 4 \t\t\t Iteration 57 Loss Valid: 0.6931218504905701\n",
      "Epoch: 4 \t\t\t Iteration 58 Loss Valid: 0.6930863857269287\n",
      "Epoch: 4 \t\t\t Iteration 59 Loss Valid: 0.6930991411209106\n",
      "Epoch: 4 \t\t\t Iteration 60 Loss Valid: 0.6931177973747253\n",
      "Epoch: 4 \t\t\t Iteration 61 Loss Valid: 0.6930902004241943\n",
      "Epoch: 4 / 5 \t\t\t Training Loss:0.8335887456177086\n",
      "Epoch: 5 \t\t\t Iteration 1 Loss Train: 0.6931527853012085\n",
      "Epoch: 5 \t\t\t Iteration 2 Loss Train: 0.6931682229042053\n",
      "Epoch: 5 \t\t\t Iteration 3 Loss Train: 0.6931447982788086\n",
      "Epoch: 5 \t\t\t Iteration 4 Loss Train: 0.693152904510498\n",
      "Epoch: 5 \t\t\t Iteration 5 Loss Train: 0.6931629776954651\n",
      "Epoch: 5 \t\t\t Iteration 6 Loss Train: 0.6931208372116089\n",
      "Epoch: 5 \t\t\t Iteration 7 Loss Train: 0.6931276321411133\n",
      "Epoch: 5 \t\t\t Iteration 8 Loss Train: 0.6931625008583069\n",
      "Epoch: 5 \t\t\t Iteration 9 Loss Train: 0.6931257843971252\n",
      "Epoch: 5 \t\t\t Iteration 10 Loss Train: 0.6931699514389038\n",
      "Epoch: 5 \t\t\t Iteration 11 Loss Train: 0.6931512355804443\n",
      "Epoch: 5 \t\t\t Iteration 12 Loss Train: 0.6931648254394531\n",
      "Epoch: 5 \t\t\t Iteration 13 Loss Train: 0.6931316256523132\n",
      "Epoch: 5 \t\t\t Iteration 14 Loss Train: 0.6931015849113464\n",
      "Epoch: 5 \t\t\t Iteration 15 Loss Train: 0.6932032108306885\n",
      "Epoch: 5 \t\t\t Iteration 16 Loss Train: 0.6931663155555725\n",
      "Epoch: 5 \t\t\t Iteration 17 Loss Train: 0.6930642127990723\n",
      "Epoch: 5 \t\t\t Iteration 18 Loss Train: 0.692999005317688\n",
      "Epoch: 5 \t\t\t Iteration 19 Loss Train: 0.6931132078170776\n",
      "Epoch: 5 \t\t\t Iteration 20 Loss Train: 0.6932138800621033\n",
      "Epoch: 5 \t\t\t Iteration 21 Loss Train: 0.6931085586547852\n",
      "Epoch: 5 \t\t\t Iteration 22 Loss Train: 0.6931514739990234\n",
      "Epoch: 5 \t\t\t Iteration 23 Loss Train: 0.6930504441261292\n",
      "Epoch: 5 \t\t\t Iteration 24 Loss Train: 0.6930720806121826\n",
      "Epoch: 5 \t\t\t Iteration 25 Loss Train: 0.6929913759231567\n",
      "Epoch: 5 \t\t\t Iteration 26 Loss Train: 0.6929736137390137\n",
      "Epoch: 5 \t\t\t Iteration 27 Loss Train: 0.6930487155914307\n",
      "Epoch: 5 \t\t\t Iteration 28 Loss Train: 0.6930360794067383\n",
      "Epoch: 5 \t\t\t Iteration 29 Loss Train: 0.6929689645767212\n",
      "Epoch: 5 \t\t\t Iteration 30 Loss Train: 0.6930077075958252\n",
      "Epoch: 5 \t\t\t Iteration 31 Loss Train: 0.693002462387085\n",
      "Epoch: 5 \t\t\t Iteration 32 Loss Train: 0.6928603649139404\n",
      "Epoch: 5 \t\t\t Iteration 33 Loss Train: 0.6930772066116333\n",
      "Epoch: 5 \t\t\t Iteration 34 Loss Train: 0.6931126117706299\n",
      "Epoch: 5 \t\t\t Iteration 35 Loss Train: 0.6931212544441223\n",
      "Epoch: 5 \t\t\t Iteration 36 Loss Train: 0.6931556463241577\n",
      "Epoch: 5 \t\t\t Iteration 37 Loss Train: 0.6934232711791992\n",
      "Epoch: 5 \t\t\t Iteration 38 Loss Train: 0.6929234862327576\n",
      "Epoch: 5 \t\t\t Iteration 39 Loss Train: 0.6930782794952393\n",
      "Epoch: 5 \t\t\t Iteration 40 Loss Train: 0.6930170059204102\n",
      "Epoch: 5 \t\t\t Iteration 41 Loss Train: 0.693034291267395\n",
      "Epoch: 5 \t\t\t Iteration 42 Loss Train: 0.6928189992904663\n",
      "Epoch: 5 \t\t\t Iteration 43 Loss Train: 0.6929216384887695\n",
      "Epoch: 5 \t\t\t Iteration 44 Loss Train: 0.6931465864181519\n",
      "Epoch: 5 \t\t\t Iteration 45 Loss Train: 0.6930103302001953\n",
      "Epoch: 5 \t\t\t Iteration 46 Loss Train: 0.6931309700012207\n",
      "Epoch: 5 \t\t\t Iteration 47 Loss Train: 0.6928442716598511\n",
      "Epoch: 5 \t\t\t Iteration 48 Loss Train: 0.6929674744606018\n",
      "Epoch: 5 \t\t\t Iteration 49 Loss Train: 0.692836582660675\n",
      "Epoch: 5 \t\t\t Iteration 50 Loss Train: 0.6932011246681213\n",
      "Epoch: 5 \t\t\t Iteration 51 Loss Train: 0.6929899454116821\n",
      "Epoch: 5 \t\t\t Iteration 52 Loss Train: 0.6925531625747681\n",
      "Epoch: 5 \t\t\t Iteration 53 Loss Train: 0.6934517621994019\n",
      "Epoch: 5 \t\t\t Iteration 54 Loss Train: 0.6931476593017578\n",
      "Epoch: 5 \t\t\t Iteration 55 Loss Train: 0.6934365034103394\n",
      "Epoch: 5 \t\t\t Iteration 56 Loss Train: 0.6933766603469849\n",
      "Epoch: 5 \t\t\t Iteration 57 Loss Train: 0.6929835081100464\n",
      "Epoch: 5 \t\t\t Iteration 58 Loss Train: 0.6929366588592529\n",
      "Epoch: 5 \t\t\t Iteration 59 Loss Train: 0.692926287651062\n",
      "Epoch: 5 \t\t\t Iteration 60 Loss Train: 0.6927970051765442\n",
      "Epoch: 5 \t\t\t Iteration 61 Loss Train: 0.6926674842834473\n",
      "Epoch: 5 \t\t\t Iteration 62 Loss Train: 0.6931480169296265\n",
      "Epoch: 5 \t\t\t Iteration 63 Loss Train: 0.6930079460144043\n",
      "Epoch: 5 \t\t\t Iteration 64 Loss Train: 0.6930932402610779\n",
      "Epoch: 5 \t\t\t Iteration 65 Loss Train: 0.6931605339050293\n",
      "Epoch: 5 \t\t\t Iteration 66 Loss Train: 0.6924434304237366\n",
      "Epoch: 5 \t\t\t Iteration 67 Loss Train: 0.693146288394928\n",
      "Epoch: 5 \t\t\t Iteration 68 Loss Train: 0.6934828162193298\n",
      "Epoch: 5 \t\t\t Iteration 69 Loss Train: 0.6928827166557312\n",
      "Epoch: 5 \t\t\t Iteration 70 Loss Train: 0.692861795425415\n",
      "Epoch: 5 \t\t\t Iteration 71 Loss Train: 0.6926774978637695\n",
      "Epoch: 5 \t\t\t Iteration 72 Loss Train: 0.6932317614555359\n",
      "Epoch: 5 \t\t\t Iteration 73 Loss Train: 0.6928749680519104\n",
      "Epoch: 5 \t\t\t Iteration 74 Loss Train: 0.6929855346679688\n",
      "Epoch: 5 \t\t\t Iteration 75 Loss Train: 0.6930100917816162\n",
      "Epoch: 5 \t\t\t Iteration 76 Loss Train: 0.6929200887680054\n",
      "Epoch: 5 \t\t\t Iteration 77 Loss Train: 0.6932327747344971\n",
      "Epoch: 5 \t\t\t Iteration 78 Loss Train: 0.6928561925888062\n",
      "Epoch: 5 \t\t\t Iteration 79 Loss Train: 0.6931401491165161\n",
      "Epoch: 5 \t\t\t Iteration 80 Loss Train: 0.6929119229316711\n",
      "Epoch: 5 \t\t\t Iteration 81 Loss Train: 0.6930744647979736\n",
      "Epoch: 5 \t\t\t Iteration 82 Loss Train: 0.6930603384971619\n",
      "Epoch: 5 \t\t\t Iteration 83 Loss Train: 0.6937137842178345\n",
      "Epoch: 5 \t\t\t Iteration 84 Loss Train: 0.6930760741233826\n",
      "Epoch: 5 \t\t\t Iteration 85 Loss Train: 0.6933771371841431\n",
      "Epoch: 5 \t\t\t Iteration 86 Loss Train: 0.6927021741867065\n",
      "Epoch: 5 \t\t\t Iteration 87 Loss Train: 0.6929991245269775\n",
      "Epoch: 5 \t\t\t Iteration 88 Loss Train: 0.692767858505249\n",
      "Epoch: 5 \t\t\t Iteration 89 Loss Train: 0.6927424669265747\n",
      "Epoch: 5 \t\t\t Iteration 90 Loss Train: 0.6929822564125061\n",
      "Epoch: 5 \t\t\t Iteration 91 Loss Train: 0.6929985284805298\n",
      "Epoch: 5 \t\t\t Iteration 92 Loss Train: 0.6931426525115967\n",
      "Epoch: 5 \t\t\t Iteration 93 Loss Train: 0.6928272247314453\n",
      "Epoch: 5 \t\t\t Iteration 94 Loss Train: 0.6930606365203857\n",
      "Epoch: 5 \t\t\t Iteration 95 Loss Train: 0.692550539970398\n",
      "Epoch: 5 \t\t\t Iteration 96 Loss Train: 0.6929116249084473\n",
      "Epoch: 5 \t\t\t Iteration 97 Loss Train: 0.6923628449440002\n",
      "Epoch: 5 \t\t\t Iteration 98 Loss Train: 0.6926873922348022\n",
      "Epoch: 5 \t\t\t Iteration 99 Loss Train: 0.693334698677063\n",
      "Epoch: 5 \t\t\t Iteration 100 Loss Train: 0.6932483911514282\n",
      "Epoch: 5 \t\t\t Iteration 101 Loss Train: 0.6926023960113525\n",
      "Epoch: 5 \t\t\t Iteration 102 Loss Train: 0.6927962303161621\n",
      "Epoch: 5 \t\t\t Iteration 103 Loss Train: 0.6928642392158508\n",
      "Epoch: 5 \t\t\t Iteration 104 Loss Train: 0.6925613880157471\n",
      "Epoch: 5 \t\t\t Iteration 105 Loss Train: 0.6929578185081482\n",
      "Epoch: 5 \t\t\t Iteration 106 Loss Train: 0.6932324171066284\n",
      "Epoch: 5 \t\t\t Iteration 107 Loss Train: 0.69325852394104\n",
      "Epoch: 5 \t\t\t Iteration 108 Loss Train: 0.6927667260169983\n",
      "Epoch: 5 \t\t\t Iteration 109 Loss Train: 0.6930314302444458\n",
      "Epoch: 5 \t\t\t Iteration 110 Loss Train: 0.6928296089172363\n",
      "Epoch: 5 \t\t\t Iteration 111 Loss Train: 0.692546546459198\n",
      "Epoch: 5 \t\t\t Iteration 112 Loss Train: 0.6922023892402649\n",
      "Epoch: 5 \t\t\t Iteration 113 Loss Train: 0.6935812830924988\n",
      "Epoch: 5 \t\t\t Iteration 114 Loss Train: 0.6931616067886353\n",
      "Epoch: 5 \t\t\t Iteration 115 Loss Train: 0.6928274631500244\n",
      "Epoch: 5 \t\t\t Iteration 116 Loss Train: 0.6931561231613159\n",
      "Epoch: 5 \t\t\t Iteration 117 Loss Train: 0.6926462650299072\n",
      "Epoch: 5 \t\t\t Iteration 118 Loss Train: 0.6934806108474731\n",
      "Epoch: 5 \t\t\t Iteration 119 Loss Train: 0.692833423614502\n",
      "Epoch: 5 \t\t\t Iteration 120 Loss Train: 0.6933699250221252\n",
      "Epoch: 5 \t\t\t Iteration 121 Loss Train: 0.6936041116714478\n",
      "Epoch: 5 \t\t\t Iteration 122 Loss Train: 0.6926256418228149\n",
      "Epoch: 5 \t\t\t Iteration 123 Loss Train: 0.6927188634872437\n",
      "Epoch: 5 \t\t\t Iteration 124 Loss Train: 0.6929314136505127\n",
      "Epoch: 5 \t\t\t Iteration 125 Loss Train: 0.6934977769851685\n",
      "Epoch: 5 \t\t\t Iteration 126 Loss Train: 0.6932517290115356\n",
      "Epoch: 5 \t\t\t Iteration 127 Loss Train: 0.6934812068939209\n",
      "Epoch: 5 \t\t\t Iteration 128 Loss Train: 0.6931750774383545\n",
      "Epoch: 5 \t\t\t Iteration 129 Loss Train: 0.6931530237197876\n",
      "Epoch: 5 \t\t\t Iteration 130 Loss Train: 0.6933610439300537\n",
      "Epoch: 5 \t\t\t Iteration 131 Loss Train: 0.6932750940322876\n",
      "Epoch: 5 \t\t\t Iteration 132 Loss Train: 0.6922913193702698\n",
      "Epoch: 5 \t\t\t Iteration 133 Loss Train: 0.6923622488975525\n",
      "Epoch: 5 \t\t\t Iteration 134 Loss Train: 0.6928028464317322\n",
      "Epoch: 5 \t\t\t Iteration 135 Loss Train: 0.6928079128265381\n",
      "Epoch: 5 \t\t\t Iteration 136 Loss Train: 0.6929157972335815\n",
      "Epoch: 5 \t\t\t Iteration 137 Loss Train: 0.6926859021186829\n",
      "Epoch: 5 \t\t\t Iteration 138 Loss Train: 0.6936212778091431\n",
      "Epoch: 5 \t\t\t Iteration 139 Loss Train: 0.6927982568740845\n",
      "Epoch: 5 \t\t\t Iteration 140 Loss Train: 0.6935082674026489\n",
      "Epoch: 5 \t\t\t Iteration 141 Loss Train: 0.6932809352874756\n",
      "Epoch: 5 \t\t\t Iteration 142 Loss Train: 0.6933847665786743\n",
      "Epoch: 5 \t\t\t Iteration 143 Loss Train: 0.6928045749664307\n",
      "Epoch: 5 \t\t\t Iteration 144 Loss Train: 0.6927882432937622\n",
      "Epoch: 5 \t\t\t Iteration 145 Loss Train: 0.6925681829452515\n",
      "Epoch: 5 \t\t\t Iteration 146 Loss Train: 0.6931784152984619\n",
      "Epoch: 5 \t\t\t Iteration 147 Loss Train: 0.6924500465393066\n",
      "Epoch: 5 \t\t\t Iteration 148 Loss Train: 0.6931727528572083\n",
      "Epoch: 5 \t\t\t Iteration 149 Loss Train: 0.6919612884521484\n",
      "Epoch: 5 \t\t\t Iteration 150 Loss Train: 0.6925346851348877\n",
      "Epoch: 5 \t\t\t Iteration 151 Loss Train: 0.6928931474685669\n",
      "Epoch: 5 \t\t\t Iteration 152 Loss Train: 0.6930193901062012\n",
      "Epoch: 5 \t\t\t Iteration 153 Loss Train: 0.6935313940048218\n",
      "Epoch: 5 \t\t\t Iteration 154 Loss Train: 0.6930267810821533\n",
      "Epoch: 5 \t\t\t Iteration 155 Loss Train: 0.693278431892395\n",
      "Epoch: 5 \t\t\t Iteration 156 Loss Train: 0.6923974752426147\n",
      "Epoch: 5 \t\t\t Iteration 157 Loss Train: 0.692493200302124\n",
      "Epoch: 5 \t\t\t Iteration 158 Loss Train: 0.6926373243331909\n",
      "Epoch: 5 \t\t\t Iteration 159 Loss Train: 0.6934212446212769\n",
      "Epoch: 5 \t\t\t Iteration 160 Loss Train: 0.6926236152648926\n",
      "Epoch: 5 \t\t\t Iteration 161 Loss Train: 0.6929101943969727\n",
      "Epoch: 5 \t\t\t Iteration 162 Loss Train: 0.692223072052002\n",
      "Epoch: 5 \t\t\t Iteration 163 Loss Train: 0.6926113367080688\n",
      "Epoch: 5 \t\t\t Iteration 164 Loss Train: 0.692460298538208\n",
      "Epoch: 5 \t\t\t Iteration 165 Loss Train: 0.6919121742248535\n",
      "Epoch: 5 \t\t\t Iteration 166 Loss Train: 0.6925976872444153\n",
      "Epoch: 5 \t\t\t Iteration 167 Loss Train: 0.6918658018112183\n",
      "Epoch: 5 \t\t\t Iteration 168 Loss Train: 0.6931512951850891\n",
      "Epoch: 5 \t\t\t Iteration 169 Loss Train: 0.6941900253295898\n",
      "Epoch: 5 \t\t\t Iteration 170 Loss Train: 0.692718505859375\n",
      "Epoch: 5 \t\t\t Iteration 171 Loss Train: 0.6924268007278442\n",
      "Epoch: 5 \t\t\t Iteration 172 Loss Train: 0.6930022835731506\n",
      "Epoch: 5 \t\t\t Iteration 173 Loss Train: 0.6925715208053589\n",
      "Epoch: 5 \t\t\t Iteration 174 Loss Train: 0.6919623017311096\n",
      "Epoch: 5 \t\t\t Iteration 175 Loss Train: 0.6921027302742004\n",
      "Epoch: 5 \t\t\t Iteration 176 Loss Train: 0.6933234333992004\n",
      "Epoch: 5 \t\t\t Iteration 177 Loss Train: 0.6931610703468323\n",
      "Epoch: 5 \t\t\t Iteration 178 Loss Train: 0.6930009126663208\n",
      "Epoch: 5 \t\t\t Iteration 179 Loss Train: 0.6934585571289062\n",
      "Epoch: 5 \t\t\t Iteration 180 Loss Train: 0.6930025815963745\n",
      "Epoch: 5 \t\t\t Iteration 181 Loss Train: 0.6939325332641602\n",
      "Epoch: 5 \t\t\t Iteration 182 Loss Train: 0.6934723854064941\n",
      "Epoch: 5 \t\t\t Iteration 183 Loss Train: 0.6937729120254517\n",
      "Epoch: 5 \t\t\t Iteration 184 Loss Train: 0.6940672993659973\n",
      "Epoch: 5 \t\t\t Iteration 185 Loss Train: 0.6919806003570557\n",
      "Epoch: 5 \t\t\t Iteration 186 Loss Train: 0.6925642490386963\n",
      "Epoch: 5 \t\t\t Iteration 187 Loss Train: 0.6933082938194275\n",
      "Epoch: 5 \t\t\t Iteration 188 Loss Train: 0.692398190498352\n",
      "Epoch: 5 \t\t\t Iteration 189 Loss Train: 0.6921008825302124\n",
      "Epoch: 5 \t\t\t Iteration 190 Loss Train: 0.6918979287147522\n",
      "Epoch: 5 \t\t\t Iteration 191 Loss Train: 0.6926909685134888\n",
      "Epoch: 5 \t\t\t Iteration 192 Loss Train: 0.6935012936592102\n",
      "Epoch: 5 \t\t\t Iteration 193 Loss Train: 0.6923589706420898\n",
      "Epoch: 5 \t\t\t Iteration 194 Loss Train: 0.691390872001648\n",
      "Epoch: 5 \t\t\t Iteration 195 Loss Train: 0.6913335919380188\n",
      "Epoch: 5 \t\t\t Iteration 196 Loss Train: 0.6929750442504883\n",
      "Epoch: 5 \t\t\t Iteration 197 Loss Train: 0.6929934024810791\n",
      "Epoch: 5 \t\t\t Iteration 198 Loss Train: 0.6921480298042297\n",
      "Epoch: 5 \t\t\t Iteration 199 Loss Train: 0.692659854888916\n",
      "Epoch: 5 \t\t\t Iteration 200 Loss Train: 0.6922881603240967\n",
      "Epoch: 5 \t\t\t Iteration 201 Loss Train: 0.692131757736206\n",
      "Epoch: 5 \t\t\t Iteration 202 Loss Train: 0.6926390528678894\n",
      "Epoch: 5 \t\t\t Iteration 203 Loss Train: 0.694044828414917\n",
      "Epoch: 5 \t\t\t Iteration 204 Loss Train: 0.6933457851409912\n",
      "Epoch: 5 \t\t\t Iteration 205 Loss Train: 0.6922857761383057\n",
      "Epoch: 5 \t\t\t Iteration 206 Loss Train: 0.6935182213783264\n",
      "Epoch: 5 \t\t\t Iteration 207 Loss Train: 0.6915711164474487\n",
      "Epoch: 5 \t\t\t Iteration 208 Loss Train: 0.6929654479026794\n",
      "Epoch: 5 \t\t\t Iteration 209 Loss Train: 0.6924495697021484\n",
      "Epoch: 5 \t\t\t Iteration 210 Loss Train: 0.6929751634597778\n",
      "Epoch: 5 \t\t\t Iteration 211 Loss Train: 0.692991316318512\n",
      "Epoch: 5 \t\t\t Iteration 212 Loss Train: 0.6929815411567688\n",
      "Epoch: 5 \t\t\t Iteration 213 Loss Train: 0.693705141544342\n",
      "Epoch: 5 \t\t\t Iteration 214 Loss Train: 0.6935490369796753\n",
      "Epoch: 5 \t\t\t Iteration 215 Loss Train: 0.6929908394813538\n",
      "Epoch: 5 \t\t\t Iteration 216 Loss Train: 0.6929815411567688\n",
      "Epoch: 5 \t\t\t Iteration 217 Loss Train: 0.6924373507499695\n",
      "Epoch: 5 \t\t\t Iteration 218 Loss Train: 0.6927835941314697\n",
      "Epoch: 5 \t\t\t Iteration 219 Loss Train: 0.6931589841842651\n",
      "Epoch: 5 \t\t\t Iteration 220 Loss Train: 0.6933518648147583\n",
      "Epoch: 5 \t\t\t Iteration 221 Loss Train: 0.6918846368789673\n",
      "Epoch: 5 \t\t\t Iteration 222 Loss Train: 0.6931643486022949\n",
      "Epoch: 5 \t\t\t Iteration 223 Loss Train: 0.6918504238128662\n",
      "Epoch: 5 \t\t\t Iteration 224 Loss Train: 0.6920364499092102\n",
      "Epoch: 5 \t\t\t Iteration 225 Loss Train: 0.6924014687538147\n",
      "Epoch: 5 \t\t\t Iteration 226 Loss Train: 0.6925690770149231\n",
      "Epoch: 5 \t\t\t Iteration 227 Loss Train: 0.6914092898368835\n",
      "Epoch: 5 \t\t\t Iteration 228 Loss Train: 0.693753719329834\n",
      "Epoch: 5 \t\t\t Iteration 229 Loss Train: 0.6923881769180298\n",
      "Epoch: 5 \t\t\t Iteration 230 Loss Train: 0.6921842694282532\n",
      "Epoch: 5 \t\t\t Iteration 231 Loss Train: 0.6929680109024048\n",
      "Epoch: 5 \t\t\t Iteration 232 Loss Train: 0.6921651363372803\n",
      "Epoch: 5 \t\t\t Iteration 233 Loss Train: 0.6937873959541321\n",
      "Epoch: 5 \t\t\t Iteration 234 Loss Train: 0.6913690567016602\n",
      "Epoch: 5 \t\t\t Iteration 235 Loss Train: 0.6925503015518188\n",
      "Epoch: 5 \t\t\t Iteration 236 Loss Train: 0.6921498775482178\n",
      "Epoch: 5 \t\t\t Iteration 237 Loss Train: 0.6938045620918274\n",
      "Epoch: 5 \t\t\t Iteration 238 Loss Train: 0.692144513130188\n",
      "Epoch: 5 \t\t\t Iteration 239 Loss Train: 0.6927679181098938\n",
      "Epoch: 5 \t\t\t Iteration 240 Loss Train: 0.6929632425308228\n",
      "Epoch: 5 \t\t\t Iteration 241 Loss Train: 0.6917155981063843\n",
      "Epoch: 5 \t\t\t Iteration 242 Loss Train: 0.6920961737632751\n",
      "Epoch: 5 \t\t\t Iteration 243 Loss Train: 0.6936061382293701\n",
      "Epoch: 5 \t\t\t Iteration 244 Loss Train: 0.6931695938110352\n",
      "Epoch: 5 \t\t\t Iteration 245 Loss Train: 0.6914850473403931\n",
      "Epoch: 5 \t\t\t Iteration 246 Loss Train: 0.6929613351821899\n",
      "Epoch: 5 \t\t\t Iteration 247 Loss Train: 0.6925076842308044\n",
      "Epoch: 5 \t\t\t Iteration 248 Loss Train: 0.6931738257408142\n",
      "Epoch: 5 \t\t\t Iteration 249 Loss Train: 0.6920833587646484\n",
      "Epoch: 5 \t\t\t Iteration 250 Loss Train: 0.6923130750656128\n",
      "Epoch: 5 \t\t\t Iteration 251 Loss Train: 0.6920806169509888\n",
      "Epoch: 5 \t\t\t Iteration 252 Loss Train: 0.6927317976951599\n",
      "Epoch: 5 \t\t\t Iteration 253 Loss Train: 0.6918447613716125\n",
      "Epoch: 5 \t\t\t Iteration 254 Loss Train: 0.6913912892341614\n",
      "Epoch: 5 \t\t\t Iteration 255 Loss Train: 0.6915922164916992\n",
      "Epoch: 5 \t\t\t Iteration 256 Loss Train: 0.68998783826828\n",
      "Epoch: 5 \t\t\t Iteration 257 Loss Train: 0.6924548745155334\n",
      "Epoch: 5 \t\t\t Iteration 258 Loss Train: 0.6936523914337158\n",
      "Epoch: 5 \t\t\t Iteration 259 Loss Train: 0.6910710334777832\n",
      "Epoch: 5 \t\t\t Iteration 260 Loss Train: 0.6929452419281006\n",
      "Epoch: 5 \t\t\t Iteration 261 Loss Train: 0.69269859790802\n",
      "Epoch: 5 \t\t\t Iteration 262 Loss Train: 0.692475438117981\n",
      "Epoch: 5 \t\t\t Iteration 263 Loss Train: 0.6922215223312378\n",
      "Epoch: 5 \t\t\t Iteration 264 Loss Train: 0.6934092044830322\n",
      "Epoch: 5 \t\t\t Iteration 265 Loss Train: 0.6924571394920349\n",
      "Epoch: 5 \t\t\t Iteration 266 Loss Train: 0.6922436356544495\n",
      "Epoch: 5 \t\t\t Iteration 267 Loss Train: 0.6924822330474854\n",
      "Epoch: 5 \t\t\t Iteration 268 Loss Train: 0.6926873922348022\n",
      "Epoch: 5 \t\t\t Iteration 269 Loss Train: 0.6926944255828857\n",
      "Epoch: 5 \t\t\t Iteration 270 Loss Train: 0.6919555068016052\n",
      "Epoch: 5 \t\t\t Iteration 271 Loss Train: 0.6924456357955933\n",
      "Epoch: 5 \t\t\t Iteration 272 Loss Train: 0.69196617603302\n",
      "Epoch: 5 \t\t\t Iteration 273 Loss Train: 0.6914652585983276\n",
      "Epoch: 5 \t\t\t Iteration 274 Loss Train: 0.6924291849136353\n",
      "Epoch: 5 \t\t\t Iteration 275 Loss Train: 0.6931752562522888\n",
      "Epoch: 5 \t\t\t Iteration 276 Loss Train: 0.6924549341201782\n",
      "Epoch: 5 \t\t\t Iteration 277 Loss Train: 0.6929386854171753\n",
      "Epoch: 5 \t\t\t Iteration 278 Loss Train: 0.6921666860580444\n",
      "Epoch: 5 \t\t\t Iteration 279 Loss Train: 0.6919310092926025\n",
      "Epoch: 5 \t\t\t Iteration 280 Loss Train: 0.6919221878051758\n",
      "Epoch: 5 \t\t\t Iteration 281 Loss Train: 0.6926698088645935\n",
      "Epoch: 5 \t\t\t Iteration 282 Loss Train: 0.6936986446380615\n",
      "Epoch: 5 \t\t\t Iteration 283 Loss Train: 0.6931567192077637\n",
      "Epoch: 5 \t\t\t Iteration 284 Loss Train: 0.6926491260528564\n",
      "Epoch: 5 \t\t\t Iteration 285 Loss Train: 0.6921632289886475\n",
      "Epoch: 5 \t\t\t Iteration 286 Loss Train: 0.6929327249526978\n",
      "Epoch: 5 \t\t\t Iteration 287 Loss Train: 0.6934430599212646\n",
      "Epoch: 5 \t\t\t Iteration 288 Loss Train: 0.6942129135131836\n",
      "Epoch: 5 \t\t\t Iteration 289 Loss Train: 0.6898853182792664\n",
      "Epoch: 5 \t\t\t Iteration 290 Loss Train: 0.6926717162132263\n",
      "Epoch: 5 \t\t\t Iteration 291 Loss Train: 0.6937164664268494\n",
      "Epoch: 5 \t\t\t Iteration 292 Loss Train: 0.691895604133606\n",
      "Epoch: 5 \t\t\t Iteration 293 Loss Train: 0.6929197907447815\n",
      "Epoch: 5 \t\t\t Iteration 294 Loss Train: 0.6921381950378418\n",
      "Epoch: 5 \t\t\t Iteration 295 Loss Train: 0.6934380531311035\n",
      "Epoch: 5 \t\t\t Iteration 296 Loss Train: 0.6926587820053101\n",
      "Epoch: 5 \t\t\t Iteration 297 Loss Train: 0.6911106705665588\n",
      "Epoch: 5 \t\t\t Iteration 298 Loss Train: 0.6913176774978638\n",
      "Epoch: 5 \t\t\t Iteration 299 Loss Train: 0.693192183971405\n",
      "Epoch: 5 \t\t\t Iteration 300 Loss Train: 0.6937211751937866\n",
      "Epoch: 5 \t\t\t Iteration 301 Loss Train: 0.6913211345672607\n",
      "Epoch: 5 \t\t\t Iteration 302 Loss Train: 0.6929632425308228\n",
      "Epoch: 5 \t\t\t Iteration 1 Loss Valid: 0.6929197311401367\n",
      "Epoch: 5 \t\t\t Iteration 2 Loss Valid: 0.6931776404380798\n",
      "Epoch: 5 \t\t\t Iteration 3 Loss Valid: 0.6934562921524048\n",
      "Epoch: 5 \t\t\t Iteration 4 Loss Valid: 0.6929122805595398\n",
      "Epoch: 5 \t\t\t Iteration 5 Loss Valid: 0.693188488483429\n",
      "Epoch: 5 \t\t\t Iteration 6 Loss Valid: 0.6939808130264282\n",
      "Epoch: 5 \t\t\t Iteration 7 Loss Valid: 0.6929141283035278\n",
      "Epoch: 5 \t\t\t Iteration 8 Loss Valid: 0.6934558749198914\n",
      "Epoch: 5 \t\t\t Iteration 9 Loss Valid: 0.6929123401641846\n",
      "Epoch: 5 \t\t\t Iteration 10 Loss Valid: 0.6926451921463013\n",
      "Epoch: 5 \t\t\t Iteration 11 Loss Valid: 0.694265604019165\n",
      "Epoch: 5 \t\t\t Iteration 12 Loss Valid: 0.6934574842453003\n",
      "Epoch: 5 \t\t\t Iteration 13 Loss Valid: 0.6931859254837036\n",
      "Epoch: 5 \t\t\t Iteration 14 Loss Valid: 0.6929195523262024\n",
      "Epoch: 5 \t\t\t Iteration 15 Loss Valid: 0.6929205656051636\n",
      "Epoch: 5 \t\t\t Iteration 16 Loss Valid: 0.6921154260635376\n",
      "Epoch: 5 \t\t\t Iteration 17 Loss Valid: 0.6926447153091431\n",
      "Epoch: 5 \t\t\t Iteration 18 Loss Valid: 0.6929156184196472\n",
      "Epoch: 5 \t\t\t Iteration 19 Loss Valid: 0.6931845545768738\n",
      "Epoch: 5 \t\t\t Iteration 20 Loss Valid: 0.6929172873497009\n",
      "Epoch: 5 \t\t\t Iteration 21 Loss Valid: 0.6923824548721313\n",
      "Epoch: 5 \t\t\t Iteration 22 Loss Valid: 0.6910431981086731\n",
      "Epoch: 5 \t\t\t Iteration 23 Loss Valid: 0.6929180026054382\n",
      "Epoch: 5 \t\t\t Iteration 24 Loss Valid: 0.6934592723846436\n",
      "Epoch: 5 \t\t\t Iteration 25 Loss Valid: 0.6931823492050171\n",
      "Epoch: 5 \t\t\t Iteration 26 Loss Valid: 0.6926487684249878\n",
      "Epoch: 5 \t\t\t Iteration 27 Loss Valid: 0.6931790113449097\n",
      "Epoch: 5 \t\t\t Iteration 28 Loss Valid: 0.6934564113616943\n",
      "Epoch: 5 \t\t\t Iteration 29 Loss Valid: 0.692914605140686\n",
      "Epoch: 5 \t\t\t Iteration 30 Loss Valid: 0.6929182410240173\n",
      "Epoch: 5 \t\t\t Iteration 31 Loss Valid: 0.6923791170120239\n",
      "Epoch: 5 \t\t\t Iteration 32 Loss Valid: 0.6923878192901611\n",
      "Epoch: 5 \t\t\t Iteration 33 Loss Valid: 0.6931829452514648\n",
      "Epoch: 5 \t\t\t Iteration 34 Loss Valid: 0.6929107904434204\n",
      "Epoch: 5 \t\t\t Iteration 35 Loss Valid: 0.6934561729431152\n",
      "Epoch: 5 \t\t\t Iteration 36 Loss Valid: 0.6931827664375305\n",
      "Epoch: 5 \t\t\t Iteration 37 Loss Valid: 0.6926482915878296\n",
      "Epoch: 5 \t\t\t Iteration 38 Loss Valid: 0.692916750907898\n",
      "Epoch: 5 \t\t\t Iteration 39 Loss Valid: 0.6926401853561401\n",
      "Epoch: 5 \t\t\t Iteration 40 Loss Valid: 0.6918395757675171\n",
      "Epoch: 5 \t\t\t Iteration 41 Loss Valid: 0.6931836605072021\n",
      "Epoch: 5 \t\t\t Iteration 42 Loss Valid: 0.6921194195747375\n",
      "Epoch: 5 \t\t\t Iteration 43 Loss Valid: 0.6881148219108582\n",
      "Epoch: 5 \t\t\t Iteration 44 Loss Valid: 0.6845575571060181\n",
      "Epoch: 5 \t\t\t Iteration 45 Loss Valid: 0.684585690498352\n",
      "Epoch: 5 \t\t\t Iteration 46 Loss Valid: 0.6846054196357727\n",
      "Epoch: 5 \t\t\t Iteration 47 Loss Valid: 0.6846480369567871\n",
      "Epoch: 5 \t\t\t Iteration 48 Loss Valid: 0.6846395134925842\n",
      "Epoch: 5 \t\t\t Iteration 49 Loss Valid: 0.6845868229866028\n",
      "Epoch: 5 \t\t\t Iteration 50 Loss Valid: 0.6846261024475098\n",
      "Epoch: 5 \t\t\t Iteration 51 Loss Valid: 0.684555172920227\n",
      "Epoch: 5 \t\t\t Iteration 52 Loss Valid: 0.6845960021018982\n",
      "Epoch: 5 \t\t\t Iteration 53 Loss Valid: 0.6846333146095276\n",
      "Epoch: 5 \t\t\t Iteration 54 Loss Valid: 0.6974774599075317\n",
      "Epoch: 5 \t\t\t Iteration 55 Loss Valid: 0.7017769813537598\n",
      "Epoch: 5 \t\t\t Iteration 56 Loss Valid: 0.7017360925674438\n",
      "Epoch: 5 \t\t\t Iteration 57 Loss Valid: 0.7017776966094971\n",
      "Epoch: 5 \t\t\t Iteration 58 Loss Valid: 0.7017595767974854\n",
      "Epoch: 5 \t\t\t Iteration 59 Loss Valid: 0.7017520666122437\n",
      "Epoch: 5 \t\t\t Iteration 60 Loss Valid: 0.7017666101455688\n",
      "Epoch: 5 \t\t\t Iteration 61 Loss Valid: 0.7017660140991211\n",
      "Epoch: 5 / 5 \t\t\t Training Loss:0.8327035909851656\n"
     ]
    }
   ],
   "source": [
    "# criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train().to(device)\n",
    "    train_loss = 0.0\n",
    "    for data,label in enumerate(train_loader):\n",
    "        img,txt,y = label\n",
    "#         print(img.shape)\n",
    "#         print(txt.shape)\n",
    "#         print(y.unsqueeze(-1).size())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img.to(device),txt.to(device))\n",
    "        # print(output.squeeze())\n",
    "#         break\n",
    "        loss = criterion(output.squeeze(-1).float().to(device), y.float().to(device)) #.unsqueeze(-1).float()# pass the output tensor to the criterion function\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() \n",
    "        print(f'Epoch: {i+1} \\t\\t\\t Iteration {data+1} Loss Train: {loss.item()}')\n",
    "    model.eval().to(device)\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data,label in enumerate(test_loader):\n",
    "          img,txt,y = label\n",
    "          optimizer.zero_grad()\n",
    "          output = model(img.to(device),txt.to(device))\n",
    "  #         print(output.squeeze(-1).size())\n",
    "  #         break\n",
    "          loss = criterion(output.squeeze(-1).float().to(device), y.float().to(device)) #.unsqueeze(-1).float()# pass the output tensor to the criterion function\n",
    "          train_loss += loss.item() \n",
    "          print(f'Epoch: {i+1} \\t\\t\\t Iteration {data+1} Loss Valid: {loss.item()}')\n",
    "\n",
    "    print(f'Epoch: {i+1} / {epochs} \\t\\t\\t Training Loss:{(train_loss/len(train_loader))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62MdQjjrjXOu",
   "metadata": {
    "id": "62MdQjjrjXOu"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"trainedAt100epochs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefca08",
   "metadata": {
    "id": "0fefca08"
   },
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9a44f",
   "metadata": {
    "id": "b9a9a44f"
   },
   "outputs": [],
   "source": [
    "def computeF1Score(preds,targets):\n",
    "    TP,FP,FN=0,0,0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i]==1 and targets[i]==1:\n",
    "            TP+=1\n",
    "        if preds[i]==1 and targets[i]==0:\n",
    "            FP+=1\n",
    "        if preds[i]==0 and targets[i]==1:\n",
    "            FN+=1\n",
    "    if TP==0:\n",
    "        TP+=1\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868aef8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c868aef8",
    "outputId": "5d19c5be-626b-4985-f9d5-9abec63acea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss:2.8605461589625625\n",
      "Correct Predictions: 8721/15912\n",
      "Accuracy: 54.807692307692314 %\n",
      "F1 Score:  0.7080745341614907\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "correct, total = 0,0\n",
    "preds=[]\n",
    "targets=[]\n",
    "model.load_state_dict(torch.load(\"trainedAt100epochs.pth\"))\n",
    "model.eval().to(device)\n",
    "for data,label in enumerate(finaltest_loader):\n",
    "    img,txt,y = label\n",
    "    output = model(img.to(device),txt.to(device))\n",
    "    output = torch.sigmoid(output)\n",
    "    for o,l in zip(output,y):\n",
    "        o = o.round()\n",
    "#         print(o)\n",
    "#         print(l)\n",
    "        preds.append(int(o))\n",
    "        targets.append(int(l))\n",
    "        if o == l:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    output = output.squeeze(-1).float() \n",
    "    loss = criterion(output.to(device),y.float().to(device))\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print(f'Testing Loss:{test_loss/len(test_loader)}')\n",
    "print(f'Correct Predictions: {correct}/{total}')\n",
    "print(f'Accuracy:', correct/total*100,'%')\n",
    "print('F1 Score: ',computeF1Score(preds,targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d0abe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5a4d0abe",
    "outputId": "4aa981b8-565f-4e93-90fd-e97bbbccb2e7"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"927pt\" height=\"1205pt\"\n",
       " viewBox=\"0.00 0.00 927.00 1205.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1201)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-1201 923,-1201 923,4 -4,4\"/>\n",
       "<!-- 139955447282032 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139955447282032</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"618,-31 553,-31 553,0 618,0 618,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"585.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (64, 1)</text>\n",
       "</g>\n",
       "<!-- 139956592833200 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139956592833200</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"636,-86 535,-86 535,-67 636,-67 636,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"585.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139956592833200&#45;&gt;139955447282032 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>139956592833200&#45;&gt;139955447282032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M585.5,-66.79C585.5,-60.07 585.5,-50.4 585.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"589,-41.19 585.5,-31.19 582,-41.19 589,-41.19\"/>\n",
       "</g>\n",
       "<!-- 139955842742016 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139955842742016</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"520,-141 419,-141 419,-122 520,-122 520,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"469.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955842742016&#45;&gt;139956592833200 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139955842742016&#45;&gt;139956592833200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M488.14,-121.98C506.8,-113.46 535.75,-100.23 557.24,-90.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"558.88,-93.51 566.52,-86.17 555.97,-87.14 558.88,-93.51\"/>\n",
       "</g>\n",
       "<!-- 139955446923648 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139955446923648</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"514,-207 425,-207 425,-177 514,-177 514,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"469.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">linear5.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"469.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139955446923648&#45;&gt;139955842742016 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139955446923648&#45;&gt;139955842742016</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M469.5,-176.84C469.5,-169.21 469.5,-159.7 469.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"473,-151.27 469.5,-141.27 466,-151.27 473,-151.27\"/>\n",
       "</g>\n",
       "<!-- 139955842739376 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139955842739376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"633,-141 538,-141 538,-122 633,-122 633,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"585.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955842739376&#45;&gt;139956592833200 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139955842739376&#45;&gt;139956592833200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M585.5,-121.75C585.5,-114.8 585.5,-104.85 585.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"589,-96.09 585.5,-86.09 582,-96.09 589,-96.09\"/>\n",
       "</g>\n",
       "<!-- 139955842737072 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139955842737072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"634,-201.5 533,-201.5 533,-182.5 634,-182.5 634,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"583.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955842737072&#45;&gt;139955842739376 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139955842737072&#45;&gt;139955842739376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M583.8,-182.37C584.07,-174.25 584.5,-161.81 584.85,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"588.36,-151.28 585.2,-141.17 581.36,-151.04 588.36,-151.28\"/>\n",
       "</g>\n",
       "<!-- 139955446469088 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139955446469088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"495,-267.5 394,-267.5 394,-248.5 495,-248.5 495,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"444.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446469088&#45;&gt;139955842737072 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139955446469088&#45;&gt;139955842737072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M463.25,-248.37C486.98,-237.44 528.07,-218.52 555.52,-205.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"556.99,-209.06 564.61,-201.7 554.06,-202.7 556.99,-209.06\"/>\n",
       "</g>\n",
       "<!-- 139955446928688 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139955446928688</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"489,-339 400,-339 400,-309 489,-309 489,-339\"/>\n",
       "<text text-anchor=\"middle\" x=\"444.5\" y=\"-327\" font-family=\"monospace\" font-size=\"10.00\">linear4.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"444.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 139955446928688&#45;&gt;139955446469088 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139955446928688&#45;&gt;139955446469088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M444.5,-308.8C444.5,-299.7 444.5,-287.79 444.5,-277.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"448,-277.84 444.5,-267.84 441,-277.84 448,-277.84\"/>\n",
       "</g>\n",
       "<!-- 139955446471824 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139955446471824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"608,-267.5 513,-267.5 513,-248.5 608,-248.5 608,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"560.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446471824&#45;&gt;139955842737072 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139955446471824&#45;&gt;139955842737072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M563.6,-248.37C566.98,-238.97 572.48,-223.67 576.84,-211.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"580.21,-212.5 580.3,-201.91 573.62,-210.13 580.21,-212.5\"/>\n",
       "</g>\n",
       "<!-- 139955446477728 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139955446477728</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"609,-333.5 508,-333.5 508,-314.5 609,-314.5 609,-333.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"558.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446477728&#45;&gt;139955446471824 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139955446477728&#45;&gt;139955446471824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M558.77,-314.37C559.06,-305.16 559.52,-290.29 559.9,-278.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"563.41,-278.01 560.22,-267.91 556.41,-277.79 563.41,-278.01\"/>\n",
       "</g>\n",
       "<!-- 139955446468560 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139955446468560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"470,-399.5 369,-399.5 369,-380.5 470,-380.5 470,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"419.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446468560&#45;&gt;139955446477728 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139955446468560&#45;&gt;139955446477728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M438.25,-380.37C461.98,-369.44 503.07,-350.52 530.52,-337.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"531.99,-341.06 539.61,-333.7 529.06,-334.7 531.99,-341.06\"/>\n",
       "</g>\n",
       "<!-- 139955446929008 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>139955446929008</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"464,-471 375,-471 375,-441 464,-441 464,-471\"/>\n",
       "<text text-anchor=\"middle\" x=\"419.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\">linear3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"419.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\"> (15)</text>\n",
       "</g>\n",
       "<!-- 139955446929008&#45;&gt;139955446468560 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139955446929008&#45;&gt;139955446468560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M419.5,-440.8C419.5,-431.7 419.5,-419.79 419.5,-409.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"423,-409.84 419.5,-399.84 416,-409.84 423,-409.84\"/>\n",
       "</g>\n",
       "<!-- 139955446465536 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>139955446465536</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"583,-399.5 488,-399.5 488,-380.5 583,-380.5 583,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"535.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446465536&#45;&gt;139955446477728 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139955446465536&#45;&gt;139955446477728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M538.6,-380.37C541.98,-370.97 547.48,-355.67 551.84,-343.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"555.21,-344.5 555.3,-333.91 548.62,-342.13 555.21,-344.5\"/>\n",
       "</g>\n",
       "<!-- 139955446465440 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>139955446465440</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"584,-465.5 483,-465.5 483,-446.5 584,-446.5 584,-465.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"533.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446465440&#45;&gt;139955446465536 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>139955446465440&#45;&gt;139955446465536</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M533.77,-446.37C534.06,-437.16 534.52,-422.29 534.9,-410.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"538.41,-410.01 535.22,-399.91 531.41,-409.79 538.41,-410.01\"/>\n",
       "</g>\n",
       "<!-- 139955446465632 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>139955446465632</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"445,-531.5 344,-531.5 344,-512.5 445,-512.5 445,-531.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446465632&#45;&gt;139955446465440 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>139955446465632&#45;&gt;139955446465440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M413.25,-512.37C436.98,-501.44 478.07,-482.52 505.52,-469.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"506.99,-473.06 514.61,-465.7 504.06,-466.7 506.99,-473.06\"/>\n",
       "</g>\n",
       "<!-- 139955446924448 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>139955446924448</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"439,-603 350,-603 350,-573 439,-573 439,-603\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\">linear2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"394.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\"> (20)</text>\n",
       "</g>\n",
       "<!-- 139955446924448&#45;&gt;139955446465632 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>139955446924448&#45;&gt;139955446465632</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.5,-572.8C394.5,-563.7 394.5,-551.79 394.5,-541.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"398,-541.84 394.5,-531.84 391,-541.84 398,-541.84\"/>\n",
       "</g>\n",
       "<!-- 139955446465344 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>139955446465344</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"558,-531.5 463,-531.5 463,-512.5 558,-512.5 558,-531.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"510.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446465344&#45;&gt;139955446465440 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>139955446465344&#45;&gt;139955446465440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M513.6,-512.37C516.98,-502.97 522.48,-487.67 526.84,-475.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"530.21,-476.5 530.3,-465.91 523.62,-474.13 530.21,-476.5\"/>\n",
       "</g>\n",
       "<!-- 139955446473888 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>139955446473888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"559,-597.5 458,-597.5 458,-578.5 559,-578.5 559,-597.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"508.5\" y=\"-585.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473888&#45;&gt;139955446465344 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>139955446473888&#45;&gt;139955446465344</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M508.77,-578.37C509.06,-569.16 509.52,-554.29 509.9,-542.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"513.41,-542.01 510.22,-531.91 506.41,-541.79 513.41,-542.01\"/>\n",
       "</g>\n",
       "<!-- 139955446473696 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>139955446473696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"363,-663.5 262,-663.5 262,-644.5 363,-644.5 363,-663.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446473696&#45;&gt;139955446473888 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>139955446473696&#45;&gt;139955446473888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M338.93,-644.37C373.37,-633.12 433.72,-613.42 472.37,-600.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"473.75,-604.03 482.17,-597.6 471.58,-597.37 473.75,-604.03\"/>\n",
       "</g>\n",
       "<!-- 139955446929088 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>139955446929088</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"327,-735 238,-735 238,-705 327,-705 327,-735\"/>\n",
       "<text text-anchor=\"middle\" x=\"282.5\" y=\"-723\" font-family=\"monospace\" font-size=\"10.00\">linear1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"282.5\" y=\"-712\" font-family=\"monospace\" font-size=\"10.00\"> (30)</text>\n",
       "</g>\n",
       "<!-- 139955446929088&#45;&gt;139955446473696 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>139955446929088&#45;&gt;139955446473696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M289.16,-704.8C293.56,-695.41 299.37,-683.02 304.09,-672.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"307.28,-674.38 308.35,-663.84 300.94,-671.41 307.28,-674.38\"/>\n",
       "</g>\n",
       "<!-- 139955446473744 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>139955446473744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"531,-663.5 442,-663.5 442,-644.5 531,-644.5 531,-663.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"486.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\">CatBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473744&#45;&gt;139955446473888 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>139955446473744&#45;&gt;139955446473888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M489.47,-644.37C492.7,-634.97 497.96,-619.67 502.13,-607.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"505.5,-608.5 505.44,-597.91 498.88,-606.22 505.5,-608.5\"/>\n",
       "</g>\n",
       "<!-- 139955446473600 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>139955446473600</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"446,-729.5 345,-729.5 345,-710.5 446,-710.5 446,-729.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.5\" y=\"-717.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473600&#45;&gt;139955446473744 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>139955446473600&#45;&gt;139955446473744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M407.77,-710.37C422.56,-699.97 447.65,-682.32 465.53,-669.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.96,-672.31 474.13,-663.7 463.94,-666.59 467.96,-672.31\"/>\n",
       "</g>\n",
       "<!-- 139955446473360 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>139955446473360</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"211,-795.5 110,-795.5 110,-776.5 211,-776.5 211,-795.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"160.5\" y=\"-783.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446473360&#45;&gt;139955446473600 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>139955446473360&#45;&gt;139955446473600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.19,-776.37C234.1,-764.96 308.01,-744.83 354.23,-732.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"355.21,-735.6 363.93,-729.6 353.37,-728.85 355.21,-735.6\"/>\n",
       "</g>\n",
       "<!-- 139955446924128 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>139955446924128</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"208,-867 101,-867 101,-837 208,-837 208,-867\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-855\" font-family=\"monospace\" font-size=\"10.00\">M1.linear3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-844\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 139955446924128&#45;&gt;139955446473360 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>139955446924128&#45;&gt;139955446473360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M155.83,-836.8C156.68,-827.7 157.8,-815.79 158.73,-805.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.22,-806.13 159.67,-795.84 155.25,-805.47 162.22,-806.13\"/>\n",
       "</g>\n",
       "<!-- 139955446473408 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>139955446473408</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"334,-795.5 239,-795.5 239,-776.5 334,-776.5 334,-795.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"286.5\" y=\"-783.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473408&#45;&gt;139955446473600 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>139955446473408&#45;&gt;139955446473600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M301.2,-776.37C319.32,-765.73 350.35,-747.51 371.84,-734.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.83,-737.78 380.68,-729.7 370.29,-731.74 373.83,-737.78\"/>\n",
       "</g>\n",
       "<!-- 139955446473264 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>139955446473264</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"327,-861.5 226,-861.5 226,-842.5 327,-842.5 327,-861.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.5\" y=\"-849.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473264&#45;&gt;139955446473408 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>139955446473264&#45;&gt;139955446473408</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277.85,-842.37C279.3,-833.07 281.66,-817.98 283.55,-805.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.02,-806.33 285.11,-795.91 280.11,-805.25 287.02,-806.33\"/>\n",
       "</g>\n",
       "<!-- 139955446473072 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>139955446473072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"107,-927.5 6,-927.5 6,-908.5 107,-908.5 107,-927.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"56.5\" y=\"-915.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446473072&#45;&gt;139955446473264 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>139955446473072&#45;&gt;139955446473264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.17,-908.37C125.23,-897 194.01,-877 237.32,-864.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"238.33,-867.75 246.95,-861.6 236.37,-861.03 238.33,-867.75\"/>\n",
       "</g>\n",
       "<!-- 139955446929888 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>139955446929888</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107,-999 0,-999 0,-969 107,-969 107,-999\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-987\" font-family=\"monospace\" font-size=\"10.00\">M1.linear2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-976\" font-family=\"monospace\" font-size=\"10.00\"> (250)</text>\n",
       "</g>\n",
       "<!-- 139955446929888&#45;&gt;139955446473072 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>139955446929888&#45;&gt;139955446473072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.17,-968.8C54.59,-959.7 55.15,-947.79 55.61,-937.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"59.11,-938 56.09,-927.84 52.12,-937.67 59.11,-938\"/>\n",
       "</g>\n",
       "<!-- 139955446473120 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>139955446473120</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"223,-927.5 128,-927.5 128,-908.5 223,-908.5 223,-927.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-915.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473120&#45;&gt;139955446473264 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>139955446473120&#45;&gt;139955446473264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.12,-908.37C205.84,-897.78 234.41,-879.67 254.31,-867.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"256.2,-870.01 262.77,-861.7 252.45,-864.1 256.2,-870.01\"/>\n",
       "</g>\n",
       "<!-- 139955446472976 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>139955446472976</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"226,-993.5 125,-993.5 125,-974.5 226,-974.5 226,-993.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-981.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446472976&#45;&gt;139955446473120 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>139955446472976&#45;&gt;139955446473120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.5,-974.37C175.5,-965.16 175.5,-950.29 175.5,-938.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179,-937.91 175.5,-927.91 172,-937.91 179,-937.91\"/>\n",
       "</g>\n",
       "<!-- 139955446472784 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>139955446472784</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"114,-1059.5 13,-1059.5 13,-1040.5 114,-1040.5 114,-1059.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.5\" y=\"-1047.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472784&#45;&gt;139955446472976 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>139955446472784&#45;&gt;139955446472976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M78.6,-1040.37C97.31,-1029.68 129.4,-1011.35 151.49,-998.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"153.33,-1001.7 160.28,-993.7 149.86,-995.62 153.33,-1001.7\"/>\n",
       "</g>\n",
       "<!-- 139955844914592 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>139955844914592</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"113,-1131 6,-1131 6,-1101 113,-1101 113,-1131\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-1119\" font-family=\"monospace\" font-size=\"10.00\">M1.linear1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-1108\" font-family=\"monospace\" font-size=\"10.00\"> (350)</text>\n",
       "</g>\n",
       "<!-- 139955844914592&#45;&gt;139955446472784 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>139955844914592&#45;&gt;139955446472784</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.39,-1100.8C60.96,-1091.7 61.7,-1079.79 62.32,-1069.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.82,-1070.04 62.95,-1059.84 58.83,-1069.61 65.82,-1070.04\"/>\n",
       "</g>\n",
       "<!-- 139955446472832 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>139955446472832</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"216,-1059.5 139,-1059.5 139,-1040.5 216,-1040.5 216,-1059.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177.5\" y=\"-1047.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446472832&#45;&gt;139955446472976 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>139955446472832&#45;&gt;139955446472976</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.23,-1040.37C176.94,-1031.16 176.48,-1016.29 176.1,-1004.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.59,-1003.79 175.78,-993.91 172.59,-1004.01 179.59,-1003.79\"/>\n",
       "</g>\n",
       "<!-- 139955446472736 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>139955446472736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"232,-1125.5 131,-1125.5 131,-1106.5 232,-1106.5 232,-1125.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-1113.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472736&#45;&gt;139955446472832 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>139955446472736&#45;&gt;139955446472832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180.96,-1106.37C180.38,-1097.07 179.44,-1081.98 178.68,-1069.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.17,-1069.67 178.06,-1059.91 175.19,-1070.1 182.17,-1069.67\"/>\n",
       "</g>\n",
       "<!-- 139955845007376 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>139955845007376</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"241,-1197 122,-1197 122,-1167 241,-1167 241,-1197\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-1185\" font-family=\"monospace\" font-size=\"10.00\">M1.linear1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"181.5\" y=\"-1174\" font-family=\"monospace\" font-size=\"10.00\"> (350, 4608)</text>\n",
       "</g>\n",
       "<!-- 139955845007376&#45;&gt;139955446472736 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>139955845007376&#45;&gt;139955446472736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.5,-1166.8C181.5,-1157.7 181.5,-1145.79 181.5,-1135.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"185,-1135.84 181.5,-1125.84 178,-1135.84 185,-1135.84\"/>\n",
       "</g>\n",
       "<!-- 139955446473168 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>139955446473168</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"318,-927.5 241,-927.5 241,-908.5 318,-908.5 318,-927.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"279.5\" y=\"-915.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473168&#45;&gt;139955446473264 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>139955446473168&#45;&gt;139955446473264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.1,-908.37C278.66,-899.07 277.95,-883.98 277.39,-871.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.88,-871.73 276.92,-861.91 273.89,-872.06 280.88,-871.73\"/>\n",
       "</g>\n",
       "<!-- 139955446472688 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>139955446472688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"345,-993.5 244,-993.5 244,-974.5 345,-974.5 345,-993.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"294.5\" y=\"-981.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472688&#45;&gt;139955446473168 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>139955446472688&#45;&gt;139955446473168</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.48,-974.37C290.3,-965.07 286.76,-949.98 283.93,-937.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.28,-936.84 281.59,-927.91 280.46,-938.44 287.28,-936.84\"/>\n",
       "</g>\n",
       "<!-- 139955446596128 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>139955446596128</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"354,-1065 235,-1065 235,-1035 354,-1035 354,-1065\"/>\n",
       "<text text-anchor=\"middle\" x=\"294.5\" y=\"-1053\" font-family=\"monospace\" font-size=\"10.00\">M1.linear2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"294.5\" y=\"-1042\" font-family=\"monospace\" font-size=\"10.00\"> (250, 350)</text>\n",
       "</g>\n",
       "<!-- 139955446596128&#45;&gt;139955446472688 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>139955446596128&#45;&gt;139955446472688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M294.5,-1034.8C294.5,-1025.7 294.5,-1013.79 294.5,-1003.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"298,-1003.84 294.5,-993.84 291,-1003.84 298,-1003.84\"/>\n",
       "</g>\n",
       "<!-- 139955446473456 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>139955446473456</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"434,-795.5 357,-795.5 357,-776.5 434,-776.5 434,-795.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.5\" y=\"-783.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473456&#45;&gt;139955446473600 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>139955446473456&#45;&gt;139955446473600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M395.5,-776.37C395.5,-767.16 395.5,-752.29 395.5,-740.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"399,-739.91 395.5,-729.91 392,-739.91 399,-739.91\"/>\n",
       "</g>\n",
       "<!-- 139955446472880 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>139955446472880</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"446,-861.5 345,-861.5 345,-842.5 446,-842.5 446,-861.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.5\" y=\"-849.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472880&#45;&gt;139955446473456 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>139955446472880&#45;&gt;139955446473456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M395.5,-842.37C395.5,-833.16 395.5,-818.29 395.5,-806.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"399,-805.91 395.5,-795.91 392,-805.91 399,-805.91\"/>\n",
       "</g>\n",
       "<!-- 139955446924368 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>139955446924368</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"455,-933 336,-933 336,-903 455,-903 455,-933\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.5\" y=\"-921\" font-family=\"monospace\" font-size=\"10.00\">M1.linear3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"395.5\" y=\"-910\" font-family=\"monospace\" font-size=\"10.00\"> (5, 250)</text>\n",
       "</g>\n",
       "<!-- 139955446924368&#45;&gt;139955446472880 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>139955446924368&#45;&gt;139955446472880</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M395.5,-902.8C395.5,-893.7 395.5,-881.79 395.5,-871.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"399,-871.84 395.5,-861.84 392,-871.84 399,-871.84\"/>\n",
       "</g>\n",
       "<!-- 139955446473552 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>139955446473552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"626,-729.5 525,-729.5 525,-710.5 626,-710.5 626,-729.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"575.5\" y=\"-717.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473552&#45;&gt;139955446473744 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>139955446473552&#45;&gt;139955446473744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M563.5,-710.37C549.03,-699.97 524.49,-682.32 507.01,-669.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"508.76,-666.7 498.6,-663.7 504.67,-672.38 508.76,-666.7\"/>\n",
       "</g>\n",
       "<!-- 139955446472544 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>139955446472544</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"568,-795.5 467,-795.5 467,-776.5 568,-776.5 568,-795.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"517.5\" y=\"-783.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472544&#45;&gt;139955446473552 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>139955446472544&#45;&gt;139955446473552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M525.32,-776.37C534.35,-766.4 549.41,-749.79 560.67,-737.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"563.31,-739.67 567.43,-729.91 558.12,-734.96 563.31,-739.67\"/>\n",
       "</g>\n",
       "<!-- 139955844913632 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>139955844913632</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"571,-867 464,-867 464,-837 571,-837 571,-867\"/>\n",
       "<text text-anchor=\"middle\" x=\"517.5\" y=\"-855\" font-family=\"monospace\" font-size=\"10.00\">M2.linear3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"517.5\" y=\"-844\" font-family=\"monospace\" font-size=\"10.00\"> (5)</text>\n",
       "</g>\n",
       "<!-- 139955844913632&#45;&gt;139955446472544 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>139955844913632&#45;&gt;139955446472544</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M517.5,-836.8C517.5,-827.7 517.5,-815.79 517.5,-805.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"521,-805.84 517.5,-795.84 514,-805.84 521,-805.84\"/>\n",
       "</g>\n",
       "<!-- 139955446473024 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>139955446473024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"681,-795.5 586,-795.5 586,-776.5 681,-776.5 681,-795.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"633.5\" y=\"-783.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473024&#45;&gt;139955446473552 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>139955446473024&#45;&gt;139955446473552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M625.68,-776.37C616.65,-766.4 601.59,-749.79 590.33,-737.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"592.88,-734.96 583.57,-729.91 587.69,-739.67 592.88,-734.96\"/>\n",
       "</g>\n",
       "<!-- 139955446473216 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>139955446473216</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"690,-861.5 589,-861.5 589,-842.5 690,-842.5 690,-861.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-849.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473216&#45;&gt;139955446473024 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>139955446473216&#45;&gt;139955446473024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M638.69,-842.37C637.82,-833.07 636.4,-817.98 635.27,-805.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"638.75,-805.53 634.33,-795.91 631.78,-806.19 638.75,-805.53\"/>\n",
       "</g>\n",
       "<!-- 139955446472448 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>139955446472448</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"574,-927.5 473,-927.5 473,-908.5 574,-908.5 574,-927.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"523.5\" y=\"-915.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472448&#45;&gt;139955446473216 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>139955446472448&#45;&gt;139955446473216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M539.14,-908.37C558.6,-897.63 592.05,-879.18 614.94,-866.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"616.67,-869.59 623.73,-861.7 613.29,-863.47 616.67,-869.59\"/>\n",
       "</g>\n",
       "<!-- 139955455539808 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>139955455539808</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"571,-999 464,-999 464,-969 571,-969 571,-999\"/>\n",
       "<text text-anchor=\"middle\" x=\"517.5\" y=\"-987\" font-family=\"monospace\" font-size=\"10.00\">M2.linear2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"517.5\" y=\"-976\" font-family=\"monospace\" font-size=\"10.00\"> (80)</text>\n",
       "</g>\n",
       "<!-- 139955455539808&#45;&gt;139955446472448 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>139955455539808&#45;&gt;139955446472448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M518.83,-968.8C519.68,-959.7 520.8,-947.79 521.73,-937.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"525.22,-938.13 522.67,-927.84 518.25,-937.47 525.22,-938.13\"/>\n",
       "</g>\n",
       "<!-- 139955446472496 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>139955446472496</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"687,-927.5 592,-927.5 592,-908.5 687,-908.5 687,-927.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-915.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446472496&#45;&gt;139955446473216 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>139955446472496&#45;&gt;139955446473216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M639.5,-908.37C639.5,-899.16 639.5,-884.29 639.5,-872.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"643,-871.91 639.5,-861.91 636,-871.91 643,-871.91\"/>\n",
       "</g>\n",
       "<!-- 139955446472352 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>139955446472352</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"690,-993.5 589,-993.5 589,-974.5 690,-974.5 690,-993.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-981.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446472352&#45;&gt;139955446472496 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>139955446472352&#45;&gt;139955446472496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M639.5,-974.37C639.5,-965.16 639.5,-950.29 639.5,-938.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"643,-937.91 639.5,-927.91 636,-937.91 643,-937.91\"/>\n",
       "</g>\n",
       "<!-- 139955446472160 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>139955446472160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"580,-1059.5 479,-1059.5 479,-1040.5 580,-1040.5 580,-1059.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"529.5\" y=\"-1047.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472160&#45;&gt;139955446472352 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>139955446472160&#45;&gt;139955446472352</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M544.34,-1040.37C562.62,-1029.73 593.94,-1011.51 615.63,-998.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"617.67,-1001.75 624.55,-993.7 614.14,-995.7 617.67,-1001.75\"/>\n",
       "</g>\n",
       "<!-- 139955455533408 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>139955455533408</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"576,-1131 469,-1131 469,-1101 576,-1101 576,-1131\"/>\n",
       "<text text-anchor=\"middle\" x=\"522.5\" y=\"-1119\" font-family=\"monospace\" font-size=\"10.00\">M2.linear1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"522.5\" y=\"-1108\" font-family=\"monospace\" font-size=\"10.00\"> (200)</text>\n",
       "</g>\n",
       "<!-- 139955455533408&#45;&gt;139955446472160 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>139955455533408&#45;&gt;139955446472160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.05,-1100.8C525.05,-1091.7 526.35,-1079.79 527.43,-1069.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"530.92,-1070.17 528.53,-1059.84 523.97,-1069.4 530.92,-1070.17\"/>\n",
       "</g>\n",
       "<!-- 139955446472208 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>139955446472208</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"679,-1059.5 602,-1059.5 602,-1040.5 679,-1040.5 679,-1059.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"640.5\" y=\"-1047.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446472208&#45;&gt;139955446472352 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>139955446472208&#45;&gt;139955446472352</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M640.37,-1040.37C640.22,-1031.16 639.99,-1016.29 639.8,-1004.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"643.3,-1003.85 639.64,-993.91 636.3,-1003.96 643.3,-1003.85\"/>\n",
       "</g>\n",
       "<!-- 139955446472112 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>139955446472112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"695,-1125.5 594,-1125.5 594,-1106.5 695,-1106.5 695,-1125.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"644.5\" y=\"-1113.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472112&#45;&gt;139955446472208 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>139955446472112&#45;&gt;139955446472208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M643.96,-1106.37C643.38,-1097.07 642.44,-1081.98 641.68,-1069.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"645.17,-1069.67 641.06,-1059.91 638.19,-1070.1 645.17,-1069.67\"/>\n",
       "</g>\n",
       "<!-- 139955455533248 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>139955455533248</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"704,-1197 585,-1197 585,-1167 704,-1167 704,-1197\"/>\n",
       "<text text-anchor=\"middle\" x=\"644.5\" y=\"-1185\" font-family=\"monospace\" font-size=\"10.00\">M2.linear1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"644.5\" y=\"-1174\" font-family=\"monospace\" font-size=\"10.00\"> (200, 100)</text>\n",
       "</g>\n",
       "<!-- 139955455533248&#45;&gt;139955446472112 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>139955455533248&#45;&gt;139955446472112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M644.5,-1166.8C644.5,-1157.7 644.5,-1145.79 644.5,-1135.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"648,-1135.84 644.5,-1125.84 641,-1135.84 648,-1135.84\"/>\n",
       "</g>\n",
       "<!-- 139955446472640 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>139955446472640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"782,-927.5 705,-927.5 705,-908.5 782,-908.5 782,-927.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"743.5\" y=\"-915.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446472640&#45;&gt;139955446473216 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>139955446472640&#45;&gt;139955446473216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M729.47,-908.37C712.26,-897.78 682.84,-879.67 662.35,-867.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"663.99,-863.96 653.64,-861.7 660.32,-869.92 663.99,-863.96\"/>\n",
       "</g>\n",
       "<!-- 139955446472064 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>139955446472064</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"809,-993.5 708,-993.5 708,-974.5 809,-974.5 809,-993.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"758.5\" y=\"-981.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472064&#45;&gt;139955446472640 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>139955446472064&#45;&gt;139955446472640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M756.48,-974.37C754.3,-965.07 750.76,-949.98 747.93,-937.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"751.28,-936.84 745.59,-927.91 744.46,-938.44 751.28,-936.84\"/>\n",
       "</g>\n",
       "<!-- 139955843558000 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>139955843558000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"818,-1065 699,-1065 699,-1035 818,-1035 818,-1065\"/>\n",
       "<text text-anchor=\"middle\" x=\"758.5\" y=\"-1053\" font-family=\"monospace\" font-size=\"10.00\">M2.linear2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"758.5\" y=\"-1042\" font-family=\"monospace\" font-size=\"10.00\"> (80, 200)</text>\n",
       "</g>\n",
       "<!-- 139955843558000&#45;&gt;139955446472064 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>139955843558000&#45;&gt;139955446472064</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M758.5,-1034.8C758.5,-1025.7 758.5,-1013.79 758.5,-1003.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"762,-1003.84 758.5,-993.84 755,-1003.84 762,-1003.84\"/>\n",
       "</g>\n",
       "<!-- 139955446473312 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>139955446473312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"776,-795.5 699,-795.5 699,-776.5 776,-776.5 776,-795.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"737.5\" y=\"-783.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473312&#45;&gt;139955446473552 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>139955446473312&#45;&gt;139955446473552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M715.65,-776.37C687.55,-765.27 638.57,-745.92 606.6,-733.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"607.85,-730.02 597.26,-729.6 605.27,-736.53 607.85,-730.02\"/>\n",
       "</g>\n",
       "<!-- 139955446472256 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>139955446472256</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"859,-861.5 758,-861.5 758,-842.5 859,-842.5 859,-861.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"808.5\" y=\"-849.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472256&#45;&gt;139955446473312 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>139955446472256&#45;&gt;139955446473312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M798.92,-842.37C787.66,-832.21 768.74,-815.16 754.87,-802.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"757.15,-800 747.38,-795.91 752.46,-805.2 757.15,-800\"/>\n",
       "</g>\n",
       "<!-- 139955844914272 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>139955844914272</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"919,-933 800,-933 800,-903 919,-903 919,-933\"/>\n",
       "<text text-anchor=\"middle\" x=\"859.5\" y=\"-921\" font-family=\"monospace\" font-size=\"10.00\">M2.linear3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"859.5\" y=\"-910\" font-family=\"monospace\" font-size=\"10.00\"> (5, 80)</text>\n",
       "</g>\n",
       "<!-- 139955844914272&#45;&gt;139955446472256 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>139955844914272&#45;&gt;139955446472256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M848.18,-902.8C840.39,-893.01 830,-879.98 821.81,-869.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"824.52,-867.48 815.55,-861.84 819.04,-871.85 824.52,-867.48\"/>\n",
       "</g>\n",
       "<!-- 139955446473792 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>139955446473792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"626,-663.5 549,-663.5 549,-644.5 626,-644.5 626,-663.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"587.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446473792&#45;&gt;139955446473888 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>139955446473792&#45;&gt;139955446473888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M576.85,-644.37C564.12,-634.06 542.62,-616.65 527.13,-604.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"529.21,-601.27 519.24,-597.7 524.8,-606.71 529.21,-601.27\"/>\n",
       "</g>\n",
       "<!-- 139955446472400 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>139955446472400</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"820,-729.5 719,-729.5 719,-710.5 820,-710.5 820,-729.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"769.5\" y=\"-717.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472400&#45;&gt;139955446473792 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>139955446472400&#45;&gt;139955446473792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M744.95,-710.37C713.12,-699.17 657.41,-679.58 621.51,-666.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"622.54,-663.61 611.95,-663.6 620.22,-670.22 622.54,-663.61\"/>\n",
       "</g>\n",
       "<!-- 139955446937088 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>139955446937088</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"895,-801 794,-801 794,-771 895,-771 895,-801\"/>\n",
       "<text text-anchor=\"middle\" x=\"844.5\" y=\"-789\" font-family=\"monospace\" font-size=\"10.00\">linear1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"844.5\" y=\"-778\" font-family=\"monospace\" font-size=\"10.00\"> (30, 10)</text>\n",
       "</g>\n",
       "<!-- 139955446937088&#45;&gt;139955446472400 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>139955446937088&#45;&gt;139955446472400</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M827.86,-770.8C815.93,-760.62 799.89,-746.93 787.64,-736.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"789.74,-733.67 779.86,-729.84 785.2,-739 789.74,-733.67\"/>\n",
       "</g>\n",
       "<!-- 139955446465296 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>139955446465296</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"653,-531.5 576,-531.5 576,-512.5 653,-512.5 653,-531.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"614.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446465296&#45;&gt;139955446465440 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>139955446465296&#45;&gt;139955446465440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M603.58,-512.37C590.53,-502.06 568.49,-484.65 552.6,-472.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"554.53,-469.15 544.51,-465.7 550.19,-474.64 554.53,-469.15\"/>\n",
       "</g>\n",
       "<!-- 139955446472592 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>139955446472592</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"722,-597.5 621,-597.5 621,-578.5 722,-578.5 722,-597.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"671.5\" y=\"-585.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446472592&#45;&gt;139955446465296 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>139955446472592&#45;&gt;139955446465296</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M663.81,-578.37C655.02,-568.5 640.43,-552.11 629.39,-539.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"631.7,-537.05 622.43,-531.91 626.47,-541.7 631.7,-537.05\"/>\n",
       "</g>\n",
       "<!-- 139955446923408 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>139955446923408</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"745,-669 644,-669 644,-639 745,-639 745,-669\"/>\n",
       "<text text-anchor=\"middle\" x=\"694.5\" y=\"-657\" font-family=\"monospace\" font-size=\"10.00\">linear2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"694.5\" y=\"-646\" font-family=\"monospace\" font-size=\"10.00\"> (20, 30)</text>\n",
       "</g>\n",
       "<!-- 139955446923408&#45;&gt;139955446472592 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>139955446923408&#45;&gt;139955446472592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M689.4,-638.8C686.06,-629.5 681.66,-617.27 678.06,-607.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"681.35,-606.07 674.68,-597.84 674.77,-608.44 681.35,-606.07\"/>\n",
       "</g>\n",
       "<!-- 139955446479360 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>139955446479360</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"678,-399.5 601,-399.5 601,-380.5 678,-380.5 678,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446479360&#45;&gt;139955446477728 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>139955446479360&#45;&gt;139955446477728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M628.58,-380.37C615.53,-370.06 593.49,-352.65 577.6,-340.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"579.53,-337.15 569.51,-333.7 575.19,-342.64 579.53,-337.15\"/>\n",
       "</g>\n",
       "<!-- 139955446473648 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>139955446473648</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"749,-465.5 648,-465.5 648,-446.5 749,-446.5 749,-465.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"698.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446473648&#45;&gt;139955446479360 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>139955446473648&#45;&gt;139955446479360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M690.54,-446.37C681.36,-436.4 666.04,-419.79 654.58,-407.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"657.06,-404.89 647.71,-399.91 651.91,-409.63 657.06,-404.89\"/>\n",
       "</g>\n",
       "<!-- 139955446928928 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>139955446928928</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"772,-537 671,-537 671,-507 772,-507 772,-537\"/>\n",
       "<text text-anchor=\"middle\" x=\"721.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\">linear3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"721.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\"> (15, 20)</text>\n",
       "</g>\n",
       "<!-- 139955446928928&#45;&gt;139955446473648 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>139955446928928&#45;&gt;139955446473648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M716.4,-506.8C713.06,-497.5 708.66,-485.27 705.06,-475.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"708.35,-474.07 701.68,-465.84 701.77,-476.44 708.35,-474.07\"/>\n",
       "</g>\n",
       "<!-- 139955446465584 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>139955446465584</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"703,-267.5 626,-267.5 626,-248.5 703,-248.5 703,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"664.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446465584&#45;&gt;139955842737072 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>139955446465584&#45;&gt;139955842737072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M653.58,-248.37C640.53,-238.06 618.49,-220.65 602.6,-208.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.53,-205.15 594.51,-201.7 600.19,-210.64 604.53,-205.15\"/>\n",
       "</g>\n",
       "<!-- 139955446473936 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>139955446473936</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"774,-333.5 673,-333.5 673,-314.5 774,-314.5 774,-333.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"723.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446473936&#45;&gt;139955446465584 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>139955446473936&#45;&gt;139955446465584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M715.54,-314.37C706.36,-304.4 691.04,-287.79 679.58,-275.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"682.06,-272.89 672.71,-267.91 676.91,-277.63 682.06,-272.89\"/>\n",
       "</g>\n",
       "<!-- 139955446929408 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>139955446929408</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"797,-405 696,-405 696,-375 797,-375 797,-405\"/>\n",
       "<text text-anchor=\"middle\" x=\"746.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\">linear4.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"746.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\"> (10, 15)</text>\n",
       "</g>\n",
       "<!-- 139955446929408&#45;&gt;139955446473936 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>139955446929408&#45;&gt;139955446473936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M741.4,-374.8C738.06,-365.5 733.66,-353.27 730.06,-343.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"733.35,-342.07 726.68,-333.84 726.77,-344.44 733.35,-342.07\"/>\n",
       "</g>\n",
       "<!-- 139955842739664 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>139955842739664</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"757,-141 680,-141 680,-122 757,-122 757,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"718.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955842739664&#45;&gt;139956592833200 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>139955842739664&#45;&gt;139956592833200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M697.13,-121.98C675.35,-113.3 641.34,-99.75 616.58,-89.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"617.84,-86.62 607.26,-86.17 615.25,-93.12 617.84,-86.62\"/>\n",
       "</g>\n",
       "<!-- 139955446465392 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>139955446465392</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"799,-201.5 698,-201.5 698,-182.5 799,-182.5 799,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"748.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446465392&#45;&gt;139955842739664 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>139955446465392&#45;&gt;139955842739664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M744.07,-182.37C739.73,-173.9 732.98,-160.74 727.51,-150.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"730.62,-148.47 722.94,-141.17 724.39,-151.66 730.62,-148.47\"/>\n",
       "</g>\n",
       "<!-- 139955446924528 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>139955446924528</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"822,-273 721,-273 721,-243 822,-243 822,-273\"/>\n",
       "<text text-anchor=\"middle\" x=\"771.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\">linear5.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"771.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 139955446924528&#45;&gt;139955446465392 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>139955446924528&#45;&gt;139955446465392</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M766.4,-242.8C763.06,-233.5 758.66,-221.27 755.06,-211.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"758.35,-210.07 751.68,-201.84 751.77,-212.44 758.35,-210.07\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f4a0258bc10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "batch = next(iter(finaltest_loader))\n",
    "out=model(batch[0].to(device),batch[1].to(device))\n",
    "make_dot(out,params=dict(list(model.named_parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff994f",
   "metadata": {
    "id": "49ff994f"
   },
   "source": [
    "# Task 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92381dd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "92381dd3",
    "outputId": "120f5214-96e7-45ad-8664-8966795d0436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4da246ac-d500-404e-b732-6e69b43d2bb1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* a plate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a plate of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plate of delicious</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of delicious food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delicious food including</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471504</th>\n",
       "      <td>on a rack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471505</th>\n",
       "      <td>a rack in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471518</th>\n",
       "      <td>window made of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471519</th>\n",
       "      <td>made of dead</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471527</th>\n",
       "      <td>has a window</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55789 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4da246ac-d500-404e-b732-6e69b43d2bb1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4da246ac-d500-404e-b732-6e69b43d2bb1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4da246ac-d500-404e-b732-6e69b43d2bb1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                          tokens word_sentiment\n",
       "0                      * a plate              0\n",
       "1                     a plate of              0\n",
       "2             plate of delicious              0\n",
       "3              of delicious food              1\n",
       "4       delicious food including              1\n",
       "...                          ...            ...\n",
       "471504                 on a rack              0\n",
       "471505                 a rack in              0\n",
       "471518            window made of              0\n",
       "471519              made of dead              0\n",
       "471527              has a window              0\n",
       "\n",
       "[55789 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "df = pd.read_csv('/content/gdrive/MyDrive/Project/sentiment/sentiment.csv')\n",
    "df=df[['tokens','word_sentiment']]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    x = row['tokens']\n",
    "    x = x[1:-1]  #remove the surrounding quotes\n",
    "    x = [s.strip() for s in x.split(',')]  #rplit the string into individual strings\n",
    "    new = []\n",
    "    for i in x:\n",
    "        i = i.replace(\"'\", '')   #converting token strings to lists\n",
    "        new.append(i)\n",
    "    df.at[_, 'tokens'] = new\n",
    "\n",
    "\n",
    "def getIntList(x):\n",
    "    x=json.loads(x)\n",
    "    intList=[]\n",
    "    for i in x:\n",
    "        intList.append(int(i))\n",
    "    return intList\n",
    "for i,row in df.iterrows():\n",
    "    row['tokens'].insert(0, '*')\n",
    "    row['tokens'].append('*')\n",
    "    row['word_sentiment']=getIntList(row['word_sentiment'])\n",
    "print(type(df.iloc[0,1]))\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    x=row['tokens']\n",
    "    trigrams=[]\n",
    "    for trigram in zip(x, x[1:], x[2:]):\n",
    "        trigram_str = ' '.join(trigram)\n",
    "        trigrams.append(trigram_str)\n",
    "    row['tokens']=trigrams\n",
    "\n",
    "df = df.explode(['tokens','word_sentiment'])\n",
    "df = df.reset_index(drop=True)\n",
    "# df = finalDf.drop('index', axis=1,drop_first=True)\n",
    "df.drop_duplicates('tokens',inplace=True)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, model):\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x=self.model.encode(self.df.iloc[index,0])\n",
    "        x = torch.tensor(x)\n",
    "        x = F.pad(x, (0,15-x.size()[0]))\n",
    "        \n",
    "        y=torch.tensor(int(self.df.iloc[index,1]))\n",
    "        \n",
    "        return (x,y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fabf1",
   "metadata": {
    "id": "204fabf1"
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "RLCKt2fh9qQA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLCKt2fh9qQA",
    "outputId": "5de335cf-c0a4-4536-df8c-f13c47554bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myModel(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=15, out_features=250, bias=True)\n",
      "  (act_fn): ReLU()\n",
      "  (linear2): Linear(in_features=250, out_features=90, bias=True)\n",
      "  (linear3): Linear(in_features=90, out_features=50, bias=True)\n",
      "  (linear4): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (linear5): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class myModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(15,250)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(250,90)\n",
    "        self.linear3 = nn.Linear(90, 50)\n",
    "        self.linear4 = nn.Linear(50, 20)\n",
    "        self.linear5 = nn.Linear(20, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear5(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model=myModel().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "IIlUvwDD9xEJ",
   "metadata": {
    "id": "IIlUvwDD9xEJ"
   },
   "outputs": [],
   "source": [
    "myDataset=CustomDataset(df,txtModel)\n",
    "train,test=random_split(myDataset, [44631, 11158])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "test_loader = DataLoader(test, batch_size=32)\n",
    "\n",
    "# train_dataset = CustomDataset(df, txtModel, train=True)\n",
    "# test_dataset = CustomDataset(df, txtModel, train=False)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a1926",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4r_a2UC39xkM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4r_a2UC39xkM",
    "outputId": "70ce40be-8102-4935-fe0c-4a835d14d9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch: 2 \t\t\t Iteration 585 Loss Train: 0.32769307494163513\n",
      "Epoch: 2 \t\t\t Iteration 586 Loss Train: 0.3854067623615265\n",
      "Epoch: 2 \t\t\t Iteration 587 Loss Train: 0.42654767632484436\n",
      "Epoch: 2 \t\t\t Iteration 588 Loss Train: 0.5569896697998047\n",
      "Epoch: 2 \t\t\t Iteration 589 Loss Train: 0.3709520697593689\n",
      "Epoch: 2 \t\t\t Iteration 590 Loss Train: 0.39385437965393066\n",
      "Epoch: 2 \t\t\t Iteration 591 Loss Train: 0.4436744451522827\n",
      "Epoch: 2 \t\t\t Iteration 592 Loss Train: 0.4476075768470764\n",
      "Epoch: 2 \t\t\t Iteration 593 Loss Train: 0.438536673784256\n",
      "Epoch: 2 \t\t\t Iteration 594 Loss Train: 0.3813953399658203\n",
      "Epoch: 2 \t\t\t Iteration 595 Loss Train: 0.6996951699256897\n",
      "Epoch: 2 \t\t\t Iteration 596 Loss Train: 0.5123264193534851\n",
      "Epoch: 2 \t\t\t Iteration 597 Loss Train: 0.42081522941589355\n",
      "Epoch: 2 \t\t\t Iteration 598 Loss Train: 0.40203213691711426\n",
      "Epoch: 2 \t\t\t Iteration 599 Loss Train: 0.5075157880783081\n",
      "Epoch: 2 \t\t\t Iteration 600 Loss Train: 0.4379410147666931\n",
      "Epoch: 2 \t\t\t Iteration 601 Loss Train: 0.6144810914993286\n",
      "Epoch: 2 \t\t\t Iteration 602 Loss Train: 0.461830198764801\n",
      "Epoch: 2 \t\t\t Iteration 603 Loss Train: 0.345302939414978\n",
      "Epoch: 2 \t\t\t Iteration 604 Loss Train: 0.3256552219390869\n",
      "Epoch: 2 \t\t\t Iteration 605 Loss Train: 0.3863453269004822\n",
      "Epoch: 2 \t\t\t Iteration 606 Loss Train: 0.37536171078681946\n",
      "Epoch: 2 \t\t\t Iteration 607 Loss Train: 0.2653292119503021\n",
      "Epoch: 2 \t\t\t Iteration 608 Loss Train: 0.431949645280838\n",
      "Epoch: 2 \t\t\t Iteration 609 Loss Train: 0.40378832817077637\n",
      "Epoch: 2 \t\t\t Iteration 610 Loss Train: 0.27075380086898804\n",
      "Epoch: 2 \t\t\t Iteration 611 Loss Train: 0.5317432880401611\n",
      "Epoch: 2 \t\t\t Iteration 612 Loss Train: 0.40518227219581604\n",
      "Epoch: 2 \t\t\t Iteration 613 Loss Train: 0.38153478503227234\n",
      "Epoch: 2 \t\t\t Iteration 614 Loss Train: 0.24421799182891846\n",
      "Epoch: 2 \t\t\t Iteration 615 Loss Train: 0.8728463649749756\n",
      "Epoch: 2 \t\t\t Iteration 616 Loss Train: 0.5721896886825562\n",
      "Epoch: 2 \t\t\t Iteration 617 Loss Train: 0.46487686038017273\n",
      "Epoch: 2 \t\t\t Iteration 618 Loss Train: 0.4058426022529602\n",
      "Epoch: 2 \t\t\t Iteration 619 Loss Train: 0.479495108127594\n",
      "Epoch: 2 \t\t\t Iteration 620 Loss Train: 0.4791947603225708\n",
      "Epoch: 2 \t\t\t Iteration 621 Loss Train: 0.5745071768760681\n",
      "Epoch: 2 \t\t\t Iteration 622 Loss Train: 0.6025774478912354\n",
      "Epoch: 2 \t\t\t Iteration 623 Loss Train: 0.5156487226486206\n",
      "Epoch: 2 \t\t\t Iteration 624 Loss Train: 0.44197753071784973\n",
      "Epoch: 2 \t\t\t Iteration 625 Loss Train: 0.3997472822666168\n",
      "Epoch: 2 \t\t\t Iteration 626 Loss Train: 0.3879510760307312\n",
      "Epoch: 2 \t\t\t Iteration 627 Loss Train: 0.5253772735595703\n",
      "Epoch: 2 \t\t\t Iteration 628 Loss Train: 0.47886836528778076\n",
      "Epoch: 2 \t\t\t Iteration 629 Loss Train: 0.5238730311393738\n",
      "Epoch: 2 \t\t\t Iteration 630 Loss Train: 0.25985729694366455\n",
      "Epoch: 2 \t\t\t Iteration 631 Loss Train: 0.3800029754638672\n",
      "Epoch: 2 \t\t\t Iteration 632 Loss Train: 0.4291207194328308\n",
      "Epoch: 2 \t\t\t Iteration 633 Loss Train: 0.5349857807159424\n",
      "Epoch: 2 \t\t\t Iteration 634 Loss Train: 0.5688419342041016\n",
      "Epoch: 2 \t\t\t Iteration 635 Loss Train: 0.4222570061683655\n",
      "Epoch: 2 \t\t\t Iteration 636 Loss Train: 0.42927324771881104\n",
      "Epoch: 2 \t\t\t Iteration 637 Loss Train: 0.4748729467391968\n",
      "Epoch: 2 \t\t\t Iteration 638 Loss Train: 0.4191572666168213\n",
      "Epoch: 2 \t\t\t Iteration 639 Loss Train: 0.5177570581436157\n",
      "Epoch: 2 \t\t\t Iteration 640 Loss Train: 0.5592012405395508\n",
      "Epoch: 2 \t\t\t Iteration 641 Loss Train: 0.503402590751648\n",
      "Epoch: 2 \t\t\t Iteration 642 Loss Train: 0.544294536113739\n",
      "Epoch: 2 \t\t\t Iteration 643 Loss Train: 0.2790503203868866\n",
      "Epoch: 2 \t\t\t Iteration 644 Loss Train: 0.3721763491630554\n",
      "Epoch: 2 \t\t\t Iteration 645 Loss Train: 0.42700207233428955\n",
      "Epoch: 2 \t\t\t Iteration 646 Loss Train: 0.3951834738254547\n",
      "Epoch: 2 \t\t\t Iteration 647 Loss Train: 0.4898471236228943\n",
      "Epoch: 2 \t\t\t Iteration 648 Loss Train: 0.3751140236854553\n",
      "Epoch: 2 \t\t\t Iteration 649 Loss Train: 0.43801024556159973\n",
      "Epoch: 2 \t\t\t Iteration 650 Loss Train: 0.399368554353714\n",
      "Epoch: 2 \t\t\t Iteration 651 Loss Train: 0.431608647108078\n",
      "Epoch: 2 \t\t\t Iteration 652 Loss Train: 0.4173855781555176\n",
      "Epoch: 2 \t\t\t Iteration 653 Loss Train: 0.4896255135536194\n",
      "Epoch: 2 \t\t\t Iteration 654 Loss Train: 0.40074652433395386\n",
      "Epoch: 2 \t\t\t Iteration 655 Loss Train: 0.33682888746261597\n",
      "Epoch: 2 \t\t\t Iteration 656 Loss Train: 0.14671851694583893\n",
      "Epoch: 2 \t\t\t Iteration 657 Loss Train: 0.5035619139671326\n",
      "Epoch: 2 \t\t\t Iteration 658 Loss Train: 0.7517694234848022\n",
      "Epoch: 2 \t\t\t Iteration 659 Loss Train: 0.36183351278305054\n",
      "Epoch: 2 \t\t\t Iteration 660 Loss Train: 0.5873339772224426\n",
      "Epoch: 2 \t\t\t Iteration 661 Loss Train: 0.434442400932312\n",
      "Epoch: 2 \t\t\t Iteration 662 Loss Train: 0.5419436693191528\n",
      "Epoch: 2 \t\t\t Iteration 663 Loss Train: 0.5280114412307739\n",
      "Epoch: 2 \t\t\t Iteration 664 Loss Train: 0.47186094522476196\n",
      "Epoch: 2 \t\t\t Iteration 665 Loss Train: 0.3644290566444397\n",
      "Epoch: 2 \t\t\t Iteration 666 Loss Train: 0.42414894700050354\n",
      "Epoch: 2 \t\t\t Iteration 667 Loss Train: 0.2783617079257965\n",
      "Epoch: 2 \t\t\t Iteration 668 Loss Train: 0.4256572127342224\n",
      "Epoch: 2 \t\t\t Iteration 669 Loss Train: 0.27337971329689026\n",
      "Epoch: 2 \t\t\t Iteration 670 Loss Train: 0.32480525970458984\n",
      "Epoch: 2 \t\t\t Iteration 671 Loss Train: 0.8500300645828247\n",
      "Epoch: 2 \t\t\t Iteration 672 Loss Train: 0.3663899302482605\n",
      "Epoch: 2 \t\t\t Iteration 673 Loss Train: 0.4939185678958893\n",
      "Epoch: 2 \t\t\t Iteration 674 Loss Train: 0.5100827813148499\n",
      "Epoch: 2 \t\t\t Iteration 675 Loss Train: 0.44598257541656494\n",
      "Epoch: 2 \t\t\t Iteration 676 Loss Train: 0.5631727576255798\n",
      "Epoch: 2 \t\t\t Iteration 677 Loss Train: 0.5263596177101135\n",
      "Epoch: 2 \t\t\t Iteration 678 Loss Train: 0.31105363368988037\n",
      "Epoch: 2 \t\t\t Iteration 679 Loss Train: 0.2441549003124237\n",
      "Epoch: 2 \t\t\t Iteration 680 Loss Train: 0.41845524311065674\n",
      "Epoch: 2 \t\t\t Iteration 681 Loss Train: 0.44798219203948975\n",
      "Epoch: 2 \t\t\t Iteration 682 Loss Train: 0.20522978901863098\n",
      "Epoch: 2 \t\t\t Iteration 683 Loss Train: 0.2766728401184082\n",
      "Epoch: 2 \t\t\t Iteration 684 Loss Train: 0.42369571328163147\n",
      "Epoch: 2 \t\t\t Iteration 685 Loss Train: 0.51985764503479\n",
      "Epoch: 2 \t\t\t Iteration 686 Loss Train: 0.47952646017074585\n",
      "Epoch: 2 \t\t\t Iteration 687 Loss Train: 0.32118120789527893\n",
      "Epoch: 2 \t\t\t Iteration 688 Loss Train: 0.9151339530944824\n",
      "Epoch: 2 \t\t\t Iteration 689 Loss Train: 0.47665637731552124\n",
      "Epoch: 2 \t\t\t Iteration 690 Loss Train: 0.40591955184936523\n",
      "Epoch: 2 \t\t\t Iteration 691 Loss Train: 0.47841933369636536\n",
      "Epoch: 2 \t\t\t Iteration 692 Loss Train: 0.5404007434844971\n",
      "Epoch: 2 \t\t\t Iteration 693 Loss Train: 0.41025418043136597\n",
      "Epoch: 2 \t\t\t Iteration 694 Loss Train: 0.39786678552627563\n",
      "Epoch: 2 \t\t\t Iteration 695 Loss Train: 0.4204297959804535\n",
      "Epoch: 2 \t\t\t Iteration 696 Loss Train: 0.521415114402771\n",
      "Epoch: 2 \t\t\t Iteration 697 Loss Train: 0.37392622232437134\n",
      "Epoch: 2 \t\t\t Iteration 698 Loss Train: 0.30584439635276794\n",
      "Epoch: 2 \t\t\t Iteration 699 Loss Train: 0.2930142283439636\n",
      "Epoch: 2 \t\t\t Iteration 700 Loss Train: 0.459281325340271\n",
      "Epoch: 2 \t\t\t Iteration 701 Loss Train: 0.4623294472694397\n",
      "Epoch: 2 \t\t\t Iteration 702 Loss Train: 0.5379312038421631\n",
      "Epoch: 2 \t\t\t Iteration 703 Loss Train: 0.5791745781898499\n",
      "Epoch: 2 \t\t\t Iteration 704 Loss Train: 0.5872468948364258\n",
      "Epoch: 2 \t\t\t Iteration 705 Loss Train: 0.6974894404411316\n",
      "Epoch: 2 \t\t\t Iteration 706 Loss Train: 0.5744376182556152\n",
      "Epoch: 2 \t\t\t Iteration 707 Loss Train: 0.40740594267845154\n",
      "Epoch: 2 \t\t\t Iteration 708 Loss Train: 0.4391095042228699\n",
      "Epoch: 2 \t\t\t Iteration 709 Loss Train: 0.45441165566444397\n",
      "Epoch: 2 \t\t\t Iteration 710 Loss Train: 0.347103476524353\n",
      "Epoch: 2 \t\t\t Iteration 711 Loss Train: 0.2710218131542206\n",
      "Epoch: 2 \t\t\t Iteration 712 Loss Train: 0.40087664127349854\n",
      "Epoch: 2 \t\t\t Iteration 713 Loss Train: 0.470483660697937\n",
      "Epoch: 2 \t\t\t Iteration 714 Loss Train: 0.4143604040145874\n",
      "Epoch: 2 \t\t\t Iteration 715 Loss Train: 0.31690719723701477\n",
      "Epoch: 2 \t\t\t Iteration 716 Loss Train: 0.37300872802734375\n",
      "Epoch: 2 \t\t\t Iteration 717 Loss Train: 0.4141552448272705\n",
      "Epoch: 2 \t\t\t Iteration 718 Loss Train: 0.41200345754623413\n",
      "Epoch: 2 \t\t\t Iteration 719 Loss Train: 0.49221551418304443\n",
      "Epoch: 2 \t\t\t Iteration 720 Loss Train: 0.6131405830383301\n",
      "Epoch: 2 \t\t\t Iteration 721 Loss Train: 0.5130038261413574\n",
      "Epoch: 2 \t\t\t Iteration 722 Loss Train: 0.37713271379470825\n",
      "Epoch: 2 \t\t\t Iteration 723 Loss Train: 0.3636452555656433\n",
      "Epoch: 2 \t\t\t Iteration 724 Loss Train: 0.41101956367492676\n",
      "Epoch: 2 \t\t\t Iteration 725 Loss Train: 0.3267030417919159\n",
      "Epoch: 2 \t\t\t Iteration 726 Loss Train: 0.5430569648742676\n",
      "Epoch: 2 \t\t\t Iteration 727 Loss Train: 0.5103786587715149\n",
      "Epoch: 2 \t\t\t Iteration 728 Loss Train: 0.4931039810180664\n",
      "Epoch: 2 \t\t\t Iteration 729 Loss Train: 0.3604711592197418\n",
      "Epoch: 2 \t\t\t Iteration 730 Loss Train: 0.3205811381340027\n",
      "Epoch: 2 \t\t\t Iteration 731 Loss Train: 0.4234088063240051\n",
      "Epoch: 2 \t\t\t Iteration 732 Loss Train: 0.28128480911254883\n",
      "Epoch: 2 \t\t\t Iteration 733 Loss Train: 0.35974013805389404\n",
      "Epoch: 2 \t\t\t Iteration 734 Loss Train: 0.7570601105690002\n",
      "Epoch: 2 \t\t\t Iteration 735 Loss Train: 0.50013267993927\n",
      "Epoch: 2 \t\t\t Iteration 736 Loss Train: 0.5468443036079407\n",
      "Epoch: 2 \t\t\t Iteration 737 Loss Train: 0.4318578541278839\n",
      "Epoch: 2 \t\t\t Iteration 738 Loss Train: 0.37176215648651123\n",
      "Epoch: 2 \t\t\t Iteration 739 Loss Train: 1.0376646518707275\n",
      "Epoch: 2 \t\t\t Iteration 740 Loss Train: 0.5662201642990112\n",
      "Epoch: 2 \t\t\t Iteration 741 Loss Train: 0.4508628845214844\n",
      "Epoch: 2 \t\t\t Iteration 742 Loss Train: 0.536484956741333\n",
      "Epoch: 2 \t\t\t Iteration 743 Loss Train: 0.47483426332473755\n",
      "Epoch: 2 \t\t\t Iteration 744 Loss Train: 0.42420700192451477\n",
      "Epoch: 2 \t\t\t Iteration 745 Loss Train: 0.5030944347381592\n",
      "Epoch: 2 \t\t\t Iteration 746 Loss Train: 0.48775514960289\n",
      "Epoch: 2 \t\t\t Iteration 747 Loss Train: 0.4483128786087036\n",
      "Epoch: 2 \t\t\t Iteration 748 Loss Train: 0.3824518322944641\n",
      "Epoch: 2 \t\t\t Iteration 749 Loss Train: 0.3860475420951843\n",
      "Epoch: 2 \t\t\t Iteration 750 Loss Train: 0.42245039343833923\n",
      "Epoch: 2 \t\t\t Iteration 751 Loss Train: 0.4467037320137024\n",
      "Epoch: 2 \t\t\t Iteration 752 Loss Train: 0.2340065836906433\n",
      "Epoch: 2 \t\t\t Iteration 753 Loss Train: 0.4327041506767273\n",
      "Epoch: 2 \t\t\t Iteration 754 Loss Train: 0.4081495404243469\n",
      "Epoch: 2 \t\t\t Iteration 755 Loss Train: 0.6264700889587402\n",
      "Epoch: 2 \t\t\t Iteration 756 Loss Train: 0.4660288095474243\n",
      "Epoch: 2 \t\t\t Iteration 757 Loss Train: 0.4288412630558014\n",
      "Epoch: 2 \t\t\t Iteration 758 Loss Train: 0.3654402494430542\n",
      "Epoch: 2 \t\t\t Iteration 759 Loss Train: 0.44854727387428284\n",
      "Epoch: 2 \t\t\t Iteration 760 Loss Train: 0.48556968569755554\n",
      "Epoch: 2 \t\t\t Iteration 761 Loss Train: 0.4870975613594055\n",
      "Epoch: 2 \t\t\t Iteration 762 Loss Train: 0.32118913531303406\n",
      "Epoch: 2 \t\t\t Iteration 763 Loss Train: 0.4118506908416748\n",
      "Epoch: 2 \t\t\t Iteration 764 Loss Train: 0.4509974718093872\n",
      "Epoch: 2 \t\t\t Iteration 765 Loss Train: 0.2164400815963745\n",
      "Epoch: 2 \t\t\t Iteration 766 Loss Train: 0.6079258918762207\n",
      "Epoch: 2 \t\t\t Iteration 767 Loss Train: 0.4150918424129486\n",
      "Epoch: 2 \t\t\t Iteration 768 Loss Train: 0.8635656833648682\n",
      "Epoch: 2 \t\t\t Iteration 769 Loss Train: 0.5452356338500977\n",
      "Epoch: 2 \t\t\t Iteration 770 Loss Train: 0.49129435420036316\n",
      "Epoch: 2 \t\t\t Iteration 771 Loss Train: 0.39154326915740967\n",
      "Epoch: 2 \t\t\t Iteration 772 Loss Train: 0.5687320232391357\n",
      "Epoch: 2 \t\t\t Iteration 773 Loss Train: 0.39938318729400635\n",
      "Epoch: 2 \t\t\t Iteration 774 Loss Train: 0.30242252349853516\n",
      "Epoch: 2 \t\t\t Iteration 775 Loss Train: 0.4172457754611969\n",
      "Epoch: 2 \t\t\t Iteration 776 Loss Train: 0.6163965463638306\n",
      "Epoch: 2 \t\t\t Iteration 777 Loss Train: 0.5303534269332886\n",
      "Epoch: 2 \t\t\t Iteration 778 Loss Train: 0.51168292760849\n",
      "Epoch: 2 \t\t\t Iteration 779 Loss Train: 0.4141518175601959\n",
      "Epoch: 2 \t\t\t Iteration 780 Loss Train: 0.44169366359710693\n",
      "Epoch: 2 \t\t\t Iteration 781 Loss Train: 0.5117497444152832\n",
      "Epoch: 2 \t\t\t Iteration 782 Loss Train: 0.4937724471092224\n",
      "Epoch: 2 \t\t\t Iteration 783 Loss Train: 0.41330742835998535\n",
      "Epoch: 2 \t\t\t Iteration 784 Loss Train: 0.4143558740615845\n",
      "Epoch: 2 \t\t\t Iteration 785 Loss Train: 0.5058153867721558\n",
      "Epoch: 2 \t\t\t Iteration 786 Loss Train: 0.44254937767982483\n",
      "Epoch: 2 \t\t\t Iteration 787 Loss Train: 0.3322054147720337\n",
      "Epoch: 2 \t\t\t Iteration 788 Loss Train: 0.26986947655677795\n",
      "Epoch: 2 \t\t\t Iteration 789 Loss Train: 0.36390408873558044\n",
      "Epoch: 2 \t\t\t Iteration 790 Loss Train: 0.6372867822647095\n",
      "Epoch: 2 \t\t\t Iteration 791 Loss Train: 0.42007195949554443\n",
      "Epoch: 2 \t\t\t Iteration 792 Loss Train: 0.4442453384399414\n",
      "Epoch: 2 \t\t\t Iteration 793 Loss Train: 0.4401736855506897\n",
      "Epoch: 2 \t\t\t Iteration 794 Loss Train: 0.5339523553848267\n",
      "Epoch: 2 \t\t\t Iteration 795 Loss Train: 0.45933306217193604\n",
      "Epoch: 2 \t\t\t Iteration 796 Loss Train: 0.3789607584476471\n",
      "Epoch: 2 \t\t\t Iteration 797 Loss Train: 0.2848442792892456\n",
      "Epoch: 2 \t\t\t Iteration 798 Loss Train: 0.42006194591522217\n",
      "Epoch: 2 \t\t\t Iteration 799 Loss Train: 0.40342772006988525\n",
      "Epoch: 2 \t\t\t Iteration 800 Loss Train: 0.3096475601196289\n",
      "Epoch: 2 \t\t\t Iteration 801 Loss Train: 0.31516215205192566\n",
      "Epoch: 2 \t\t\t Iteration 802 Loss Train: 0.27710461616516113\n",
      "Epoch: 2 \t\t\t Iteration 803 Loss Train: 0.6468112468719482\n",
      "Epoch: 2 \t\t\t Iteration 804 Loss Train: 0.5006835460662842\n",
      "Epoch: 2 \t\t\t Iteration 805 Loss Train: 0.5148869752883911\n",
      "Epoch: 2 \t\t\t Iteration 806 Loss Train: 0.2743147909641266\n",
      "Epoch: 2 \t\t\t Iteration 807 Loss Train: 0.49916279315948486\n",
      "Epoch: 2 \t\t\t Iteration 808 Loss Train: 0.5002914071083069\n",
      "Epoch: 2 \t\t\t Iteration 809 Loss Train: 0.6182609796524048\n",
      "Epoch: 2 \t\t\t Iteration 810 Loss Train: 0.5073873996734619\n",
      "Epoch: 2 \t\t\t Iteration 811 Loss Train: 0.37307190895080566\n",
      "Epoch: 2 \t\t\t Iteration 812 Loss Train: 0.29795464873313904\n",
      "Epoch: 2 \t\t\t Iteration 813 Loss Train: 0.2818286418914795\n",
      "Epoch: 2 \t\t\t Iteration 814 Loss Train: 0.47688478231430054\n",
      "Epoch: 2 \t\t\t Iteration 815 Loss Train: 0.5121064782142639\n",
      "Epoch: 2 \t\t\t Iteration 816 Loss Train: 0.33968663215637207\n",
      "Epoch: 2 \t\t\t Iteration 817 Loss Train: 0.37010473012924194\n",
      "Epoch: 2 \t\t\t Iteration 818 Loss Train: 0.501731276512146\n",
      "Epoch: 2 \t\t\t Iteration 819 Loss Train: 0.3963264226913452\n",
      "Epoch: 2 \t\t\t Iteration 820 Loss Train: 0.49067240953445435\n",
      "Epoch: 2 \t\t\t Iteration 821 Loss Train: 0.49033159017562866\n",
      "Epoch: 2 \t\t\t Iteration 822 Loss Train: 0.4622458219528198\n",
      "Epoch: 2 \t\t\t Iteration 823 Loss Train: 0.49340978264808655\n",
      "Epoch: 2 \t\t\t Iteration 824 Loss Train: 0.469978004693985\n",
      "Epoch: 2 \t\t\t Iteration 825 Loss Train: 0.3552471995353699\n",
      "Epoch: 2 \t\t\t Iteration 826 Loss Train: 0.33269283175468445\n",
      "Epoch: 2 \t\t\t Iteration 827 Loss Train: 0.3839811682701111\n",
      "Epoch: 2 \t\t\t Iteration 828 Loss Train: 0.5326548218727112\n",
      "Epoch: 2 \t\t\t Iteration 829 Loss Train: 0.5365495681762695\n",
      "Epoch: 2 \t\t\t Iteration 830 Loss Train: 1.1257269382476807\n",
      "Epoch: 2 \t\t\t Iteration 831 Loss Train: 0.38660866022109985\n",
      "Epoch: 2 \t\t\t Iteration 832 Loss Train: 0.3070835471153259\n",
      "Epoch: 2 \t\t\t Iteration 833 Loss Train: 0.45989012718200684\n",
      "Epoch: 2 \t\t\t Iteration 834 Loss Train: 0.3033710718154907\n",
      "Epoch: 2 \t\t\t Iteration 835 Loss Train: 0.3715137243270874\n",
      "Epoch: 2 \t\t\t Iteration 836 Loss Train: 0.3545069396495819\n",
      "Epoch: 2 \t\t\t Iteration 837 Loss Train: 0.6156121492385864\n",
      "Epoch: 2 \t\t\t Iteration 838 Loss Train: 0.4975231885910034\n",
      "Epoch: 2 \t\t\t Iteration 839 Loss Train: 0.49019256234169006\n",
      "Epoch: 2 \t\t\t Iteration 840 Loss Train: 0.4281504154205322\n",
      "Epoch: 2 \t\t\t Iteration 841 Loss Train: 0.5581216812133789\n",
      "Epoch: 2 \t\t\t Iteration 842 Loss Train: 0.2667738199234009\n",
      "Epoch: 2 \t\t\t Iteration 843 Loss Train: 0.30919432640075684\n",
      "Epoch: 2 \t\t\t Iteration 844 Loss Train: 0.43105047941207886\n",
      "Epoch: 2 \t\t\t Iteration 845 Loss Train: 0.5666614770889282\n",
      "Epoch: 2 \t\t\t Iteration 846 Loss Train: 0.44742923974990845\n",
      "Epoch: 2 \t\t\t Iteration 847 Loss Train: 0.36054590344429016\n",
      "Epoch: 2 \t\t\t Iteration 848 Loss Train: 0.42994022369384766\n",
      "Epoch: 2 \t\t\t Iteration 849 Loss Train: 0.36128363013267517\n",
      "Epoch: 2 \t\t\t Iteration 850 Loss Train: 0.32952791452407837\n",
      "Epoch: 2 \t\t\t Iteration 851 Loss Train: 0.48499831557273865\n",
      "Epoch: 2 \t\t\t Iteration 852 Loss Train: 0.4969719350337982\n",
      "Epoch: 2 \t\t\t Iteration 853 Loss Train: 0.3783959746360779\n",
      "Epoch: 2 \t\t\t Iteration 854 Loss Train: 0.4578670263290405\n",
      "Epoch: 2 \t\t\t Iteration 855 Loss Train: 0.3255549967288971\n",
      "Epoch: 2 \t\t\t Iteration 856 Loss Train: 0.5478097796440125\n",
      "Epoch: 2 \t\t\t Iteration 857 Loss Train: 0.359774649143219\n",
      "Epoch: 2 \t\t\t Iteration 858 Loss Train: 0.391137957572937\n",
      "Epoch: 2 \t\t\t Iteration 859 Loss Train: 0.3660767674446106\n",
      "Epoch: 2 \t\t\t Iteration 860 Loss Train: 0.41908425092697144\n",
      "Epoch: 2 \t\t\t Iteration 861 Loss Train: 0.3638668656349182\n",
      "Epoch: 2 \t\t\t Iteration 862 Loss Train: 0.2768635153770447\n",
      "Epoch: 2 \t\t\t Iteration 863 Loss Train: 0.5251365900039673\n",
      "Epoch: 2 \t\t\t Iteration 864 Loss Train: 0.6176668405532837\n",
      "Epoch: 2 \t\t\t Iteration 865 Loss Train: 0.5062531232833862\n",
      "Epoch: 2 \t\t\t Iteration 866 Loss Train: 0.26180267333984375\n",
      "Epoch: 2 \t\t\t Iteration 867 Loss Train: 0.39219069480895996\n",
      "Epoch: 2 \t\t\t Iteration 868 Loss Train: 0.6375656127929688\n",
      "Epoch: 2 \t\t\t Iteration 869 Loss Train: 0.4709293246269226\n",
      "Epoch: 2 \t\t\t Iteration 870 Loss Train: 0.4332769215106964\n",
      "Epoch: 2 \t\t\t Iteration 871 Loss Train: 0.42135584354400635\n",
      "Epoch: 2 \t\t\t Iteration 872 Loss Train: 0.3901846408843994\n",
      "Epoch: 2 \t\t\t Iteration 873 Loss Train: 0.5901046991348267\n",
      "Epoch: 2 \t\t\t Iteration 874 Loss Train: 0.5748800039291382\n",
      "Epoch: 2 \t\t\t Iteration 875 Loss Train: 0.39151474833488464\n",
      "Epoch: 2 \t\t\t Iteration 876 Loss Train: 0.2894955277442932\n",
      "Epoch: 2 \t\t\t Iteration 877 Loss Train: 0.5274320244789124\n",
      "Epoch: 2 \t\t\t Iteration 878 Loss Train: 0.43135732412338257\n",
      "Epoch: 2 \t\t\t Iteration 879 Loss Train: 0.5570988655090332\n",
      "Epoch: 2 \t\t\t Iteration 880 Loss Train: 0.56624436378479\n",
      "Epoch: 2 \t\t\t Iteration 881 Loss Train: 0.45586422085762024\n",
      "Epoch: 2 \t\t\t Iteration 882 Loss Train: 0.5462180972099304\n",
      "Epoch: 2 \t\t\t Iteration 883 Loss Train: 0.475154310464859\n",
      "Epoch: 2 \t\t\t Iteration 884 Loss Train: 0.6304745674133301\n",
      "Epoch: 2 \t\t\t Iteration 885 Loss Train: 0.4812444746494293\n",
      "Epoch: 2 \t\t\t Iteration 886 Loss Train: 0.33104562759399414\n",
      "Epoch: 2 \t\t\t Iteration 887 Loss Train: 0.4315509796142578\n",
      "Epoch: 2 \t\t\t Iteration 888 Loss Train: 0.4196265935897827\n",
      "Epoch: 2 \t\t\t Iteration 889 Loss Train: 0.35313862562179565\n",
      "Epoch: 2 \t\t\t Iteration 890 Loss Train: 0.7308384776115417\n",
      "Epoch: 2 \t\t\t Iteration 891 Loss Train: 0.47568872570991516\n",
      "Epoch: 2 \t\t\t Iteration 892 Loss Train: 0.4543251097202301\n",
      "Epoch: 2 \t\t\t Iteration 893 Loss Train: 0.3260226845741272\n",
      "Epoch: 2 \t\t\t Iteration 894 Loss Train: 0.453133761882782\n",
      "Epoch: 2 \t\t\t Iteration 895 Loss Train: 0.5548158884048462\n",
      "Epoch: 2 \t\t\t Iteration 896 Loss Train: 0.30575835704803467\n",
      "Epoch: 2 \t\t\t Iteration 897 Loss Train: 0.4621400833129883\n",
      "Epoch: 2 \t\t\t Iteration 898 Loss Train: 0.4140252470970154\n",
      "Epoch: 2 \t\t\t Iteration 899 Loss Train: 0.5475554466247559\n",
      "Epoch: 2 \t\t\t Iteration 900 Loss Train: 0.27087411284446716\n",
      "Epoch: 2 \t\t\t Iteration 901 Loss Train: 0.5214411020278931\n",
      "Epoch: 2 \t\t\t Iteration 902 Loss Train: 0.45221802592277527\n",
      "Epoch: 2 \t\t\t Iteration 903 Loss Train: 0.48015284538269043\n",
      "Epoch: 2 \t\t\t Iteration 904 Loss Train: 0.41798776388168335\n",
      "Epoch: 2 \t\t\t Iteration 905 Loss Train: 0.4284482002258301\n",
      "Epoch: 2 \t\t\t Iteration 906 Loss Train: 0.28835225105285645\n",
      "Epoch: 2 \t\t\t Iteration 907 Loss Train: 0.5732504725456238\n",
      "Epoch: 2 \t\t\t Iteration 908 Loss Train: 0.45987874269485474\n",
      "Epoch: 2 \t\t\t Iteration 909 Loss Train: 0.4105936288833618\n",
      "Epoch: 2 \t\t\t Iteration 910 Loss Train: 0.6814762353897095\n",
      "Epoch: 2 \t\t\t Iteration 911 Loss Train: 0.5443506836891174\n",
      "Epoch: 2 \t\t\t Iteration 912 Loss Train: 0.4482627809047699\n",
      "Epoch: 2 \t\t\t Iteration 913 Loss Train: 0.334625244140625\n",
      "Epoch: 2 \t\t\t Iteration 914 Loss Train: 0.35113272070884705\n",
      "Epoch: 2 \t\t\t Iteration 915 Loss Train: 0.4472225308418274\n",
      "Epoch: 2 \t\t\t Iteration 916 Loss Train: 0.47770512104034424\n",
      "Epoch: 2 \t\t\t Iteration 917 Loss Train: 0.5064502954483032\n",
      "Epoch: 2 \t\t\t Iteration 918 Loss Train: 0.4388742446899414\n",
      "Epoch: 2 \t\t\t Iteration 919 Loss Train: 0.39333707094192505\n",
      "Epoch: 2 \t\t\t Iteration 920 Loss Train: 0.46094146370887756\n",
      "Epoch: 2 \t\t\t Iteration 921 Loss Train: 0.43677228689193726\n",
      "Epoch: 2 \t\t\t Iteration 922 Loss Train: 0.30867618322372437\n",
      "Epoch: 2 \t\t\t Iteration 923 Loss Train: 0.5291500091552734\n",
      "Epoch: 2 \t\t\t Iteration 924 Loss Train: 0.5070523023605347\n",
      "Epoch: 2 \t\t\t Iteration 925 Loss Train: 0.44141286611557007\n",
      "Epoch: 2 \t\t\t Iteration 926 Loss Train: 0.3735834062099457\n",
      "Epoch: 2 \t\t\t Iteration 927 Loss Train: 0.4852864742279053\n",
      "Epoch: 2 \t\t\t Iteration 928 Loss Train: 0.5416556000709534\n",
      "Epoch: 2 \t\t\t Iteration 929 Loss Train: 0.47223961353302\n",
      "Epoch: 2 \t\t\t Iteration 930 Loss Train: 0.32916590571403503\n",
      "Epoch: 2 \t\t\t Iteration 931 Loss Train: 0.440805584192276\n",
      "Epoch: 2 \t\t\t Iteration 932 Loss Train: 0.3325554430484772\n",
      "Epoch: 2 \t\t\t Iteration 933 Loss Train: 0.3750866651535034\n",
      "Epoch: 2 \t\t\t Iteration 934 Loss Train: 0.5097202658653259\n",
      "Epoch: 2 \t\t\t Iteration 935 Loss Train: 0.66208815574646\n",
      "Epoch: 2 \t\t\t Iteration 936 Loss Train: 0.6254062652587891\n",
      "Epoch: 2 \t\t\t Iteration 937 Loss Train: 0.364658921957016\n",
      "Epoch: 2 \t\t\t Iteration 938 Loss Train: 0.5377558469772339\n",
      "Epoch: 2 \t\t\t Iteration 939 Loss Train: 0.4425812363624573\n",
      "Epoch: 2 \t\t\t Iteration 940 Loss Train: 0.4140778183937073\n",
      "Epoch: 2 \t\t\t Iteration 941 Loss Train: 0.33956480026245117\n",
      "Epoch: 2 \t\t\t Iteration 942 Loss Train: 0.52073073387146\n",
      "Epoch: 2 \t\t\t Iteration 943 Loss Train: 0.37062281370162964\n",
      "Epoch: 2 \t\t\t Iteration 944 Loss Train: 0.4094015657901764\n",
      "Epoch: 2 \t\t\t Iteration 945 Loss Train: 0.4052089750766754\n",
      "Epoch: 2 \t\t\t Iteration 946 Loss Train: 0.3342706263065338\n",
      "Epoch: 2 \t\t\t Iteration 947 Loss Train: 0.21266788244247437\n",
      "Epoch: 2 \t\t\t Iteration 948 Loss Train: 0.12189480662345886\n",
      "Epoch: 2 \t\t\t Iteration 949 Loss Train: 0.38295674324035645\n",
      "Epoch: 2 \t\t\t Iteration 950 Loss Train: 0.4065011143684387\n",
      "Epoch: 2 \t\t\t Iteration 951 Loss Train: 0.4739421010017395\n",
      "Epoch: 2 \t\t\t Iteration 952 Loss Train: 0.42720746994018555\n",
      "Epoch: 2 \t\t\t Iteration 953 Loss Train: 0.24113523960113525\n",
      "Epoch: 2 \t\t\t Iteration 954 Loss Train: 0.6576550006866455\n",
      "Epoch: 2 \t\t\t Iteration 955 Loss Train: 0.5409458875656128\n",
      "Epoch: 2 \t\t\t Iteration 956 Loss Train: 0.43378081917762756\n",
      "Epoch: 2 \t\t\t Iteration 957 Loss Train: 0.5394785404205322\n",
      "Epoch: 2 \t\t\t Iteration 958 Loss Train: 0.6074808239936829\n",
      "Epoch: 2 \t\t\t Iteration 959 Loss Train: 0.5118695497512817\n",
      "Epoch: 2 \t\t\t Iteration 960 Loss Train: 0.32970988750457764\n",
      "Epoch: 2 \t\t\t Iteration 961 Loss Train: 0.8491952419281006\n",
      "Epoch: 2 \t\t\t Iteration 962 Loss Train: 0.36812928318977356\n",
      "Epoch: 2 \t\t\t Iteration 963 Loss Train: 0.4416448771953583\n",
      "Epoch: 2 \t\t\t Iteration 964 Loss Train: 0.40666043758392334\n",
      "Epoch: 2 \t\t\t Iteration 965 Loss Train: 0.4685540795326233\n",
      "Epoch: 2 \t\t\t Iteration 966 Loss Train: 0.3861083984375\n",
      "Epoch: 2 \t\t\t Iteration 967 Loss Train: 0.24225248396396637\n",
      "Epoch: 2 \t\t\t Iteration 968 Loss Train: 0.3849576711654663\n",
      "Epoch: 2 \t\t\t Iteration 969 Loss Train: 0.5364590287208557\n",
      "Epoch: 2 \t\t\t Iteration 970 Loss Train: 0.3460560441017151\n",
      "Epoch: 2 \t\t\t Iteration 971 Loss Train: 0.45535606145858765\n",
      "Epoch: 2 \t\t\t Iteration 972 Loss Train: 0.6318597793579102\n",
      "Epoch: 2 \t\t\t Iteration 973 Loss Train: 0.5566803812980652\n",
      "Epoch: 2 \t\t\t Iteration 974 Loss Train: 0.3872532248497009\n",
      "Epoch: 2 \t\t\t Iteration 975 Loss Train: 0.5145364999771118\n",
      "Epoch: 2 \t\t\t Iteration 976 Loss Train: 0.41214868426322937\n",
      "Epoch: 2 \t\t\t Iteration 977 Loss Train: 0.4190986156463623\n",
      "Epoch: 2 \t\t\t Iteration 978 Loss Train: 0.5243397951126099\n",
      "Epoch: 2 \t\t\t Iteration 979 Loss Train: 0.4490891695022583\n",
      "Epoch: 2 \t\t\t Iteration 980 Loss Train: 0.3644754886627197\n",
      "Epoch: 2 \t\t\t Iteration 981 Loss Train: 0.21309325098991394\n",
      "Epoch: 2 \t\t\t Iteration 982 Loss Train: 0.4404934048652649\n",
      "Epoch: 2 \t\t\t Iteration 983 Loss Train: 0.4220823645591736\n",
      "Epoch: 2 \t\t\t Iteration 984 Loss Train: 0.6205503940582275\n",
      "Epoch: 2 \t\t\t Iteration 985 Loss Train: 0.537076473236084\n",
      "Epoch: 2 \t\t\t Iteration 986 Loss Train: 0.46997904777526855\n",
      "Epoch: 2 \t\t\t Iteration 987 Loss Train: 0.45333802700042725\n",
      "Epoch: 2 \t\t\t Iteration 988 Loss Train: 0.2714691758155823\n",
      "Epoch: 2 \t\t\t Iteration 989 Loss Train: 0.3568662106990814\n",
      "Epoch: 2 \t\t\t Iteration 990 Loss Train: 0.35508108139038086\n",
      "Epoch: 2 \t\t\t Iteration 991 Loss Train: 0.32221120595932007\n",
      "Epoch: 2 \t\t\t Iteration 992 Loss Train: 0.4340519905090332\n",
      "Epoch: 2 \t\t\t Iteration 993 Loss Train: 0.4807549715042114\n",
      "Epoch: 2 \t\t\t Iteration 994 Loss Train: 0.3309746980667114\n",
      "Epoch: 2 \t\t\t Iteration 995 Loss Train: 0.34584924578666687\n",
      "Epoch: 2 \t\t\t Iteration 996 Loss Train: 0.6551743745803833\n",
      "Epoch: 2 \t\t\t Iteration 997 Loss Train: 0.4948148727416992\n",
      "Epoch: 2 \t\t\t Iteration 998 Loss Train: 0.4903087317943573\n",
      "Epoch: 2 \t\t\t Iteration 999 Loss Train: 0.6298120617866516\n",
      "Epoch: 2 \t\t\t Iteration 1000 Loss Train: 0.5229424834251404\n",
      "Epoch: 2 \t\t\t Iteration 1001 Loss Train: 0.4452506899833679\n",
      "Epoch: 2 \t\t\t Iteration 1002 Loss Train: 0.5275302529335022\n",
      "Epoch: 2 \t\t\t Iteration 1003 Loss Train: 0.23520252108573914\n",
      "Epoch: 2 \t\t\t Iteration 1004 Loss Train: 0.32651233673095703\n",
      "Epoch: 2 \t\t\t Iteration 1005 Loss Train: 0.5155689716339111\n",
      "Epoch: 2 \t\t\t Iteration 1006 Loss Train: 0.336532860994339\n",
      "Epoch: 2 \t\t\t Iteration 1007 Loss Train: 0.4582061469554901\n",
      "Epoch: 2 \t\t\t Iteration 1008 Loss Train: 0.38276755809783936\n",
      "Epoch: 2 \t\t\t Iteration 1009 Loss Train: 0.39029279351234436\n",
      "Epoch: 2 \t\t\t Iteration 1010 Loss Train: 0.4664444923400879\n",
      "Epoch: 2 \t\t\t Iteration 1011 Loss Train: 0.49850159883499146\n",
      "Epoch: 2 \t\t\t Iteration 1012 Loss Train: 0.5089373588562012\n",
      "Epoch: 2 \t\t\t Iteration 1013 Loss Train: 0.2698451280593872\n",
      "Epoch: 2 \t\t\t Iteration 1014 Loss Train: 0.5616030693054199\n",
      "Epoch: 2 \t\t\t Iteration 1015 Loss Train: 0.32651960849761963\n",
      "Epoch: 2 \t\t\t Iteration 1016 Loss Train: 0.3912351727485657\n",
      "Epoch: 2 \t\t\t Iteration 1017 Loss Train: 0.5205576419830322\n",
      "Epoch: 2 \t\t\t Iteration 1018 Loss Train: 0.5296654105186462\n",
      "Epoch: 2 \t\t\t Iteration 1019 Loss Train: 0.33027616143226624\n",
      "Epoch: 2 \t\t\t Iteration 1020 Loss Train: 0.4593651592731476\n",
      "Epoch: 2 \t\t\t Iteration 1021 Loss Train: 0.4505397081375122\n",
      "Epoch: 2 \t\t\t Iteration 1022 Loss Train: 0.5602966547012329\n",
      "Epoch: 2 \t\t\t Iteration 1023 Loss Train: 0.6133149862289429\n",
      "Epoch: 2 \t\t\t Iteration 1024 Loss Train: 0.26323428750038147\n",
      "Epoch: 2 \t\t\t Iteration 1025 Loss Train: 0.3911404311656952\n",
      "Epoch: 2 \t\t\t Iteration 1026 Loss Train: 0.44548946619033813\n",
      "Epoch: 2 \t\t\t Iteration 1027 Loss Train: 0.5992611646652222\n",
      "Epoch: 2 \t\t\t Iteration 1028 Loss Train: 0.31050920486450195\n",
      "Epoch: 2 \t\t\t Iteration 1029 Loss Train: 0.1667385846376419\n",
      "Epoch: 2 \t\t\t Iteration 1030 Loss Train: 0.4851308763027191\n",
      "Epoch: 2 \t\t\t Iteration 1031 Loss Train: 0.35033565759658813\n",
      "Epoch: 2 \t\t\t Iteration 1032 Loss Train: 0.46323269605636597\n",
      "Epoch: 2 \t\t\t Iteration 1033 Loss Train: 0.4849153757095337\n",
      "Epoch: 2 \t\t\t Iteration 1034 Loss Train: 0.4694877862930298\n",
      "Epoch: 2 \t\t\t Iteration 1035 Loss Train: 0.333230584859848\n",
      "Epoch: 2 \t\t\t Iteration 1036 Loss Train: 0.28836432099342346\n",
      "Epoch: 2 \t\t\t Iteration 1037 Loss Train: 0.23349541425704956\n",
      "Epoch: 2 \t\t\t Iteration 1038 Loss Train: 0.5463281273841858\n",
      "Epoch: 2 \t\t\t Iteration 1039 Loss Train: 0.40939342975616455\n",
      "Epoch: 2 \t\t\t Iteration 1040 Loss Train: 0.44906336069107056\n",
      "Epoch: 2 \t\t\t Iteration 1041 Loss Train: 0.5288043022155762\n",
      "Epoch: 2 \t\t\t Iteration 1042 Loss Train: 0.4760720133781433\n",
      "Epoch: 2 \t\t\t Iteration 1043 Loss Train: 0.6603024005889893\n",
      "Epoch: 2 \t\t\t Iteration 1044 Loss Train: 0.42115914821624756\n",
      "Epoch: 2 \t\t\t Iteration 1045 Loss Train: 0.38797760009765625\n",
      "Epoch: 2 \t\t\t Iteration 1046 Loss Train: 0.3506154417991638\n",
      "Epoch: 2 \t\t\t Iteration 1047 Loss Train: 0.32426780462265015\n",
      "Epoch: 2 \t\t\t Iteration 1048 Loss Train: 0.5669583082199097\n",
      "Epoch: 2 \t\t\t Iteration 1049 Loss Train: 0.3949744701385498\n",
      "Epoch: 2 \t\t\t Iteration 1050 Loss Train: 0.5032480359077454\n",
      "Epoch: 2 \t\t\t Iteration 1051 Loss Train: 0.33635494112968445\n",
      "Epoch: 2 \t\t\t Iteration 1052 Loss Train: 0.597123384475708\n",
      "Epoch: 2 \t\t\t Iteration 1053 Loss Train: 0.4260835349559784\n",
      "Epoch: 2 \t\t\t Iteration 1054 Loss Train: 0.3460553288459778\n",
      "Epoch: 2 \t\t\t Iteration 1055 Loss Train: 0.5127664804458618\n",
      "Epoch: 2 \t\t\t Iteration 1056 Loss Train: 0.3355144262313843\n",
      "Epoch: 2 \t\t\t Iteration 1057 Loss Train: 0.4175247550010681\n",
      "Epoch: 2 \t\t\t Iteration 1058 Loss Train: 0.5229703783988953\n",
      "Epoch: 2 \t\t\t Iteration 1059 Loss Train: 0.43889617919921875\n",
      "Epoch: 2 \t\t\t Iteration 1060 Loss Train: 0.3868526220321655\n",
      "Epoch: 2 \t\t\t Iteration 1061 Loss Train: 0.22218456864356995\n",
      "Epoch: 2 \t\t\t Iteration 1062 Loss Train: 0.5336582660675049\n",
      "Epoch: 2 \t\t\t Iteration 1063 Loss Train: 0.48245683312416077\n",
      "Epoch: 2 \t\t\t Iteration 1064 Loss Train: 0.34048745036125183\n",
      "Epoch: 2 \t\t\t Iteration 1065 Loss Train: 0.4152398407459259\n",
      "Epoch: 2 \t\t\t Iteration 1066 Loss Train: 0.6024868488311768\n",
      "Epoch: 2 \t\t\t Iteration 1067 Loss Train: 0.4387238025665283\n",
      "Epoch: 2 \t\t\t Iteration 1068 Loss Train: 0.4111196994781494\n",
      "Epoch: 2 \t\t\t Iteration 1069 Loss Train: 0.33426642417907715\n",
      "Epoch: 2 \t\t\t Iteration 1070 Loss Train: 0.3974153995513916\n",
      "Epoch: 2 \t\t\t Iteration 1071 Loss Train: 0.22999098896980286\n",
      "Epoch: 2 \t\t\t Iteration 1072 Loss Train: 0.4407595992088318\n",
      "Epoch: 2 \t\t\t Iteration 1073 Loss Train: 0.4812803268432617\n",
      "Epoch: 2 \t\t\t Iteration 1074 Loss Train: 0.6446601152420044\n",
      "Epoch: 2 \t\t\t Iteration 1075 Loss Train: 0.3734198212623596\n",
      "Epoch: 2 \t\t\t Iteration 1076 Loss Train: 0.3692457675933838\n",
      "Epoch: 2 \t\t\t Iteration 1077 Loss Train: 0.3462616205215454\n",
      "Epoch: 2 \t\t\t Iteration 1078 Loss Train: 0.42236560583114624\n",
      "Epoch: 2 \t\t\t Iteration 1079 Loss Train: 0.34693339467048645\n",
      "Epoch: 2 \t\t\t Iteration 1080 Loss Train: 0.27393555641174316\n",
      "Epoch: 2 \t\t\t Iteration 1081 Loss Train: 0.3299161195755005\n",
      "Epoch: 2 \t\t\t Iteration 1082 Loss Train: 0.2882487177848816\n",
      "Epoch: 2 \t\t\t Iteration 1083 Loss Train: 0.4907957911491394\n",
      "Epoch: 2 \t\t\t Iteration 1084 Loss Train: 0.41875314712524414\n",
      "Epoch: 2 \t\t\t Iteration 1085 Loss Train: 0.590339720249176\n",
      "Epoch: 2 \t\t\t Iteration 1086 Loss Train: 0.5017572641372681\n",
      "Epoch: 2 \t\t\t Iteration 1087 Loss Train: 0.48437052965164185\n",
      "Epoch: 2 \t\t\t Iteration 1088 Loss Train: 0.5478614568710327\n",
      "Epoch: 2 \t\t\t Iteration 1089 Loss Train: 0.5254790782928467\n",
      "Epoch: 2 \t\t\t Iteration 1090 Loss Train: 0.42948514223098755\n",
      "Epoch: 2 \t\t\t Iteration 1091 Loss Train: 0.43977415561676025\n",
      "Epoch: 2 \t\t\t Iteration 1092 Loss Train: 0.43122565746307373\n",
      "Epoch: 2 \t\t\t Iteration 1093 Loss Train: 0.47592389583587646\n",
      "Epoch: 2 \t\t\t Iteration 1094 Loss Train: 0.454612135887146\n",
      "Epoch: 2 \t\t\t Iteration 1095 Loss Train: 0.33180874586105347\n",
      "Epoch: 2 \t\t\t Iteration 1096 Loss Train: 0.3660818040370941\n",
      "Epoch: 2 \t\t\t Iteration 1097 Loss Train: 0.8535635471343994\n",
      "Epoch: 2 \t\t\t Iteration 1098 Loss Train: 0.5089226961135864\n",
      "Epoch: 2 \t\t\t Iteration 1099 Loss Train: 0.39945387840270996\n",
      "Epoch: 2 \t\t\t Iteration 1100 Loss Train: 0.4243621230125427\n",
      "Epoch: 2 \t\t\t Iteration 1101 Loss Train: 0.29597941040992737\n",
      "Epoch: 2 \t\t\t Iteration 1102 Loss Train: 0.33889105916023254\n",
      "Epoch: 2 \t\t\t Iteration 1103 Loss Train: 0.45313572883605957\n",
      "Epoch: 2 \t\t\t Iteration 1104 Loss Train: 0.4323318600654602\n",
      "Epoch: 2 \t\t\t Iteration 1105 Loss Train: 0.3617097735404968\n",
      "Epoch: 2 \t\t\t Iteration 1106 Loss Train: 0.39186057448387146\n",
      "Epoch: 2 \t\t\t Iteration 1107 Loss Train: 0.39546841382980347\n",
      "Epoch: 2 \t\t\t Iteration 1108 Loss Train: 0.23460283875465393\n",
      "Epoch: 2 \t\t\t Iteration 1109 Loss Train: 0.5006020069122314\n",
      "Epoch: 2 \t\t\t Iteration 1110 Loss Train: 0.43014922738075256\n",
      "Epoch: 2 \t\t\t Iteration 1111 Loss Train: 0.5118127465248108\n",
      "Epoch: 2 \t\t\t Iteration 1112 Loss Train: 0.3173167109489441\n",
      "Epoch: 2 \t\t\t Iteration 1113 Loss Train: 0.4783550798892975\n",
      "Epoch: 2 \t\t\t Iteration 1114 Loss Train: 0.33421817421913147\n",
      "Epoch: 2 \t\t\t Iteration 1115 Loss Train: 0.38169193267822266\n",
      "Epoch: 2 \t\t\t Iteration 1116 Loss Train: 0.47465085983276367\n",
      "Epoch: 2 \t\t\t Iteration 1117 Loss Train: 0.47804057598114014\n",
      "Epoch: 2 \t\t\t Iteration 1118 Loss Train: 0.3573455214500427\n",
      "Epoch: 2 \t\t\t Iteration 1119 Loss Train: 0.3677419424057007\n",
      "Epoch: 2 \t\t\t Iteration 1120 Loss Train: 0.42621105909347534\n",
      "Epoch: 2 \t\t\t Iteration 1121 Loss Train: 0.3560548424720764\n",
      "Epoch: 2 \t\t\t Iteration 1122 Loss Train: 0.48484718799591064\n",
      "Epoch: 2 \t\t\t Iteration 1123 Loss Train: 0.473651260137558\n",
      "Epoch: 2 \t\t\t Iteration 1124 Loss Train: 0.5219699740409851\n",
      "Epoch: 2 \t\t\t Iteration 1125 Loss Train: 0.4383162558078766\n",
      "Epoch: 2 \t\t\t Iteration 1126 Loss Train: 0.4481144845485687\n",
      "Epoch: 2 \t\t\t Iteration 1127 Loss Train: 0.3931523263454437\n",
      "Epoch: 2 \t\t\t Iteration 1128 Loss Train: 0.24171753227710724\n",
      "Epoch: 2 \t\t\t Iteration 1129 Loss Train: 0.63741534948349\n",
      "Epoch: 2 \t\t\t Iteration 1130 Loss Train: 0.36863505840301514\n",
      "Epoch: 2 \t\t\t Iteration 1131 Loss Train: 0.4792739748954773\n",
      "Epoch: 2 \t\t\t Iteration 1132 Loss Train: 0.37617745995521545\n",
      "Epoch: 2 \t\t\t Iteration 1133 Loss Train: 0.5571557283401489\n",
      "Epoch: 2 \t\t\t Iteration 1134 Loss Train: 0.374009370803833\n",
      "Epoch: 2 \t\t\t Iteration 1135 Loss Train: 0.26608598232269287\n",
      "Epoch: 2 \t\t\t Iteration 1136 Loss Train: 0.6186838746070862\n",
      "Epoch: 2 \t\t\t Iteration 1137 Loss Train: 0.45379137992858887\n",
      "Epoch: 2 \t\t\t Iteration 1138 Loss Train: 0.3742188513278961\n",
      "Epoch: 2 \t\t\t Iteration 1139 Loss Train: 0.3352425694465637\n",
      "Epoch: 2 \t\t\t Iteration 1140 Loss Train: 0.4727533459663391\n",
      "Epoch: 2 \t\t\t Iteration 1141 Loss Train: 0.5476773977279663\n",
      "Epoch: 2 \t\t\t Iteration 1142 Loss Train: 0.3977286219596863\n",
      "Epoch: 2 \t\t\t Iteration 1143 Loss Train: 0.3841622471809387\n",
      "Epoch: 2 \t\t\t Iteration 1144 Loss Train: 0.5038254857063293\n",
      "Epoch: 2 \t\t\t Iteration 1145 Loss Train: 0.4860247075557709\n",
      "Epoch: 2 \t\t\t Iteration 1146 Loss Train: 0.4523671865463257\n",
      "Epoch: 2 \t\t\t Iteration 1147 Loss Train: 0.4397207796573639\n",
      "Epoch: 2 \t\t\t Iteration 1148 Loss Train: 0.3311127722263336\n",
      "Epoch: 2 \t\t\t Iteration 1149 Loss Train: 0.3856010138988495\n",
      "Epoch: 2 \t\t\t Iteration 1150 Loss Train: 0.3851991593837738\n",
      "Epoch: 2 \t\t\t Iteration 1151 Loss Train: 0.5515889525413513\n",
      "Epoch: 2 \t\t\t Iteration 1152 Loss Train: 0.5260395407676697\n",
      "Epoch: 2 \t\t\t Iteration 1153 Loss Train: 0.2898808717727661\n",
      "Epoch: 2 \t\t\t Iteration 1154 Loss Train: 0.6070991158485413\n",
      "Epoch: 2 \t\t\t Iteration 1155 Loss Train: 0.3805869221687317\n",
      "Epoch: 2 \t\t\t Iteration 1156 Loss Train: 0.4353516101837158\n",
      "Epoch: 2 \t\t\t Iteration 1157 Loss Train: 0.4328153133392334\n",
      "Epoch: 2 \t\t\t Iteration 1158 Loss Train: 0.36977142095565796\n",
      "Epoch: 2 \t\t\t Iteration 1159 Loss Train: 0.3089703321456909\n",
      "Epoch: 2 \t\t\t Iteration 1160 Loss Train: 0.36027708649635315\n",
      "Epoch: 2 \t\t\t Iteration 1161 Loss Train: 0.25639402866363525\n",
      "Epoch: 2 \t\t\t Iteration 1162 Loss Train: 0.27077001333236694\n",
      "Epoch: 2 \t\t\t Iteration 1163 Loss Train: 0.1928228735923767\n",
      "Epoch: 2 \t\t\t Iteration 1164 Loss Train: 0.7202097773551941\n",
      "Epoch: 2 \t\t\t Iteration 1165 Loss Train: 0.4856094717979431\n",
      "Epoch: 2 \t\t\t Iteration 1166 Loss Train: 0.3774775266647339\n",
      "Epoch: 2 \t\t\t Iteration 1167 Loss Train: 0.3534269332885742\n",
      "Epoch: 2 \t\t\t Iteration 1168 Loss Train: 0.4422396123409271\n",
      "Epoch: 2 \t\t\t Iteration 1169 Loss Train: 0.41441261768341064\n",
      "Epoch: 2 \t\t\t Iteration 1170 Loss Train: 0.5814099907875061\n",
      "Epoch: 2 \t\t\t Iteration 1171 Loss Train: 0.327056348323822\n",
      "Epoch: 2 \t\t\t Iteration 1172 Loss Train: 0.4124974012374878\n",
      "Epoch: 2 \t\t\t Iteration 1173 Loss Train: 0.43494200706481934\n",
      "Epoch: 2 \t\t\t Iteration 1174 Loss Train: 0.33395910263061523\n",
      "Epoch: 2 \t\t\t Iteration 1175 Loss Train: 0.4004254937171936\n",
      "Epoch: 2 \t\t\t Iteration 1176 Loss Train: 0.4731249213218689\n",
      "Epoch: 2 \t\t\t Iteration 1177 Loss Train: 0.6096757650375366\n",
      "Epoch: 2 \t\t\t Iteration 1178 Loss Train: 0.45118194818496704\n",
      "Epoch: 2 \t\t\t Iteration 1179 Loss Train: 0.34271320700645447\n",
      "Epoch: 2 \t\t\t Iteration 1180 Loss Train: 0.32939255237579346\n",
      "Epoch: 2 \t\t\t Iteration 1181 Loss Train: 0.6652176380157471\n",
      "Epoch: 2 \t\t\t Iteration 1182 Loss Train: 0.41649237275123596\n",
      "Epoch: 2 \t\t\t Iteration 1183 Loss Train: 0.5041489601135254\n",
      "Epoch: 2 \t\t\t Iteration 1184 Loss Train: 0.4909203052520752\n",
      "Epoch: 2 \t\t\t Iteration 1185 Loss Train: 0.5104809999465942\n",
      "Epoch: 2 \t\t\t Iteration 1186 Loss Train: 0.25234439969062805\n",
      "Epoch: 2 \t\t\t Iteration 1187 Loss Train: 0.33431726694107056\n",
      "Epoch: 2 \t\t\t Iteration 1188 Loss Train: 0.4579014182090759\n",
      "Epoch: 2 \t\t\t Iteration 1189 Loss Train: 0.4449419677257538\n",
      "Epoch: 2 \t\t\t Iteration 1190 Loss Train: 0.3845880329608917\n",
      "Epoch: 2 \t\t\t Iteration 1191 Loss Train: 0.3535570502281189\n",
      "Epoch: 2 \t\t\t Iteration 1192 Loss Train: 0.3563190698623657\n",
      "Epoch: 2 \t\t\t Iteration 1193 Loss Train: 0.3916319012641907\n",
      "Epoch: 2 \t\t\t Iteration 1194 Loss Train: 0.5231409072875977\n",
      "Epoch: 2 \t\t\t Iteration 1195 Loss Train: 0.4288393259048462\n",
      "Epoch: 2 \t\t\t Iteration 1196 Loss Train: 0.6900973320007324\n",
      "Epoch: 2 \t\t\t Iteration 1197 Loss Train: 0.46718183159828186\n",
      "Epoch: 2 \t\t\t Iteration 1198 Loss Train: 0.5558432340621948\n",
      "Epoch: 2 \t\t\t Iteration 1199 Loss Train: 0.36835014820098877\n",
      "Epoch: 2 \t\t\t Iteration 1200 Loss Train: 0.39736729860305786\n",
      "Epoch: 2 \t\t\t Iteration 1201 Loss Train: 0.5033190846443176\n",
      "Epoch: 2 \t\t\t Iteration 1202 Loss Train: 0.2572839856147766\n",
      "Epoch: 2 \t\t\t Iteration 1203 Loss Train: 0.4259166121482849\n",
      "Epoch: 2 \t\t\t Iteration 1204 Loss Train: 0.5472128391265869\n",
      "Epoch: 2 \t\t\t Iteration 1205 Loss Train: 0.46743935346603394\n",
      "Epoch: 2 \t\t\t Iteration 1206 Loss Train: 0.5241959095001221\n",
      "Epoch: 2 \t\t\t Iteration 1207 Loss Train: 0.5028868317604065\n",
      "Epoch: 2 \t\t\t Iteration 1208 Loss Train: 0.3124706745147705\n",
      "Epoch: 2 \t\t\t Iteration 1209 Loss Train: 0.35752958059310913\n",
      "Epoch: 2 \t\t\t Iteration 1210 Loss Train: 0.26594045758247375\n",
      "Epoch: 2 \t\t\t Iteration 1211 Loss Train: 0.5603438019752502\n",
      "Epoch: 2 \t\t\t Iteration 1212 Loss Train: 0.43519848585128784\n",
      "Epoch: 2 \t\t\t Iteration 1213 Loss Train: 0.4043335020542145\n",
      "Epoch: 2 \t\t\t Iteration 1214 Loss Train: 0.4839482307434082\n",
      "Epoch: 2 \t\t\t Iteration 1215 Loss Train: 0.3858431577682495\n",
      "Epoch: 2 \t\t\t Iteration 1216 Loss Train: 0.33664920926094055\n",
      "Epoch: 2 \t\t\t Iteration 1217 Loss Train: 0.6780988574028015\n",
      "Epoch: 2 \t\t\t Iteration 1218 Loss Train: 0.5576661825180054\n",
      "Epoch: 2 \t\t\t Iteration 1219 Loss Train: 0.3425215184688568\n",
      "Epoch: 2 \t\t\t Iteration 1220 Loss Train: 0.525762677192688\n",
      "Epoch: 2 \t\t\t Iteration 1221 Loss Train: 0.3039610981941223\n",
      "Epoch: 2 \t\t\t Iteration 1222 Loss Train: 0.30345016717910767\n",
      "Epoch: 2 \t\t\t Iteration 1223 Loss Train: 0.4350242018699646\n",
      "Epoch: 2 \t\t\t Iteration 1224 Loss Train: 0.3839713931083679\n",
      "Epoch: 2 \t\t\t Iteration 1225 Loss Train: 0.29441726207733154\n",
      "Epoch: 2 \t\t\t Iteration 1226 Loss Train: 0.5249168276786804\n",
      "Epoch: 2 \t\t\t Iteration 1227 Loss Train: 0.43752437829971313\n",
      "Epoch: 2 \t\t\t Iteration 1228 Loss Train: 0.39411166310310364\n",
      "Epoch: 2 \t\t\t Iteration 1229 Loss Train: 0.5175215005874634\n",
      "Epoch: 2 \t\t\t Iteration 1230 Loss Train: 0.3415229320526123\n",
      "Epoch: 2 \t\t\t Iteration 1231 Loss Train: 0.32978683710098267\n",
      "Epoch: 2 \t\t\t Iteration 1232 Loss Train: 0.42134860157966614\n",
      "Epoch: 2 \t\t\t Iteration 1233 Loss Train: 0.2782105803489685\n",
      "Epoch: 2 \t\t\t Iteration 1234 Loss Train: 0.5375418066978455\n",
      "Epoch: 2 \t\t\t Iteration 1235 Loss Train: 0.3423885703086853\n",
      "Epoch: 2 \t\t\t Iteration 1236 Loss Train: 0.3809395432472229\n",
      "Epoch: 2 \t\t\t Iteration 1237 Loss Train: 0.35335448384284973\n",
      "Epoch: 2 \t\t\t Iteration 1238 Loss Train: 0.7354981899261475\n",
      "Epoch: 2 \t\t\t Iteration 1239 Loss Train: 0.6497763395309448\n",
      "Epoch: 2 \t\t\t Iteration 1240 Loss Train: 0.4788006842136383\n",
      "Epoch: 2 \t\t\t Iteration 1241 Loss Train: 0.3758440613746643\n",
      "Epoch: 2 \t\t\t Iteration 1242 Loss Train: 0.40872982144355774\n",
      "Epoch: 2 \t\t\t Iteration 1243 Loss Train: 0.3469174802303314\n",
      "Epoch: 2 \t\t\t Iteration 1244 Loss Train: 0.5220348834991455\n",
      "Epoch: 2 \t\t\t Iteration 1245 Loss Train: 0.24005500972270966\n",
      "Epoch: 2 \t\t\t Iteration 1246 Loss Train: 0.39411821961402893\n",
      "Epoch: 2 \t\t\t Iteration 1247 Loss Train: 0.39849066734313965\n",
      "Epoch: 2 \t\t\t Iteration 1248 Loss Train: 0.23801636695861816\n",
      "Epoch: 2 \t\t\t Iteration 1249 Loss Train: 0.6281328201293945\n",
      "Epoch: 2 \t\t\t Iteration 1250 Loss Train: 0.4024509787559509\n",
      "Epoch: 2 \t\t\t Iteration 1251 Loss Train: 0.39329907298088074\n",
      "Epoch: 2 \t\t\t Iteration 1252 Loss Train: 0.5102371573448181\n",
      "Epoch: 2 \t\t\t Iteration 1253 Loss Train: 0.37123560905456543\n",
      "Epoch: 2 \t\t\t Iteration 1254 Loss Train: 0.4483542740345001\n",
      "Epoch: 2 \t\t\t Iteration 1255 Loss Train: 0.26891443133354187\n",
      "Epoch: 2 \t\t\t Iteration 1256 Loss Train: 0.37921541929244995\n",
      "Epoch: 2 \t\t\t Iteration 1257 Loss Train: 0.43027186393737793\n",
      "Epoch: 2 \t\t\t Iteration 1258 Loss Train: 0.3563801348209381\n",
      "Epoch: 2 \t\t\t Iteration 1259 Loss Train: 0.5224229097366333\n",
      "Epoch: 2 \t\t\t Iteration 1260 Loss Train: 0.42192426323890686\n",
      "Epoch: 2 \t\t\t Iteration 1261 Loss Train: 0.12217772752046585\n",
      "Epoch: 2 \t\t\t Iteration 1262 Loss Train: 0.3366786241531372\n",
      "Epoch: 2 \t\t\t Iteration 1263 Loss Train: 0.44114089012145996\n",
      "Epoch: 2 \t\t\t Iteration 1264 Loss Train: 0.42647585272789\n",
      "Epoch: 2 \t\t\t Iteration 1265 Loss Train: 0.47295036911964417\n",
      "Epoch: 2 \t\t\t Iteration 1266 Loss Train: 0.548330545425415\n",
      "Epoch: 2 \t\t\t Iteration 1267 Loss Train: 0.42152512073516846\n",
      "Epoch: 2 \t\t\t Iteration 1268 Loss Train: 0.39954203367233276\n",
      "Epoch: 2 \t\t\t Iteration 1269 Loss Train: 0.4018223285675049\n",
      "Epoch: 2 \t\t\t Iteration 1270 Loss Train: 0.47275930643081665\n",
      "Epoch: 2 \t\t\t Iteration 1271 Loss Train: 0.38750940561294556\n",
      "Epoch: 2 \t\t\t Iteration 1272 Loss Train: 0.28146839141845703\n",
      "Epoch: 2 \t\t\t Iteration 1273 Loss Train: 0.40630412101745605\n",
      "Epoch: 2 \t\t\t Iteration 1274 Loss Train: 0.37414684891700745\n",
      "Epoch: 2 \t\t\t Iteration 1275 Loss Train: 0.40251046419143677\n",
      "Epoch: 2 \t\t\t Iteration 1276 Loss Train: 0.42933881282806396\n",
      "Epoch: 2 \t\t\t Iteration 1277 Loss Train: 0.37497150897979736\n",
      "Epoch: 2 \t\t\t Iteration 1278 Loss Train: 0.3385937511920929\n",
      "Epoch: 2 \t\t\t Iteration 1279 Loss Train: 0.3566662073135376\n",
      "Epoch: 2 \t\t\t Iteration 1280 Loss Train: 0.3759053945541382\n",
      "Epoch: 2 \t\t\t Iteration 1281 Loss Train: 0.41584160923957825\n",
      "Epoch: 2 \t\t\t Iteration 1282 Loss Train: 0.2369384467601776\n",
      "Epoch: 2 \t\t\t Iteration 1283 Loss Train: 0.3743244409561157\n",
      "Epoch: 2 \t\t\t Iteration 1284 Loss Train: 0.5862718224525452\n",
      "Epoch: 2 \t\t\t Iteration 1285 Loss Train: 0.3868653178215027\n",
      "Epoch: 2 \t\t\t Iteration 1286 Loss Train: 0.554706335067749\n",
      "Epoch: 2 \t\t\t Iteration 1287 Loss Train: 0.49418383836746216\n",
      "Epoch: 2 \t\t\t Iteration 1288 Loss Train: 0.38281360268592834\n",
      "Epoch: 2 \t\t\t Iteration 1289 Loss Train: 0.30182403326034546\n",
      "Epoch: 2 \t\t\t Iteration 1290 Loss Train: 0.411368727684021\n",
      "Epoch: 2 \t\t\t Iteration 1291 Loss Train: 0.5754790306091309\n",
      "Epoch: 2 \t\t\t Iteration 1292 Loss Train: 0.3952755331993103\n",
      "Epoch: 2 \t\t\t Iteration 1293 Loss Train: 0.4167504608631134\n",
      "Epoch: 2 \t\t\t Iteration 1294 Loss Train: 0.36627697944641113\n",
      "Epoch: 2 \t\t\t Iteration 1295 Loss Train: 0.3951314091682434\n",
      "Epoch: 2 \t\t\t Iteration 1296 Loss Train: 0.3282357454299927\n",
      "Epoch: 2 \t\t\t Iteration 1297 Loss Train: 0.4215109348297119\n",
      "Epoch: 2 \t\t\t Iteration 1298 Loss Train: 0.4446210563182831\n",
      "Epoch: 2 \t\t\t Iteration 1299 Loss Train: 0.7276057600975037\n",
      "Epoch: 2 \t\t\t Iteration 1300 Loss Train: 0.47106897830963135\n",
      "Epoch: 2 \t\t\t Iteration 1301 Loss Train: 0.4639361500740051\n",
      "Epoch: 2 \t\t\t Iteration 1302 Loss Train: 0.5773090124130249\n",
      "Epoch: 2 \t\t\t Iteration 1303 Loss Train: 0.41087663173675537\n",
      "Epoch: 2 \t\t\t Iteration 1304 Loss Train: 0.2910803556442261\n",
      "Epoch: 2 \t\t\t Iteration 1305 Loss Train: 0.32233327627182007\n",
      "Epoch: 2 \t\t\t Iteration 1306 Loss Train: 0.3455486595630646\n",
      "Epoch: 2 \t\t\t Iteration 1307 Loss Train: 0.3981579542160034\n",
      "Epoch: 2 \t\t\t Iteration 1308 Loss Train: 0.3565216660499573\n",
      "Epoch: 2 \t\t\t Iteration 1309 Loss Train: 0.5505391359329224\n",
      "Epoch: 2 \t\t\t Iteration 1310 Loss Train: 0.6945448517799377\n",
      "Epoch: 2 \t\t\t Iteration 1311 Loss Train: 0.35444626212120056\n",
      "Epoch: 2 \t\t\t Iteration 1312 Loss Train: 0.37385088205337524\n",
      "Epoch: 2 \t\t\t Iteration 1313 Loss Train: 0.7733490467071533\n",
      "Epoch: 2 \t\t\t Iteration 1314 Loss Train: 0.3263123035430908\n",
      "Epoch: 2 \t\t\t Iteration 1315 Loss Train: 0.3926598131656647\n",
      "Epoch: 2 \t\t\t Iteration 1316 Loss Train: 0.44618111848831177\n",
      "Epoch: 2 \t\t\t Iteration 1317 Loss Train: 0.35409432649612427\n",
      "Epoch: 2 \t\t\t Iteration 1318 Loss Train: 0.5608737468719482\n",
      "Epoch: 2 \t\t\t Iteration 1319 Loss Train: 0.5452091693878174\n",
      "Epoch: 2 \t\t\t Iteration 1320 Loss Train: 0.333681583404541\n",
      "Epoch: 2 \t\t\t Iteration 1321 Loss Train: 0.41173356771469116\n",
      "Epoch: 2 \t\t\t Iteration 1322 Loss Train: 0.46531930565834045\n",
      "Epoch: 2 \t\t\t Iteration 1323 Loss Train: 0.29401978850364685\n",
      "Epoch: 2 \t\t\t Iteration 1324 Loss Train: 0.4584794044494629\n",
      "Epoch: 2 \t\t\t Iteration 1325 Loss Train: 0.580620527267456\n",
      "Epoch: 2 \t\t\t Iteration 1326 Loss Train: 0.45266860723495483\n",
      "Epoch: 2 \t\t\t Iteration 1327 Loss Train: 0.4599483013153076\n",
      "Epoch: 2 \t\t\t Iteration 1328 Loss Train: 0.4590558111667633\n",
      "Epoch: 2 \t\t\t Iteration 1329 Loss Train: 0.4692996144294739\n",
      "Epoch: 2 \t\t\t Iteration 1330 Loss Train: 0.3920491337776184\n",
      "Epoch: 2 \t\t\t Iteration 1331 Loss Train: 0.4175761044025421\n",
      "Epoch: 2 \t\t\t Iteration 1332 Loss Train: 0.5025818347930908\n",
      "Epoch: 2 \t\t\t Iteration 1333 Loss Train: 0.45893505215644836\n",
      "Epoch: 2 \t\t\t Iteration 1334 Loss Train: 0.40484094619750977\n",
      "Epoch: 2 \t\t\t Iteration 1335 Loss Train: 0.32486245036125183\n",
      "Epoch: 2 \t\t\t Iteration 1336 Loss Train: 0.18083146214485168\n",
      "Epoch: 2 \t\t\t Iteration 1337 Loss Train: 0.7751153707504272\n",
      "Epoch: 2 \t\t\t Iteration 1338 Loss Train: 0.4795414209365845\n",
      "Epoch: 2 \t\t\t Iteration 1339 Loss Train: 0.4202892780303955\n",
      "Epoch: 2 \t\t\t Iteration 1340 Loss Train: 0.569616436958313\n",
      "Epoch: 2 \t\t\t Iteration 1341 Loss Train: 0.44461995363235474\n",
      "Epoch: 2 \t\t\t Iteration 1342 Loss Train: 0.38568949699401855\n",
      "Epoch: 2 \t\t\t Iteration 1343 Loss Train: 0.3424217104911804\n",
      "Epoch: 2 \t\t\t Iteration 1344 Loss Train: 0.1941574066877365\n",
      "Epoch: 2 \t\t\t Iteration 1345 Loss Train: 0.39243224263191223\n",
      "Epoch: 2 \t\t\t Iteration 1346 Loss Train: 0.4730282425880432\n",
      "Epoch: 2 \t\t\t Iteration 1347 Loss Train: 0.4813828468322754\n",
      "Epoch: 2 \t\t\t Iteration 1348 Loss Train: 0.5805448293685913\n",
      "Epoch: 2 \t\t\t Iteration 1349 Loss Train: 0.4689244031906128\n",
      "Epoch: 2 \t\t\t Iteration 1350 Loss Train: 0.27975568175315857\n",
      "Epoch: 2 \t\t\t Iteration 1351 Loss Train: 0.48351678252220154\n",
      "Epoch: 2 \t\t\t Iteration 1352 Loss Train: 0.3251849412918091\n",
      "Epoch: 2 \t\t\t Iteration 1353 Loss Train: 0.43955737352371216\n",
      "Epoch: 2 \t\t\t Iteration 1354 Loss Train: 0.5143570303916931\n",
      "Epoch: 2 \t\t\t Iteration 1355 Loss Train: 0.5053904056549072\n",
      "Epoch: 2 \t\t\t Iteration 1356 Loss Train: 0.4708518385887146\n",
      "Epoch: 2 \t\t\t Iteration 1357 Loss Train: 0.3510860800743103\n",
      "Epoch: 2 \t\t\t Iteration 1358 Loss Train: 0.6151593923568726\n",
      "Epoch: 2 \t\t\t Iteration 1359 Loss Train: 0.45389509201049805\n",
      "Epoch: 2 \t\t\t Iteration 1360 Loss Train: 0.46297702193260193\n",
      "Epoch: 2 \t\t\t Iteration 1361 Loss Train: 0.30956658720970154\n",
      "Epoch: 2 \t\t\t Iteration 1362 Loss Train: 0.5582406520843506\n",
      "Epoch: 2 \t\t\t Iteration 1363 Loss Train: 0.26717090606689453\n",
      "Epoch: 2 \t\t\t Iteration 1364 Loss Train: 0.32438671588897705\n",
      "Epoch: 2 \t\t\t Iteration 1365 Loss Train: 0.2554562985897064\n",
      "Epoch: 2 \t\t\t Iteration 1366 Loss Train: 0.7307237386703491\n",
      "Epoch: 2 \t\t\t Iteration 1367 Loss Train: 0.39792400598526\n",
      "Epoch: 2 \t\t\t Iteration 1368 Loss Train: 0.37702763080596924\n",
      "Epoch: 2 \t\t\t Iteration 1369 Loss Train: 0.40747493505477905\n",
      "Epoch: 2 \t\t\t Iteration 1370 Loss Train: 0.4589901566505432\n",
      "Epoch: 2 \t\t\t Iteration 1371 Loss Train: 0.39585232734680176\n",
      "Epoch: 2 \t\t\t Iteration 1372 Loss Train: 0.4170581102371216\n",
      "Epoch: 2 \t\t\t Iteration 1373 Loss Train: 0.3979887366294861\n",
      "Epoch: 2 \t\t\t Iteration 1374 Loss Train: 0.30092430114746094\n",
      "Epoch: 2 \t\t\t Iteration 1375 Loss Train: 0.2351241111755371\n",
      "Epoch: 2 \t\t\t Iteration 1376 Loss Train: 0.7178065776824951\n",
      "Epoch: 2 \t\t\t Iteration 1377 Loss Train: 0.3822387158870697\n",
      "Epoch: 2 \t\t\t Iteration 1378 Loss Train: 0.3771580159664154\n",
      "Epoch: 2 \t\t\t Iteration 1379 Loss Train: 0.2825606167316437\n",
      "Epoch: 2 \t\t\t Iteration 1380 Loss Train: 0.3294115662574768\n",
      "Epoch: 2 \t\t\t Iteration 1381 Loss Train: 0.48529839515686035\n",
      "Epoch: 2 \t\t\t Iteration 1382 Loss Train: 0.2710244059562683\n",
      "Epoch: 2 \t\t\t Iteration 1383 Loss Train: 0.6886084079742432\n",
      "Epoch: 2 \t\t\t Iteration 1384 Loss Train: 0.3245497941970825\n",
      "Epoch: 2 \t\t\t Iteration 1385 Loss Train: 0.4458700120449066\n",
      "Epoch: 2 \t\t\t Iteration 1386 Loss Train: 0.4156859517097473\n",
      "Epoch: 2 \t\t\t Iteration 1387 Loss Train: 0.6463640928268433\n",
      "Epoch: 2 \t\t\t Iteration 1388 Loss Train: 0.40557771921157837\n",
      "Epoch: 2 \t\t\t Iteration 1389 Loss Train: 0.36531129479408264\n",
      "Epoch: 2 \t\t\t Iteration 1390 Loss Train: 0.29239165782928467\n",
      "Epoch: 2 \t\t\t Iteration 1391 Loss Train: 0.39765843749046326\n",
      "Epoch: 2 \t\t\t Iteration 1392 Loss Train: 0.3897680640220642\n",
      "Epoch: 2 \t\t\t Iteration 1393 Loss Train: 0.3448048532009125\n",
      "Epoch: 2 \t\t\t Iteration 1394 Loss Train: 0.31763792037963867\n",
      "Epoch: 2 \t\t\t Iteration 1395 Loss Train: 0.7716251015663147\n",
      "Epoch: 2 / 5 \t\t\t Training Loss:0.4427809989099861\n",
      "Epoch: 3 \t\t\t Iteration 1 Loss Train: 0.9150760769844055\n",
      "Epoch: 3 \t\t\t Iteration 2 Loss Train: 0.5093109607696533\n",
      "Epoch: 3 \t\t\t Iteration 3 Loss Train: 0.919803261756897\n",
      "Epoch: 3 \t\t\t Iteration 4 Loss Train: 0.4007280766963959\n",
      "Epoch: 3 \t\t\t Iteration 5 Loss Train: 0.5153301954269409\n",
      "Epoch: 3 \t\t\t Iteration 6 Loss Train: 0.46024954319000244\n",
      "Epoch: 3 \t\t\t Iteration 7 Loss Train: 0.34847238659858704\n",
      "Epoch: 3 \t\t\t Iteration 8 Loss Train: 0.22581398487091064\n",
      "Epoch: 3 \t\t\t Iteration 9 Loss Train: 0.5583462715148926\n",
      "Epoch: 3 \t\t\t Iteration 10 Loss Train: 0.33762791752815247\n",
      "Epoch: 3 \t\t\t Iteration 11 Loss Train: 0.37724626064300537\n",
      "Epoch: 3 \t\t\t Iteration 12 Loss Train: 0.17104792594909668\n",
      "Epoch: 3 \t\t\t Iteration 13 Loss Train: 0.5757797360420227\n",
      "Epoch: 3 \t\t\t Iteration 14 Loss Train: 0.41296643018722534\n",
      "Epoch: 3 \t\t\t Iteration 15 Loss Train: 0.38383930921554565\n",
      "Epoch: 3 \t\t\t Iteration 16 Loss Train: 0.26909124851226807\n",
      "Epoch: 3 \t\t\t Iteration 17 Loss Train: 0.23806992173194885\n",
      "Epoch: 3 \t\t\t Iteration 18 Loss Train: 0.3779276907444\n",
      "Epoch: 3 \t\t\t Iteration 19 Loss Train: 0.3079642057418823\n",
      "Epoch: 3 \t\t\t Iteration 20 Loss Train: 0.3919534683227539\n",
      "Epoch: 3 \t\t\t Iteration 21 Loss Train: 0.41480010747909546\n",
      "Epoch: 3 \t\t\t Iteration 22 Loss Train: 0.4204328656196594\n",
      "Epoch: 3 \t\t\t Iteration 23 Loss Train: 0.3179805278778076\n",
      "Epoch: 3 \t\t\t Iteration 24 Loss Train: 0.44167253375053406\n",
      "Epoch: 3 \t\t\t Iteration 25 Loss Train: 0.41089650988578796\n",
      "Epoch: 3 \t\t\t Iteration 26 Loss Train: 0.42741715908050537\n",
      "Epoch: 3 \t\t\t Iteration 27 Loss Train: 0.8717676401138306\n",
      "Epoch: 3 \t\t\t Iteration 28 Loss Train: 0.500885546207428\n",
      "Epoch: 3 \t\t\t Iteration 29 Loss Train: 0.40922290086746216\n",
      "Epoch: 3 \t\t\t Iteration 30 Loss Train: 0.37030234932899475\n",
      "Epoch: 3 \t\t\t Iteration 31 Loss Train: 0.6396641731262207\n",
      "Epoch: 3 \t\t\t Iteration 32 Loss Train: 0.38381320238113403\n",
      "Epoch: 3 \t\t\t Iteration 33 Loss Train: 0.41836249828338623\n",
      "Epoch: 3 \t\t\t Iteration 34 Loss Train: 0.3973858058452606\n",
      "Epoch: 3 \t\t\t Iteration 35 Loss Train: 0.4788164794445038\n",
      "Epoch: 3 \t\t\t Iteration 36 Loss Train: 0.4760504364967346\n",
      "Epoch: 3 \t\t\t Iteration 37 Loss Train: 0.37920722365379333\n",
      "Epoch: 3 \t\t\t Iteration 38 Loss Train: 0.47478392720222473\n",
      "Epoch: 3 \t\t\t Iteration 39 Loss Train: 0.46973204612731934\n",
      "Epoch: 3 \t\t\t Iteration 40 Loss Train: 0.38284027576446533\n",
      "Epoch: 3 \t\t\t Iteration 41 Loss Train: 0.35993584990501404\n",
      "Epoch: 3 \t\t\t Iteration 42 Loss Train: 0.4002781808376312\n",
      "Epoch: 3 \t\t\t Iteration 43 Loss Train: 0.5095580220222473\n",
      "Epoch: 3 \t\t\t Iteration 44 Loss Train: 0.4178958535194397\n",
      "Epoch: 3 \t\t\t Iteration 45 Loss Train: 0.5160119533538818\n",
      "Epoch: 3 \t\t\t Iteration 46 Loss Train: 0.36023280024528503\n",
      "Epoch: 3 \t\t\t Iteration 47 Loss Train: 0.28209102153778076\n",
      "Epoch: 3 \t\t\t Iteration 48 Loss Train: 0.6742892265319824\n",
      "Epoch: 3 \t\t\t Iteration 49 Loss Train: 0.4163592755794525\n",
      "Epoch: 3 \t\t\t Iteration 50 Loss Train: 0.5034435391426086\n",
      "Epoch: 3 \t\t\t Iteration 51 Loss Train: 0.4224960505962372\n",
      "Epoch: 3 \t\t\t Iteration 52 Loss Train: 0.3636298179626465\n",
      "Epoch: 3 \t\t\t Iteration 53 Loss Train: 0.4110960066318512\n",
      "Epoch: 3 \t\t\t Iteration 54 Loss Train: 0.5713284015655518\n",
      "Epoch: 3 \t\t\t Iteration 55 Loss Train: 0.348479688167572\n",
      "Epoch: 3 \t\t\t Iteration 56 Loss Train: 0.2782909572124481\n",
      "Epoch: 3 \t\t\t Iteration 57 Loss Train: 0.49411314725875854\n",
      "Epoch: 3 \t\t\t Iteration 58 Loss Train: 0.43686753511428833\n",
      "Epoch: 3 \t\t\t Iteration 59 Loss Train: 0.28965291380882263\n",
      "Epoch: 3 \t\t\t Iteration 60 Loss Train: 0.4006105661392212\n",
      "Epoch: 3 \t\t\t Iteration 61 Loss Train: 0.30185896158218384\n",
      "Epoch: 3 \t\t\t Iteration 62 Loss Train: 0.41667842864990234\n",
      "Epoch: 3 \t\t\t Iteration 63 Loss Train: 0.3765547573566437\n",
      "Epoch: 3 \t\t\t Iteration 64 Loss Train: 0.6240677833557129\n",
      "Epoch: 3 \t\t\t Iteration 65 Loss Train: 0.44027119874954224\n",
      "Epoch: 3 \t\t\t Iteration 66 Loss Train: 0.41254061460494995\n",
      "Epoch: 3 \t\t\t Iteration 67 Loss Train: 0.6236122846603394\n",
      "Epoch: 3 \t\t\t Iteration 68 Loss Train: 0.4759317636489868\n",
      "Epoch: 3 \t\t\t Iteration 69 Loss Train: 0.5751808881759644\n",
      "Epoch: 3 \t\t\t Iteration 70 Loss Train: 0.34120726585388184\n",
      "Epoch: 3 \t\t\t Iteration 71 Loss Train: 0.5777148008346558\n",
      "Epoch: 3 \t\t\t Iteration 72 Loss Train: 0.4585307836532593\n",
      "Epoch: 3 \t\t\t Iteration 73 Loss Train: 0.5031570196151733\n",
      "Epoch: 3 \t\t\t Iteration 74 Loss Train: 0.40069809556007385\n",
      "Epoch: 3 \t\t\t Iteration 75 Loss Train: 0.3838735818862915\n",
      "Epoch: 3 \t\t\t Iteration 76 Loss Train: 0.41021928191185\n",
      "Epoch: 3 \t\t\t Iteration 77 Loss Train: 0.5738376379013062\n",
      "Epoch: 3 \t\t\t Iteration 78 Loss Train: 0.46855518221855164\n",
      "Epoch: 3 \t\t\t Iteration 79 Loss Train: 0.3637852668762207\n",
      "Epoch: 3 \t\t\t Iteration 80 Loss Train: 0.3510183095932007\n",
      "Epoch: 3 \t\t\t Iteration 81 Loss Train: 0.6170672178268433\n",
      "Epoch: 3 \t\t\t Iteration 82 Loss Train: 0.5272819995880127\n",
      "Epoch: 3 \t\t\t Iteration 83 Loss Train: 0.43767356872558594\n",
      "Epoch: 3 \t\t\t Iteration 84 Loss Train: 0.31387609243392944\n",
      "Epoch: 3 \t\t\t Iteration 85 Loss Train: 0.19044259190559387\n",
      "Epoch: 3 \t\t\t Iteration 86 Loss Train: 0.3926318883895874\n",
      "Epoch: 3 \t\t\t Iteration 87 Loss Train: 0.5010225176811218\n",
      "Epoch: 3 \t\t\t Iteration 88 Loss Train: 0.4230136275291443\n",
      "Epoch: 3 \t\t\t Iteration 89 Loss Train: 0.41919127106666565\n",
      "Epoch: 3 \t\t\t Iteration 90 Loss Train: 0.443665087223053\n",
      "Epoch: 3 \t\t\t Iteration 91 Loss Train: 0.3814944922924042\n",
      "Epoch: 3 \t\t\t Iteration 92 Loss Train: 0.44534575939178467\n",
      "Epoch: 3 \t\t\t Iteration 93 Loss Train: 0.3486172556877136\n",
      "Epoch: 3 \t\t\t Iteration 94 Loss Train: 0.5479919910430908\n",
      "Epoch: 3 \t\t\t Iteration 95 Loss Train: 0.45846813917160034\n",
      "Epoch: 3 \t\t\t Iteration 96 Loss Train: 0.5380924344062805\n",
      "Epoch: 3 \t\t\t Iteration 97 Loss Train: 0.5091322064399719\n",
      "Epoch: 3 \t\t\t Iteration 98 Loss Train: 0.5742719173431396\n",
      "Epoch: 3 \t\t\t Iteration 99 Loss Train: 0.5925446152687073\n",
      "Epoch: 3 \t\t\t Iteration 100 Loss Train: 0.37835267186164856\n",
      "Epoch: 3 \t\t\t Iteration 101 Loss Train: 0.43952229619026184\n",
      "Epoch: 3 \t\t\t Iteration 102 Loss Train: 0.4040693938732147\n",
      "Epoch: 3 \t\t\t Iteration 103 Loss Train: 0.3502849340438843\n",
      "Epoch: 3 \t\t\t Iteration 104 Loss Train: 0.40500783920288086\n",
      "Epoch: 3 \t\t\t Iteration 105 Loss Train: 0.48751986026763916\n",
      "Epoch: 3 \t\t\t Iteration 106 Loss Train: 0.442100465297699\n",
      "Epoch: 3 \t\t\t Iteration 107 Loss Train: 0.6734466552734375\n",
      "Epoch: 3 \t\t\t Iteration 108 Loss Train: 0.525969386100769\n",
      "Epoch: 3 \t\t\t Iteration 109 Loss Train: 0.38622623682022095\n",
      "Epoch: 3 \t\t\t Iteration 110 Loss Train: 0.27021288871765137\n",
      "Epoch: 3 \t\t\t Iteration 111 Loss Train: 0.49099797010421753\n",
      "Epoch: 3 \t\t\t Iteration 112 Loss Train: 0.45720070600509644\n",
      "Epoch: 3 \t\t\t Iteration 113 Loss Train: 0.477190226316452\n",
      "Epoch: 3 \t\t\t Iteration 114 Loss Train: 0.4249478578567505\n",
      "Epoch: 3 \t\t\t Iteration 115 Loss Train: 0.49567675590515137\n",
      "Epoch: 3 \t\t\t Iteration 116 Loss Train: 0.47660917043685913\n",
      "Epoch: 3 \t\t\t Iteration 117 Loss Train: 0.3686675429344177\n",
      "Epoch: 3 \t\t\t Iteration 118 Loss Train: 0.2090681791305542\n",
      "Epoch: 3 \t\t\t Iteration 119 Loss Train: 0.36836546659469604\n",
      "Epoch: 3 \t\t\t Iteration 120 Loss Train: 0.26033520698547363\n",
      "Epoch: 3 \t\t\t Iteration 121 Loss Train: 0.42720475792884827\n",
      "Epoch: 3 \t\t\t Iteration 122 Loss Train: 0.4416798949241638\n",
      "Epoch: 3 \t\t\t Iteration 123 Loss Train: 0.5065822005271912\n",
      "Epoch: 3 \t\t\t Iteration 124 Loss Train: 0.32802388072013855\n",
      "Epoch: 3 \t\t\t Iteration 125 Loss Train: 0.3524109125137329\n",
      "Epoch: 3 \t\t\t Iteration 126 Loss Train: 0.22410278022289276\n",
      "Epoch: 3 \t\t\t Iteration 127 Loss Train: 0.5588164329528809\n",
      "Epoch: 3 \t\t\t Iteration 128 Loss Train: 0.5118478536605835\n",
      "Epoch: 3 \t\t\t Iteration 129 Loss Train: 0.4489171802997589\n",
      "Epoch: 3 \t\t\t Iteration 130 Loss Train: 0.2703646719455719\n",
      "Epoch: 3 \t\t\t Iteration 131 Loss Train: 0.48852843046188354\n",
      "Epoch: 3 \t\t\t Iteration 132 Loss Train: 0.3751094341278076\n",
      "Epoch: 3 \t\t\t Iteration 133 Loss Train: 0.29632723331451416\n",
      "Epoch: 3 \t\t\t Iteration 134 Loss Train: 0.5763240456581116\n",
      "Epoch: 3 \t\t\t Iteration 135 Loss Train: 0.42322736978530884\n",
      "Epoch: 3 \t\t\t Iteration 136 Loss Train: 0.6098781824111938\n",
      "Epoch: 3 \t\t\t Iteration 137 Loss Train: 0.27105653285980225\n",
      "Epoch: 3 \t\t\t Iteration 138 Loss Train: 0.48819899559020996\n",
      "Epoch: 3 \t\t\t Iteration 139 Loss Train: 0.39029982686042786\n",
      "Epoch: 3 \t\t\t Iteration 140 Loss Train: 0.2852906882762909\n",
      "Epoch: 3 \t\t\t Iteration 141 Loss Train: 0.5627608895301819\n",
      "Epoch: 3 \t\t\t Iteration 142 Loss Train: 0.4435902237892151\n",
      "Epoch: 3 \t\t\t Iteration 143 Loss Train: 0.41556477546691895\n",
      "Epoch: 3 \t\t\t Iteration 144 Loss Train: 0.41034722328186035\n",
      "Epoch: 3 \t\t\t Iteration 145 Loss Train: 0.43484044075012207\n",
      "Epoch: 3 \t\t\t Iteration 146 Loss Train: 0.35631710290908813\n",
      "Epoch: 3 \t\t\t Iteration 147 Loss Train: 0.32644808292388916\n",
      "Epoch: 3 \t\t\t Iteration 148 Loss Train: 0.4123656749725342\n",
      "Epoch: 3 \t\t\t Iteration 149 Loss Train: 0.4977061152458191\n",
      "Epoch: 3 \t\t\t Iteration 150 Loss Train: 0.3921509385108948\n",
      "Epoch: 3 \t\t\t Iteration 151 Loss Train: 0.4008065164089203\n",
      "Epoch: 3 \t\t\t Iteration 152 Loss Train: 0.42821213603019714\n",
      "Epoch: 3 \t\t\t Iteration 153 Loss Train: 0.4249303340911865\n",
      "Epoch: 3 \t\t\t Iteration 154 Loss Train: 0.3747084438800812\n",
      "Epoch: 3 \t\t\t Iteration 155 Loss Train: 0.37213531136512756\n",
      "Epoch: 3 \t\t\t Iteration 156 Loss Train: 0.42582449316978455\n",
      "Epoch: 3 \t\t\t Iteration 157 Loss Train: 0.3909068703651428\n",
      "Epoch: 3 \t\t\t Iteration 158 Loss Train: 0.5052075386047363\n",
      "Epoch: 3 \t\t\t Iteration 159 Loss Train: 0.3452261686325073\n",
      "Epoch: 3 \t\t\t Iteration 160 Loss Train: 0.2178536057472229\n",
      "Epoch: 3 \t\t\t Iteration 161 Loss Train: 0.9179500341415405\n",
      "Epoch: 3 \t\t\t Iteration 162 Loss Train: 0.4931814670562744\n",
      "Epoch: 3 \t\t\t Iteration 163 Loss Train: 0.4184980094432831\n",
      "Epoch: 3 \t\t\t Iteration 164 Loss Train: 0.4090009033679962\n",
      "Epoch: 3 \t\t\t Iteration 165 Loss Train: 0.4736940860748291\n",
      "Epoch: 3 \t\t\t Iteration 166 Loss Train: 0.4119133949279785\n",
      "Epoch: 3 \t\t\t Iteration 167 Loss Train: 0.5010524988174438\n",
      "Epoch: 3 \t\t\t Iteration 168 Loss Train: 0.322011798620224\n",
      "Epoch: 3 \t\t\t Iteration 169 Loss Train: 0.39341264963150024\n",
      "Epoch: 3 \t\t\t Iteration 170 Loss Train: 0.5299800038337708\n",
      "Epoch: 3 \t\t\t Iteration 171 Loss Train: 0.4208744466304779\n",
      "Epoch: 3 \t\t\t Iteration 172 Loss Train: 0.4534282684326172\n",
      "Epoch: 3 \t\t\t Iteration 173 Loss Train: 0.47710728645324707\n",
      "Epoch: 3 \t\t\t Iteration 174 Loss Train: 0.4398670196533203\n",
      "Epoch: 3 \t\t\t Iteration 175 Loss Train: 0.4600764513015747\n",
      "Epoch: 3 \t\t\t Iteration 176 Loss Train: 0.5463595390319824\n",
      "Epoch: 3 \t\t\t Iteration 177 Loss Train: 0.44040191173553467\n",
      "Epoch: 3 \t\t\t Iteration 178 Loss Train: 0.4784056544303894\n",
      "Epoch: 3 \t\t\t Iteration 179 Loss Train: 0.3275206685066223\n",
      "Epoch: 3 \t\t\t Iteration 180 Loss Train: 0.30837976932525635\n",
      "Epoch: 3 \t\t\t Iteration 181 Loss Train: 0.8017231822013855\n",
      "Epoch: 3 \t\t\t Iteration 182 Loss Train: 0.45127254724502563\n",
      "Epoch: 3 \t\t\t Iteration 183 Loss Train: 0.351751446723938\n",
      "Epoch: 3 \t\t\t Iteration 184 Loss Train: 0.46495217084884644\n",
      "Epoch: 3 \t\t\t Iteration 185 Loss Train: 0.5459612607955933\n",
      "Epoch: 3 \t\t\t Iteration 186 Loss Train: 0.4365200400352478\n",
      "Epoch: 3 \t\t\t Iteration 187 Loss Train: 0.44969719648361206\n",
      "Epoch: 3 \t\t\t Iteration 188 Loss Train: 0.3824019134044647\n",
      "Epoch: 3 \t\t\t Iteration 189 Loss Train: 0.34170103073120117\n",
      "Epoch: 3 \t\t\t Iteration 190 Loss Train: 0.5815408229827881\n",
      "Epoch: 3 \t\t\t Iteration 191 Loss Train: 0.4878878891468048\n",
      "Epoch: 3 \t\t\t Iteration 192 Loss Train: 0.35868656635284424\n",
      "Epoch: 3 \t\t\t Iteration 193 Loss Train: 0.4832760691642761\n",
      "Epoch: 3 \t\t\t Iteration 194 Loss Train: 0.3613908886909485\n",
      "Epoch: 3 \t\t\t Iteration 195 Loss Train: 0.42114704847335815\n",
      "Epoch: 3 \t\t\t Iteration 196 Loss Train: 0.36601555347442627\n",
      "Epoch: 3 \t\t\t Iteration 197 Loss Train: 0.2988722324371338\n",
      "Epoch: 3 \t\t\t Iteration 198 Loss Train: 0.5119481086730957\n",
      "Epoch: 3 \t\t\t Iteration 199 Loss Train: 0.4141198992729187\n",
      "Epoch: 3 \t\t\t Iteration 200 Loss Train: 0.5880139470100403\n",
      "Epoch: 3 \t\t\t Iteration 201 Loss Train: 0.6079563498497009\n",
      "Epoch: 3 \t\t\t Iteration 202 Loss Train: 0.45620930194854736\n",
      "Epoch: 3 \t\t\t Iteration 203 Loss Train: 0.5511906743049622\n",
      "Epoch: 3 \t\t\t Iteration 204 Loss Train: 0.5339372158050537\n",
      "Epoch: 3 \t\t\t Iteration 205 Loss Train: 0.40961045026779175\n",
      "Epoch: 3 \t\t\t Iteration 206 Loss Train: 0.3241211771965027\n",
      "Epoch: 3 \t\t\t Iteration 207 Loss Train: 0.5011075735092163\n",
      "Epoch: 3 \t\t\t Iteration 208 Loss Train: 0.5522639751434326\n",
      "Epoch: 3 \t\t\t Iteration 209 Loss Train: 0.3094356656074524\n",
      "Epoch: 3 \t\t\t Iteration 210 Loss Train: 0.2768632769584656\n",
      "Epoch: 3 \t\t\t Iteration 211 Loss Train: 0.43787866830825806\n",
      "Epoch: 3 \t\t\t Iteration 212 Loss Train: 0.4176870286464691\n",
      "Epoch: 3 \t\t\t Iteration 213 Loss Train: 0.21909889578819275\n",
      "Epoch: 3 \t\t\t Iteration 214 Loss Train: 0.3745351731777191\n",
      "Epoch: 3 \t\t\t Iteration 215 Loss Train: 0.37221941351890564\n",
      "Epoch: 3 \t\t\t Iteration 216 Loss Train: 0.37018364667892456\n",
      "Epoch: 3 \t\t\t Iteration 217 Loss Train: 0.5443568229675293\n",
      "Epoch: 3 \t\t\t Iteration 218 Loss Train: 0.49144187569618225\n",
      "Epoch: 3 \t\t\t Iteration 219 Loss Train: 0.4051769971847534\n",
      "Epoch: 3 \t\t\t Iteration 220 Loss Train: 0.4216175675392151\n",
      "Epoch: 3 \t\t\t Iteration 221 Loss Train: 0.2704867124557495\n",
      "Epoch: 3 \t\t\t Iteration 222 Loss Train: 0.6557326316833496\n",
      "Epoch: 3 \t\t\t Iteration 223 Loss Train: 0.4444349706172943\n",
      "Epoch: 3 \t\t\t Iteration 224 Loss Train: 0.37499669194221497\n",
      "Epoch: 3 \t\t\t Iteration 225 Loss Train: 0.3670140504837036\n",
      "Epoch: 3 \t\t\t Iteration 226 Loss Train: 0.2573700249195099\n",
      "Epoch: 3 \t\t\t Iteration 227 Loss Train: 0.2776877284049988\n",
      "Epoch: 3 \t\t\t Iteration 228 Loss Train: 0.49614766240119934\n",
      "Epoch: 3 \t\t\t Iteration 229 Loss Train: 0.5325403809547424\n",
      "Epoch: 3 \t\t\t Iteration 230 Loss Train: 0.3868919610977173\n",
      "Epoch: 3 \t\t\t Iteration 231 Loss Train: 0.4376840591430664\n",
      "Epoch: 3 \t\t\t Iteration 232 Loss Train: 0.5336198806762695\n",
      "Epoch: 3 \t\t\t Iteration 233 Loss Train: 0.46488091349601746\n",
      "Epoch: 3 \t\t\t Iteration 234 Loss Train: 0.4878199100494385\n",
      "Epoch: 3 \t\t\t Iteration 235 Loss Train: 0.5018358826637268\n",
      "Epoch: 3 \t\t\t Iteration 236 Loss Train: 0.5299288034439087\n",
      "Epoch: 3 \t\t\t Iteration 237 Loss Train: 0.5060251951217651\n",
      "Epoch: 3 \t\t\t Iteration 238 Loss Train: 0.3018020689487457\n",
      "Epoch: 3 \t\t\t Iteration 239 Loss Train: 0.1152443215250969\n",
      "Epoch: 3 \t\t\t Iteration 240 Loss Train: 0.516459047794342\n",
      "Epoch: 3 \t\t\t Iteration 241 Loss Train: 0.23059360682964325\n",
      "Epoch: 3 \t\t\t Iteration 242 Loss Train: 0.5697675347328186\n",
      "Epoch: 3 \t\t\t Iteration 243 Loss Train: 0.43051010370254517\n",
      "Epoch: 3 \t\t\t Iteration 244 Loss Train: 0.522969663143158\n",
      "Epoch: 3 \t\t\t Iteration 245 Loss Train: 0.4368952512741089\n",
      "Epoch: 3 \t\t\t Iteration 246 Loss Train: 0.4973689317703247\n",
      "Epoch: 3 \t\t\t Iteration 247 Loss Train: 0.4480246305465698\n",
      "Epoch: 3 \t\t\t Iteration 248 Loss Train: 0.3984798789024353\n",
      "Epoch: 3 \t\t\t Iteration 249 Loss Train: 0.2894681692123413\n",
      "Epoch: 3 \t\t\t Iteration 250 Loss Train: 0.4173417091369629\n",
      "Epoch: 3 \t\t\t Iteration 251 Loss Train: 0.4945129156112671\n",
      "Epoch: 3 \t\t\t Iteration 252 Loss Train: 0.450088232755661\n",
      "Epoch: 3 \t\t\t Iteration 253 Loss Train: 0.34961631894111633\n",
      "Epoch: 3 \t\t\t Iteration 254 Loss Train: 0.44375479221343994\n",
      "Epoch: 3 \t\t\t Iteration 255 Loss Train: 0.3357242941856384\n",
      "Epoch: 3 \t\t\t Iteration 256 Loss Train: 0.4083213806152344\n",
      "Epoch: 3 \t\t\t Iteration 257 Loss Train: 0.33110591769218445\n",
      "Epoch: 3 \t\t\t Iteration 258 Loss Train: 0.5669440031051636\n",
      "Epoch: 3 \t\t\t Iteration 259 Loss Train: 0.34468525648117065\n",
      "Epoch: 3 \t\t\t Iteration 260 Loss Train: 0.40856093168258667\n",
      "Epoch: 3 \t\t\t Iteration 261 Loss Train: 0.4605189561843872\n",
      "Epoch: 3 \t\t\t Iteration 262 Loss Train: 0.4709165096282959\n",
      "Epoch: 3 \t\t\t Iteration 263 Loss Train: 0.31034055352211\n",
      "Epoch: 3 \t\t\t Iteration 264 Loss Train: 0.5318791270256042\n",
      "Epoch: 3 \t\t\t Iteration 265 Loss Train: 0.4566211998462677\n",
      "Epoch: 3 \t\t\t Iteration 266 Loss Train: 0.3078370988368988\n",
      "Epoch: 3 \t\t\t Iteration 267 Loss Train: 0.6328105330467224\n",
      "Epoch: 3 \t\t\t Iteration 268 Loss Train: 0.46796250343322754\n",
      "Epoch: 3 \t\t\t Iteration 269 Loss Train: 0.39733701944351196\n",
      "Epoch: 3 \t\t\t Iteration 270 Loss Train: 0.4432870149612427\n",
      "Epoch: 3 \t\t\t Iteration 271 Loss Train: 0.4919220209121704\n",
      "Epoch: 3 \t\t\t Iteration 272 Loss Train: 0.3677072525024414\n",
      "Epoch: 3 \t\t\t Iteration 273 Loss Train: 0.39219510555267334\n",
      "Epoch: 3 \t\t\t Iteration 274 Loss Train: 0.4955768883228302\n",
      "Epoch: 3 \t\t\t Iteration 275 Loss Train: 0.38123056292533875\n",
      "Epoch: 3 \t\t\t Iteration 276 Loss Train: 0.4659675359725952\n",
      "Epoch: 3 \t\t\t Iteration 277 Loss Train: 0.4541173279285431\n",
      "Epoch: 3 \t\t\t Iteration 278 Loss Train: 0.5716733932495117\n",
      "Epoch: 3 \t\t\t Iteration 279 Loss Train: 0.5130793452262878\n",
      "Epoch: 3 \t\t\t Iteration 280 Loss Train: 0.4690621495246887\n",
      "Epoch: 3 \t\t\t Iteration 281 Loss Train: 0.44977033138275146\n",
      "Epoch: 3 \t\t\t Iteration 282 Loss Train: 0.4201148450374603\n",
      "Epoch: 3 \t\t\t Iteration 283 Loss Train: 0.5598077774047852\n",
      "Epoch: 3 \t\t\t Iteration 284 Loss Train: 0.47141432762145996\n",
      "Epoch: 3 \t\t\t Iteration 285 Loss Train: 0.4230078160762787\n",
      "Epoch: 3 \t\t\t Iteration 286 Loss Train: 0.4295157492160797\n",
      "Epoch: 3 \t\t\t Iteration 287 Loss Train: 0.2874360680580139\n",
      "Epoch: 3 \t\t\t Iteration 288 Loss Train: 0.3961806297302246\n",
      "Epoch: 3 \t\t\t Iteration 289 Loss Train: 0.3024403154850006\n",
      "Epoch: 3 \t\t\t Iteration 290 Loss Train: 0.29099011421203613\n",
      "Epoch: 3 \t\t\t Iteration 291 Loss Train: 0.4578421115875244\n",
      "Epoch: 3 \t\t\t Iteration 292 Loss Train: 0.4545537531375885\n",
      "Epoch: 3 \t\t\t Iteration 293 Loss Train: 0.3808865547180176\n",
      "Epoch: 3 \t\t\t Iteration 294 Loss Train: 0.4751022458076477\n",
      "Epoch: 3 \t\t\t Iteration 295 Loss Train: 0.4739568531513214\n",
      "Epoch: 3 \t\t\t Iteration 296 Loss Train: 0.4001394808292389\n",
      "Epoch: 3 \t\t\t Iteration 297 Loss Train: 0.4843967854976654\n",
      "Epoch: 3 \t\t\t Iteration 298 Loss Train: 0.4344373643398285\n",
      "Epoch: 3 \t\t\t Iteration 299 Loss Train: 0.5493824481964111\n",
      "Epoch: 3 \t\t\t Iteration 300 Loss Train: 0.3798738121986389\n",
      "Epoch: 3 \t\t\t Iteration 301 Loss Train: 0.3026723265647888\n",
      "Epoch: 3 \t\t\t Iteration 302 Loss Train: 0.387238085269928\n",
      "Epoch: 3 \t\t\t Iteration 303 Loss Train: 0.3308311104774475\n",
      "Epoch: 3 \t\t\t Iteration 304 Loss Train: 0.4114620089530945\n",
      "Epoch: 3 \t\t\t Iteration 305 Loss Train: 0.33137673139572144\n",
      "Epoch: 3 \t\t\t Iteration 306 Loss Train: 0.6318882703781128\n",
      "Epoch: 3 \t\t\t Iteration 307 Loss Train: 0.45747947692871094\n",
      "Epoch: 3 \t\t\t Iteration 308 Loss Train: 0.40305691957473755\n",
      "Epoch: 3 \t\t\t Iteration 309 Loss Train: 0.45621272921562195\n",
      "Epoch: 3 \t\t\t Iteration 310 Loss Train: 0.41367554664611816\n",
      "Epoch: 3 \t\t\t Iteration 311 Loss Train: 0.30131831765174866\n",
      "Epoch: 3 \t\t\t Iteration 312 Loss Train: 0.6158121228218079\n",
      "Epoch: 3 \t\t\t Iteration 313 Loss Train: 0.44013434648513794\n",
      "Epoch: 3 \t\t\t Iteration 314 Loss Train: 0.5478160977363586\n",
      "Epoch: 3 \t\t\t Iteration 315 Loss Train: 0.3206585645675659\n",
      "Epoch: 3 \t\t\t Iteration 316 Loss Train: 0.43953514099121094\n",
      "Epoch: 3 \t\t\t Iteration 317 Loss Train: 0.272316575050354\n",
      "Epoch: 3 \t\t\t Iteration 318 Loss Train: 0.3632616400718689\n",
      "Epoch: 3 \t\t\t Iteration 319 Loss Train: 0.4697692394256592\n",
      "Epoch: 3 \t\t\t Iteration 320 Loss Train: 0.3485924005508423\n",
      "Epoch: 3 \t\t\t Iteration 321 Loss Train: 0.336986243724823\n",
      "Epoch: 3 \t\t\t Iteration 322 Loss Train: 0.30407461524009705\n",
      "Epoch: 3 \t\t\t Iteration 323 Loss Train: 0.6953447461128235\n",
      "Epoch: 3 \t\t\t Iteration 324 Loss Train: 0.49687623977661133\n",
      "Epoch: 3 \t\t\t Iteration 325 Loss Train: 0.4586794376373291\n",
      "Epoch: 3 \t\t\t Iteration 326 Loss Train: 0.43761664628982544\n",
      "Epoch: 3 \t\t\t Iteration 327 Loss Train: 0.4073278605937958\n",
      "Epoch: 3 \t\t\t Iteration 328 Loss Train: 0.4096986651420593\n",
      "Epoch: 3 \t\t\t Iteration 329 Loss Train: 0.3617519736289978\n",
      "Epoch: 3 \t\t\t Iteration 330 Loss Train: 0.29071956872940063\n",
      "Epoch: 3 \t\t\t Iteration 331 Loss Train: 0.3127236068248749\n",
      "Epoch: 3 \t\t\t Iteration 332 Loss Train: 0.5730500221252441\n",
      "Epoch: 3 \t\t\t Iteration 333 Loss Train: 0.4431900084018707\n",
      "Epoch: 3 \t\t\t Iteration 334 Loss Train: 0.49891412258148193\n",
      "Epoch: 3 \t\t\t Iteration 335 Loss Train: 0.35925430059432983\n",
      "Epoch: 3 \t\t\t Iteration 336 Loss Train: 0.37404561042785645\n",
      "Epoch: 3 \t\t\t Iteration 337 Loss Train: 0.5695772171020508\n",
      "Epoch: 3 \t\t\t Iteration 338 Loss Train: 0.4531172513961792\n",
      "Epoch: 3 \t\t\t Iteration 339 Loss Train: 0.5974478721618652\n",
      "Epoch: 3 \t\t\t Iteration 340 Loss Train: 0.5254700779914856\n",
      "Epoch: 3 \t\t\t Iteration 341 Loss Train: 0.5754783153533936\n",
      "Epoch: 3 \t\t\t Iteration 342 Loss Train: 0.40936020016670227\n",
      "Epoch: 3 \t\t\t Iteration 343 Loss Train: 0.3418981432914734\n",
      "Epoch: 3 \t\t\t Iteration 344 Loss Train: 0.5132473111152649\n",
      "Epoch: 3 \t\t\t Iteration 345 Loss Train: 0.5766714811325073\n",
      "Epoch: 3 \t\t\t Iteration 346 Loss Train: 0.5663362145423889\n",
      "Epoch: 3 \t\t\t Iteration 347 Loss Train: 0.36124569177627563\n",
      "Epoch: 3 \t\t\t Iteration 348 Loss Train: 0.3833622932434082\n",
      "Epoch: 3 \t\t\t Iteration 349 Loss Train: 0.39347130060195923\n",
      "Epoch: 3 \t\t\t Iteration 350 Loss Train: 0.4144514799118042\n",
      "Epoch: 3 \t\t\t Iteration 351 Loss Train: 0.3965539336204529\n",
      "Epoch: 3 \t\t\t Iteration 352 Loss Train: 0.35167115926742554\n",
      "Epoch: 3 \t\t\t Iteration 353 Loss Train: 0.29565709829330444\n",
      "Epoch: 3 \t\t\t Iteration 354 Loss Train: 0.44382593035697937\n",
      "Epoch: 3 \t\t\t Iteration 355 Loss Train: 0.4178471565246582\n",
      "Epoch: 3 \t\t\t Iteration 356 Loss Train: 0.3702671527862549\n",
      "Epoch: 3 \t\t\t Iteration 357 Loss Train: 0.4521544575691223\n",
      "Epoch: 3 \t\t\t Iteration 358 Loss Train: 0.43336549401283264\n",
      "Epoch: 3 \t\t\t Iteration 359 Loss Train: 0.37007200717926025\n",
      "Epoch: 3 \t\t\t Iteration 360 Loss Train: 0.44341403245925903\n",
      "Epoch: 3 \t\t\t Iteration 361 Loss Train: 0.4956094026565552\n",
      "Epoch: 3 \t\t\t Iteration 362 Loss Train: 0.37961438298225403\n",
      "Epoch: 3 \t\t\t Iteration 363 Loss Train: 0.5399873852729797\n",
      "Epoch: 3 \t\t\t Iteration 364 Loss Train: 0.3534987270832062\n",
      "Epoch: 3 \t\t\t Iteration 365 Loss Train: 0.2531740963459015\n",
      "Epoch: 3 \t\t\t Iteration 366 Loss Train: 0.3240339756011963\n",
      "Epoch: 3 \t\t\t Iteration 367 Loss Train: 0.3615269958972931\n",
      "Epoch: 3 \t\t\t Iteration 368 Loss Train: 0.3720754384994507\n",
      "Epoch: 3 \t\t\t Iteration 369 Loss Train: 0.399737685918808\n",
      "Epoch: 3 \t\t\t Iteration 370 Loss Train: 0.5167896747589111\n",
      "Epoch: 3 \t\t\t Iteration 371 Loss Train: 0.40987634658813477\n",
      "Epoch: 3 \t\t\t Iteration 372 Loss Train: 0.3740151524543762\n",
      "Epoch: 3 \t\t\t Iteration 373 Loss Train: 0.3281143307685852\n",
      "Epoch: 3 \t\t\t Iteration 374 Loss Train: 0.42111659049987793\n",
      "Epoch: 3 \t\t\t Iteration 375 Loss Train: 0.3702397346496582\n",
      "Epoch: 3 \t\t\t Iteration 376 Loss Train: 0.3143939673900604\n",
      "Epoch: 3 \t\t\t Iteration 377 Loss Train: 0.44816461205482483\n",
      "Epoch: 3 \t\t\t Iteration 378 Loss Train: 0.3757345676422119\n",
      "Epoch: 3 \t\t\t Iteration 379 Loss Train: 0.48830145597457886\n",
      "Epoch: 3 \t\t\t Iteration 380 Loss Train: 0.3590421676635742\n",
      "Epoch: 3 \t\t\t Iteration 381 Loss Train: 0.3938683271408081\n",
      "Epoch: 3 \t\t\t Iteration 382 Loss Train: 0.5212074518203735\n",
      "Epoch: 3 \t\t\t Iteration 383 Loss Train: 0.4239928424358368\n",
      "Epoch: 3 \t\t\t Iteration 384 Loss Train: 0.4344390630722046\n",
      "Epoch: 3 \t\t\t Iteration 385 Loss Train: 0.6154031157493591\n",
      "Epoch: 3 \t\t\t Iteration 386 Loss Train: 0.38914233446121216\n",
      "Epoch: 3 \t\t\t Iteration 387 Loss Train: 0.44220203161239624\n",
      "Epoch: 3 \t\t\t Iteration 388 Loss Train: 0.4166676998138428\n",
      "Epoch: 3 \t\t\t Iteration 389 Loss Train: 0.3956775665283203\n",
      "Epoch: 3 \t\t\t Iteration 390 Loss Train: 0.5471527576446533\n",
      "Epoch: 3 \t\t\t Iteration 391 Loss Train: 0.3549736738204956\n",
      "Epoch: 3 \t\t\t Iteration 392 Loss Train: 0.3258194923400879\n",
      "Epoch: 3 \t\t\t Iteration 393 Loss Train: 0.4212877154350281\n",
      "Epoch: 3 \t\t\t Iteration 394 Loss Train: 0.45300400257110596\n",
      "Epoch: 3 \t\t\t Iteration 395 Loss Train: 0.4010234475135803\n",
      "Epoch: 3 \t\t\t Iteration 396 Loss Train: 0.3926429748535156\n",
      "Epoch: 3 \t\t\t Iteration 397 Loss Train: 0.6463550925254822\n",
      "Epoch: 3 \t\t\t Iteration 398 Loss Train: 0.4044223725795746\n",
      "Epoch: 3 \t\t\t Iteration 399 Loss Train: 0.35216236114501953\n",
      "Epoch: 3 \t\t\t Iteration 400 Loss Train: 0.2800581455230713\n",
      "Epoch: 3 \t\t\t Iteration 401 Loss Train: 0.4537203907966614\n",
      "Epoch: 3 \t\t\t Iteration 402 Loss Train: 0.3061877489089966\n",
      "Epoch: 3 \t\t\t Iteration 403 Loss Train: 0.42469891905784607\n",
      "Epoch: 3 \t\t\t Iteration 404 Loss Train: 0.4270826578140259\n",
      "Epoch: 3 \t\t\t Iteration 405 Loss Train: 0.44694483280181885\n",
      "Epoch: 3 \t\t\t Iteration 406 Loss Train: 0.47634685039520264\n",
      "Epoch: 3 \t\t\t Iteration 407 Loss Train: 0.5988479256629944\n",
      "Epoch: 3 \t\t\t Iteration 408 Loss Train: 0.4697975516319275\n",
      "Epoch: 3 \t\t\t Iteration 409 Loss Train: 0.45171791315078735\n",
      "Epoch: 3 \t\t\t Iteration 410 Loss Train: 0.366110622882843\n",
      "Epoch: 3 \t\t\t Iteration 411 Loss Train: 0.5472614169120789\n",
      "Epoch: 3 \t\t\t Iteration 412 Loss Train: 0.5203970670700073\n",
      "Epoch: 3 \t\t\t Iteration 413 Loss Train: 0.4925265908241272\n",
      "Epoch: 3 \t\t\t Iteration 414 Loss Train: 0.3990800380706787\n",
      "Epoch: 3 \t\t\t Iteration 415 Loss Train: 0.2931136190891266\n",
      "Epoch: 3 \t\t\t Iteration 416 Loss Train: 0.42627501487731934\n",
      "Epoch: 3 \t\t\t Iteration 417 Loss Train: 0.562028169631958\n",
      "Epoch: 3 \t\t\t Iteration 418 Loss Train: 0.5842001438140869\n",
      "Epoch: 3 \t\t\t Iteration 419 Loss Train: 0.48015278577804565\n",
      "Epoch: 3 \t\t\t Iteration 420 Loss Train: 0.48622050881385803\n",
      "Epoch: 3 \t\t\t Iteration 421 Loss Train: 0.3686479330062866\n",
      "Epoch: 3 \t\t\t Iteration 422 Loss Train: 0.4492987394332886\n",
      "Epoch: 3 \t\t\t Iteration 423 Loss Train: 0.3942923843860626\n",
      "Epoch: 3 \t\t\t Iteration 424 Loss Train: 0.32411378622055054\n",
      "Epoch: 3 \t\t\t Iteration 425 Loss Train: 0.35590559244155884\n",
      "Epoch: 3 \t\t\t Iteration 426 Loss Train: 0.5688956379890442\n",
      "Epoch: 3 \t\t\t Iteration 427 Loss Train: 0.40856409072875977\n",
      "Epoch: 3 \t\t\t Iteration 428 Loss Train: 0.35250964760780334\n",
      "Epoch: 3 \t\t\t Iteration 429 Loss Train: 0.303672194480896\n",
      "Epoch: 3 \t\t\t Iteration 430 Loss Train: 0.41510307788848877\n",
      "Epoch: 3 \t\t\t Iteration 431 Loss Train: 0.2868630886077881\n",
      "Epoch: 3 \t\t\t Iteration 432 Loss Train: 0.34633785486221313\n",
      "Epoch: 3 \t\t\t Iteration 433 Loss Train: 0.42170777916908264\n",
      "Epoch: 3 \t\t\t Iteration 434 Loss Train: 0.196295365691185\n",
      "Epoch: 3 \t\t\t Iteration 435 Loss Train: 0.3744848072528839\n",
      "Epoch: 3 \t\t\t Iteration 436 Loss Train: 0.4668181240558624\n",
      "Epoch: 3 \t\t\t Iteration 437 Loss Train: 0.3394905924797058\n",
      "Epoch: 3 \t\t\t Iteration 438 Loss Train: 0.5216124653816223\n",
      "Epoch: 3 \t\t\t Iteration 439 Loss Train: 0.3638681173324585\n",
      "Epoch: 3 \t\t\t Iteration 440 Loss Train: 0.21532109379768372\n",
      "Epoch: 3 \t\t\t Iteration 441 Loss Train: 0.280478298664093\n",
      "Epoch: 3 \t\t\t Iteration 442 Loss Train: 0.6544106006622314\n",
      "Epoch: 3 \t\t\t Iteration 443 Loss Train: 0.5341524481773376\n",
      "Epoch: 3 \t\t\t Iteration 444 Loss Train: 0.4192892611026764\n",
      "Epoch: 3 \t\t\t Iteration 445 Loss Train: 0.46430689096450806\n",
      "Epoch: 3 \t\t\t Iteration 446 Loss Train: 0.43704235553741455\n",
      "Epoch: 3 \t\t\t Iteration 447 Loss Train: 0.28372907638549805\n",
      "Epoch: 3 \t\t\t Iteration 448 Loss Train: 0.409362256526947\n",
      "Epoch: 3 \t\t\t Iteration 449 Loss Train: 0.4039698541164398\n",
      "Epoch: 3 \t\t\t Iteration 450 Loss Train: 0.3988443613052368\n",
      "Epoch: 3 \t\t\t Iteration 451 Loss Train: 0.5292657017707825\n",
      "Epoch: 3 \t\t\t Iteration 452 Loss Train: 0.3428463339805603\n",
      "Epoch: 3 \t\t\t Iteration 453 Loss Train: 0.2750522494316101\n",
      "Epoch: 3 \t\t\t Iteration 454 Loss Train: 0.40773385763168335\n",
      "Epoch: 3 \t\t\t Iteration 455 Loss Train: 0.24592900276184082\n",
      "Epoch: 3 \t\t\t Iteration 456 Loss Train: 0.504786491394043\n",
      "Epoch: 3 \t\t\t Iteration 457 Loss Train: 0.4215002655982971\n",
      "Epoch: 3 \t\t\t Iteration 458 Loss Train: 0.3963184356689453\n",
      "Epoch: 3 \t\t\t Iteration 459 Loss Train: 0.5031552314758301\n",
      "Epoch: 3 \t\t\t Iteration 460 Loss Train: 0.4321172833442688\n",
      "Epoch: 3 \t\t\t Iteration 461 Loss Train: 0.4004649519920349\n",
      "Epoch: 3 \t\t\t Iteration 462 Loss Train: 0.4099578261375427\n",
      "Epoch: 3 \t\t\t Iteration 463 Loss Train: 0.546431303024292\n",
      "Epoch: 3 \t\t\t Iteration 464 Loss Train: 0.3123667538166046\n",
      "Epoch: 3 \t\t\t Iteration 465 Loss Train: 0.33356165885925293\n",
      "Epoch: 3 \t\t\t Iteration 466 Loss Train: 0.2615646719932556\n",
      "Epoch: 3 \t\t\t Iteration 467 Loss Train: 0.28298377990722656\n",
      "Epoch: 3 \t\t\t Iteration 468 Loss Train: 0.2392253875732422\n",
      "Epoch: 3 \t\t\t Iteration 469 Loss Train: 0.1217578798532486\n",
      "Epoch: 3 \t\t\t Iteration 470 Loss Train: 0.8134276270866394\n",
      "Epoch: 3 \t\t\t Iteration 471 Loss Train: 0.4336245059967041\n",
      "Epoch: 3 \t\t\t Iteration 472 Loss Train: 0.3724876940250397\n",
      "Epoch: 3 \t\t\t Iteration 473 Loss Train: 0.3275110125541687\n",
      "Epoch: 3 \t\t\t Iteration 474 Loss Train: 0.32146865129470825\n",
      "Epoch: 3 \t\t\t Iteration 475 Loss Train: 0.4188600778579712\n",
      "Epoch: 3 \t\t\t Iteration 476 Loss Train: 0.43077585101127625\n",
      "Epoch: 3 \t\t\t Iteration 477 Loss Train: 0.35569649934768677\n",
      "Epoch: 3 \t\t\t Iteration 478 Loss Train: 0.3094785809516907\n",
      "Epoch: 3 \t\t\t Iteration 479 Loss Train: 0.7751790881156921\n",
      "Epoch: 3 \t\t\t Iteration 480 Loss Train: 0.5234386920928955\n",
      "Epoch: 3 \t\t\t Iteration 481 Loss Train: 0.34069985151290894\n",
      "Epoch: 3 \t\t\t Iteration 482 Loss Train: 0.37949270009994507\n",
      "Epoch: 3 \t\t\t Iteration 483 Loss Train: 0.21169212460517883\n",
      "Epoch: 3 \t\t\t Iteration 484 Loss Train: 0.42936718463897705\n",
      "Epoch: 3 \t\t\t Iteration 485 Loss Train: 0.4105624556541443\n",
      "Epoch: 3 \t\t\t Iteration 486 Loss Train: 0.500469982624054\n",
      "Epoch: 3 \t\t\t Iteration 487 Loss Train: 0.5087164640426636\n",
      "Epoch: 3 \t\t\t Iteration 488 Loss Train: 0.39771273732185364\n",
      "Epoch: 3 \t\t\t Iteration 489 Loss Train: 0.2398332953453064\n",
      "Epoch: 3 \t\t\t Iteration 490 Loss Train: 0.420783132314682\n",
      "Epoch: 3 \t\t\t Iteration 491 Loss Train: 0.3394380211830139\n",
      "Epoch: 3 \t\t\t Iteration 492 Loss Train: 0.6030469536781311\n",
      "Epoch: 3 \t\t\t Iteration 493 Loss Train: 0.4425100088119507\n",
      "Epoch: 3 \t\t\t Iteration 494 Loss Train: 0.43245068192481995\n",
      "Epoch: 3 \t\t\t Iteration 495 Loss Train: 0.3586861789226532\n",
      "Epoch: 3 \t\t\t Iteration 496 Loss Train: 0.5059374570846558\n",
      "Epoch: 3 \t\t\t Iteration 497 Loss Train: 0.26135414838790894\n",
      "Epoch: 3 \t\t\t Iteration 498 Loss Train: 0.6219606399536133\n",
      "Epoch: 3 \t\t\t Iteration 499 Loss Train: 0.4694705605506897\n",
      "Epoch: 3 \t\t\t Iteration 500 Loss Train: 0.3163496255874634\n",
      "Epoch: 3 \t\t\t Iteration 501 Loss Train: 0.5939251184463501\n",
      "Epoch: 3 \t\t\t Iteration 502 Loss Train: 0.4926121234893799\n",
      "Epoch: 3 \t\t\t Iteration 503 Loss Train: 0.551998496055603\n",
      "Epoch: 3 \t\t\t Iteration 504 Loss Train: 0.47905805706977844\n",
      "Epoch: 3 \t\t\t Iteration 505 Loss Train: 0.4984503388404846\n",
      "Epoch: 3 \t\t\t Iteration 506 Loss Train: 0.4750168025493622\n",
      "Epoch: 3 \t\t\t Iteration 507 Loss Train: 0.7169802188873291\n",
      "Epoch: 3 \t\t\t Iteration 508 Loss Train: 0.4630663990974426\n",
      "Epoch: 3 \t\t\t Iteration 509 Loss Train: 0.37300336360931396\n",
      "Epoch: 3 \t\t\t Iteration 510 Loss Train: 0.35460835695266724\n",
      "Epoch: 3 \t\t\t Iteration 511 Loss Train: 0.3721265196800232\n",
      "Epoch: 3 \t\t\t Iteration 512 Loss Train: 0.37387821078300476\n",
      "Epoch: 3 \t\t\t Iteration 513 Loss Train: 0.38311856985092163\n",
      "Epoch: 3 \t\t\t Iteration 514 Loss Train: 0.5527933835983276\n",
      "Epoch: 3 \t\t\t Iteration 515 Loss Train: 0.3349699378013611\n",
      "Epoch: 3 \t\t\t Iteration 516 Loss Train: 0.38128775358200073\n",
      "Epoch: 3 \t\t\t Iteration 517 Loss Train: 0.4430176913738251\n",
      "Epoch: 3 \t\t\t Iteration 518 Loss Train: 0.48110347986221313\n",
      "Epoch: 3 \t\t\t Iteration 519 Loss Train: 0.4055992662906647\n",
      "Epoch: 3 \t\t\t Iteration 520 Loss Train: 0.3814147114753723\n",
      "Epoch: 3 \t\t\t Iteration 521 Loss Train: 0.35686594247817993\n",
      "Epoch: 3 \t\t\t Iteration 522 Loss Train: 0.3812064528465271\n",
      "Epoch: 3 \t\t\t Iteration 523 Loss Train: 0.38333624601364136\n",
      "Epoch: 3 \t\t\t Iteration 524 Loss Train: 0.35178589820861816\n",
      "Epoch: 3 \t\t\t Iteration 525 Loss Train: 0.46504390239715576\n",
      "Epoch: 3 \t\t\t Iteration 526 Loss Train: 0.45444610714912415\n",
      "Epoch: 3 \t\t\t Iteration 527 Loss Train: 0.40974587202072144\n",
      "Epoch: 3 \t\t\t Iteration 528 Loss Train: 0.5318747758865356\n",
      "Epoch: 3 \t\t\t Iteration 529 Loss Train: 0.42424339056015015\n",
      "Epoch: 3 \t\t\t Iteration 530 Loss Train: 0.42008182406425476\n",
      "Epoch: 3 \t\t\t Iteration 531 Loss Train: 0.3266492486000061\n",
      "Epoch: 3 \t\t\t Iteration 532 Loss Train: 0.39040201902389526\n",
      "Epoch: 3 \t\t\t Iteration 533 Loss Train: 0.162916362285614\n",
      "Epoch: 3 \t\t\t Iteration 534 Loss Train: 0.3229660391807556\n",
      "Epoch: 3 \t\t\t Iteration 535 Loss Train: 0.35423165559768677\n",
      "Epoch: 3 \t\t\t Iteration 536 Loss Train: 0.35189634561538696\n",
      "Epoch: 3 \t\t\t Iteration 537 Loss Train: 0.5222589373588562\n",
      "Epoch: 3 \t\t\t Iteration 538 Loss Train: 0.534273624420166\n",
      "Epoch: 3 \t\t\t Iteration 539 Loss Train: 0.40016472339630127\n",
      "Epoch: 3 \t\t\t Iteration 540 Loss Train: 0.336442232131958\n",
      "Epoch: 3 \t\t\t Iteration 541 Loss Train: 0.25319620966911316\n",
      "Epoch: 3 \t\t\t Iteration 542 Loss Train: 0.5347952842712402\n",
      "Epoch: 3 \t\t\t Iteration 543 Loss Train: 0.3796137869358063\n",
      "Epoch: 3 \t\t\t Iteration 544 Loss Train: 0.4732482433319092\n",
      "Epoch: 3 \t\t\t Iteration 545 Loss Train: 0.3570239543914795\n",
      "Epoch: 3 \t\t\t Iteration 546 Loss Train: 0.4483945071697235\n",
      "Epoch: 3 \t\t\t Iteration 547 Loss Train: 0.4773384630680084\n",
      "Epoch: 3 \t\t\t Iteration 548 Loss Train: 0.46292129158973694\n",
      "Epoch: 3 \t\t\t Iteration 549 Loss Train: 0.27174460887908936\n",
      "Epoch: 3 \t\t\t Iteration 550 Loss Train: 0.3262569308280945\n",
      "Epoch: 3 \t\t\t Iteration 551 Loss Train: 0.33558011054992676\n",
      "Epoch: 3 \t\t\t Iteration 552 Loss Train: 0.33901312947273254\n",
      "Epoch: 3 \t\t\t Iteration 553 Loss Train: 0.48478901386260986\n",
      "Epoch: 3 \t\t\t Iteration 554 Loss Train: 0.42475298047065735\n",
      "Epoch: 3 \t\t\t Iteration 555 Loss Train: 0.5533732771873474\n",
      "Epoch: 3 \t\t\t Iteration 556 Loss Train: 0.3564562201499939\n",
      "Epoch: 3 \t\t\t Iteration 557 Loss Train: 0.33755284547805786\n",
      "Epoch: 3 \t\t\t Iteration 558 Loss Train: 0.4661675691604614\n",
      "Epoch: 3 \t\t\t Iteration 559 Loss Train: 0.37907785177230835\n",
      "Epoch: 3 \t\t\t Iteration 560 Loss Train: 0.4666249752044678\n",
      "Epoch: 3 \t\t\t Iteration 561 Loss Train: 0.41073110699653625\n",
      "Epoch: 3 \t\t\t Iteration 562 Loss Train: 0.39473891258239746\n",
      "Epoch: 3 \t\t\t Iteration 563 Loss Train: 0.3388083875179291\n",
      "Epoch: 3 \t\t\t Iteration 564 Loss Train: 0.4916875958442688\n",
      "Epoch: 3 \t\t\t Iteration 565 Loss Train: 0.2952404022216797\n",
      "Epoch: 3 \t\t\t Iteration 566 Loss Train: 0.38206446170806885\n",
      "Epoch: 3 \t\t\t Iteration 567 Loss Train: 0.41514211893081665\n",
      "Epoch: 3 \t\t\t Iteration 568 Loss Train: 0.31113606691360474\n",
      "Epoch: 3 \t\t\t Iteration 569 Loss Train: 0.42921262979507446\n",
      "Epoch: 3 \t\t\t Iteration 570 Loss Train: 0.4820968508720398\n",
      "Epoch: 3 \t\t\t Iteration 571 Loss Train: 0.5439339280128479\n",
      "Epoch: 3 \t\t\t Iteration 572 Loss Train: 0.37662750482559204\n",
      "Epoch: 3 \t\t\t Iteration 573 Loss Train: 0.37586545944213867\n",
      "Epoch: 3 \t\t\t Iteration 574 Loss Train: 0.6149060130119324\n",
      "Epoch: 3 \t\t\t Iteration 575 Loss Train: 0.44497358798980713\n",
      "Epoch: 3 \t\t\t Iteration 576 Loss Train: 0.45276081562042236\n",
      "Epoch: 3 \t\t\t Iteration 577 Loss Train: 0.2134626805782318\n",
      "Epoch: 3 \t\t\t Iteration 578 Loss Train: 0.30860549211502075\n",
      "Epoch: 3 \t\t\t Iteration 579 Loss Train: 0.591345489025116\n",
      "Epoch: 3 \t\t\t Iteration 580 Loss Train: 0.6936100721359253\n",
      "Epoch: 3 \t\t\t Iteration 581 Loss Train: 0.4801763594150543\n",
      "Epoch: 3 \t\t\t Iteration 582 Loss Train: 0.38670361042022705\n",
      "Epoch: 3 \t\t\t Iteration 583 Loss Train: 0.4289659857749939\n",
      "Epoch: 3 \t\t\t Iteration 584 Loss Train: 0.40645742416381836\n",
      "Epoch: 3 \t\t\t Iteration 585 Loss Train: 0.33012160658836365\n",
      "Epoch: 3 \t\t\t Iteration 586 Loss Train: 0.3988552391529083\n",
      "Epoch: 3 \t\t\t Iteration 587 Loss Train: 0.3938804566860199\n",
      "Epoch: 3 \t\t\t Iteration 588 Loss Train: 0.5359069108963013\n",
      "Epoch: 3 \t\t\t Iteration 589 Loss Train: 0.4144412875175476\n",
      "Epoch: 3 \t\t\t Iteration 590 Loss Train: 0.38835519552230835\n",
      "Epoch: 3 \t\t\t Iteration 591 Loss Train: 0.4610523581504822\n",
      "Epoch: 3 \t\t\t Iteration 592 Loss Train: 0.44458329677581787\n",
      "Epoch: 3 \t\t\t Iteration 593 Loss Train: 0.42912885546684265\n",
      "Epoch: 3 \t\t\t Iteration 594 Loss Train: 0.3867318332195282\n",
      "Epoch: 3 \t\t\t Iteration 595 Loss Train: 0.5613400936126709\n",
      "Epoch: 3 \t\t\t Iteration 596 Loss Train: 0.4355684220790863\n",
      "Epoch: 3 \t\t\t Iteration 597 Loss Train: 0.38382256031036377\n",
      "Epoch: 3 \t\t\t Iteration 598 Loss Train: 0.33501383662223816\n",
      "Epoch: 3 \t\t\t Iteration 599 Loss Train: 0.4204041659832001\n",
      "Epoch: 3 \t\t\t Iteration 600 Loss Train: 0.45490968227386475\n",
      "Epoch: 3 \t\t\t Iteration 601 Loss Train: 0.6007719039916992\n",
      "Epoch: 3 \t\t\t Iteration 602 Loss Train: 0.43135693669319153\n",
      "Epoch: 3 \t\t\t Iteration 603 Loss Train: 0.3373295068740845\n",
      "Epoch: 3 \t\t\t Iteration 604 Loss Train: 0.3063000440597534\n",
      "Epoch: 3 \t\t\t Iteration 605 Loss Train: 0.41814059019088745\n",
      "Epoch: 3 \t\t\t Iteration 606 Loss Train: 0.3695479929447174\n",
      "Epoch: 3 \t\t\t Iteration 607 Loss Train: 0.23748087882995605\n",
      "Epoch: 3 \t\t\t Iteration 608 Loss Train: 0.40258246660232544\n",
      "Epoch: 3 \t\t\t Iteration 609 Loss Train: 0.41069620847702026\n",
      "Epoch: 3 \t\t\t Iteration 610 Loss Train: 0.27733784914016724\n",
      "Epoch: 3 \t\t\t Iteration 611 Loss Train: 0.46065571904182434\n",
      "Epoch: 3 \t\t\t Iteration 612 Loss Train: 0.38801950216293335\n",
      "Epoch: 3 \t\t\t Iteration 613 Loss Train: 0.3804682493209839\n",
      "Epoch: 3 \t\t\t Iteration 614 Loss Train: 0.23650223016738892\n",
      "Epoch: 3 \t\t\t Iteration 615 Loss Train: 0.7918646335601807\n",
      "Epoch: 3 \t\t\t Iteration 616 Loss Train: 0.5631662607192993\n",
      "Epoch: 3 \t\t\t Iteration 617 Loss Train: 0.44830408692359924\n",
      "Epoch: 3 \t\t\t Iteration 618 Loss Train: 0.3774024546146393\n",
      "Epoch: 3 \t\t\t Iteration 619 Loss Train: 0.4373420178890228\n",
      "Epoch: 3 \t\t\t Iteration 620 Loss Train: 0.4618985056877136\n",
      "Epoch: 3 \t\t\t Iteration 621 Loss Train: 0.6702135801315308\n",
      "Epoch: 3 \t\t\t Iteration 622 Loss Train: 0.5549584627151489\n",
      "Epoch: 3 \t\t\t Iteration 623 Loss Train: 0.48085907101631165\n",
      "Epoch: 3 \t\t\t Iteration 624 Loss Train: 0.4105552136898041\n",
      "Epoch: 3 \t\t\t Iteration 625 Loss Train: 0.3827541768550873\n",
      "Epoch: 3 \t\t\t Iteration 626 Loss Train: 0.348280131816864\n",
      "Epoch: 3 \t\t\t Iteration 627 Loss Train: 0.49599263072013855\n",
      "Epoch: 3 \t\t\t Iteration 628 Loss Train: 0.4601581394672394\n",
      "Epoch: 3 \t\t\t Iteration 629 Loss Train: 0.5170473456382751\n",
      "Epoch: 3 \t\t\t Iteration 630 Loss Train: 0.26817449927330017\n",
      "Epoch: 3 \t\t\t Iteration 631 Loss Train: 0.37543362379074097\n",
      "Epoch: 3 \t\t\t Iteration 632 Loss Train: 0.39425376057624817\n",
      "Epoch: 3 \t\t\t Iteration 633 Loss Train: 0.5223114490509033\n",
      "Epoch: 3 \t\t\t Iteration 634 Loss Train: 0.48331165313720703\n",
      "Epoch: 3 \t\t\t Iteration 635 Loss Train: 0.3527817726135254\n",
      "Epoch: 3 \t\t\t Iteration 636 Loss Train: 0.4198744297027588\n",
      "Epoch: 3 \t\t\t Iteration 637 Loss Train: 0.4455992579460144\n",
      "Epoch: 3 \t\t\t Iteration 638 Loss Train: 0.33526086807250977\n",
      "Epoch: 3 \t\t\t Iteration 639 Loss Train: 0.48098984360694885\n",
      "Epoch: 3 \t\t\t Iteration 640 Loss Train: 0.5137354135513306\n",
      "Epoch: 3 \t\t\t Iteration 641 Loss Train: 0.44312429428100586\n",
      "Epoch: 3 \t\t\t Iteration 642 Loss Train: 0.5448477864265442\n",
      "Epoch: 3 \t\t\t Iteration 643 Loss Train: 0.2304326444864273\n",
      "Epoch: 3 \t\t\t Iteration 644 Loss Train: 0.3638146221637726\n",
      "Epoch: 3 \t\t\t Iteration 645 Loss Train: 0.43442481756210327\n",
      "Epoch: 3 \t\t\t Iteration 646 Loss Train: 0.3245266377925873\n",
      "Epoch: 3 \t\t\t Iteration 647 Loss Train: 0.4210853576660156\n",
      "Epoch: 3 \t\t\t Iteration 648 Loss Train: 0.3130846321582794\n",
      "Epoch: 3 \t\t\t Iteration 649 Loss Train: 0.4681265652179718\n",
      "Epoch: 3 \t\t\t Iteration 650 Loss Train: 0.3771264851093292\n",
      "Epoch: 3 \t\t\t Iteration 651 Loss Train: 0.41406959295272827\n",
      "Epoch: 3 \t\t\t Iteration 652 Loss Train: 0.41256383061408997\n",
      "Epoch: 3 \t\t\t Iteration 653 Loss Train: 0.4537690281867981\n",
      "Epoch: 3 \t\t\t Iteration 654 Loss Train: 0.3373526334762573\n",
      "Epoch: 3 \t\t\t Iteration 655 Loss Train: 0.30776247382164\n",
      "Epoch: 3 \t\t\t Iteration 656 Loss Train: 0.1875559538602829\n",
      "Epoch: 3 \t\t\t Iteration 657 Loss Train: 0.45960450172424316\n",
      "Epoch: 3 \t\t\t Iteration 658 Loss Train: 0.7837369441986084\n",
      "Epoch: 3 \t\t\t Iteration 659 Loss Train: 0.41202694177627563\n",
      "Epoch: 3 \t\t\t Iteration 660 Loss Train: 0.5553560853004456\n",
      "Epoch: 3 \t\t\t Iteration 661 Loss Train: 0.43716007471084595\n",
      "Epoch: 3 \t\t\t Iteration 662 Loss Train: 0.5146394371986389\n",
      "Epoch: 3 \t\t\t Iteration 663 Loss Train: 0.5109822154045105\n",
      "Epoch: 3 \t\t\t Iteration 664 Loss Train: 0.4800456166267395\n",
      "Epoch: 3 \t\t\t Iteration 665 Loss Train: 0.35458293557167053\n",
      "Epoch: 3 \t\t\t Iteration 666 Loss Train: 0.42151787877082825\n",
      "Epoch: 3 \t\t\t Iteration 667 Loss Train: 0.2478131651878357\n",
      "Epoch: 3 \t\t\t Iteration 668 Loss Train: 0.3967052698135376\n",
      "Epoch: 3 \t\t\t Iteration 669 Loss Train: 0.24345669150352478\n",
      "Epoch: 3 \t\t\t Iteration 670 Loss Train: 0.3583820164203644\n",
      "Epoch: 3 \t\t\t Iteration 671 Loss Train: 0.7460347414016724\n",
      "Epoch: 3 \t\t\t Iteration 672 Loss Train: 0.3875044286251068\n",
      "Epoch: 3 \t\t\t Iteration 673 Loss Train: 0.4843120574951172\n",
      "Epoch: 3 \t\t\t Iteration 674 Loss Train: 0.49026840925216675\n",
      "Epoch: 3 \t\t\t Iteration 675 Loss Train: 0.45572149753570557\n",
      "Epoch: 3 \t\t\t Iteration 676 Loss Train: 0.5824273228645325\n",
      "Epoch: 3 \t\t\t Iteration 677 Loss Train: 0.4944959580898285\n",
      "Epoch: 3 \t\t\t Iteration 678 Loss Train: 0.27203604578971863\n",
      "Epoch: 3 \t\t\t Iteration 679 Loss Train: 0.23978060483932495\n",
      "Epoch: 3 \t\t\t Iteration 680 Loss Train: 0.394225537776947\n",
      "Epoch: 3 \t\t\t Iteration 681 Loss Train: 0.46065258979797363\n",
      "Epoch: 3 \t\t\t Iteration 682 Loss Train: 0.2302953451871872\n",
      "Epoch: 3 \t\t\t Iteration 683 Loss Train: 0.23003144562244415\n",
      "Epoch: 3 \t\t\t Iteration 684 Loss Train: 0.4422622621059418\n",
      "Epoch: 3 \t\t\t Iteration 685 Loss Train: 0.5044633150100708\n",
      "Epoch: 3 \t\t\t Iteration 686 Loss Train: 0.46564632654190063\n",
      "Epoch: 3 \t\t\t Iteration 687 Loss Train: 0.3183405101299286\n",
      "Epoch: 3 \t\t\t Iteration 688 Loss Train: 0.8322536945343018\n",
      "Epoch: 3 \t\t\t Iteration 689 Loss Train: 0.4668814539909363\n",
      "Epoch: 3 \t\t\t Iteration 690 Loss Train: 0.3783286213874817\n",
      "Epoch: 3 \t\t\t Iteration 691 Loss Train: 0.4824952185153961\n",
      "Epoch: 3 \t\t\t Iteration 692 Loss Train: 0.5069855451583862\n",
      "Epoch: 3 \t\t\t Iteration 693 Loss Train: 0.3872182369232178\n",
      "Epoch: 3 \t\t\t Iteration 694 Loss Train: 0.3918045163154602\n",
      "Epoch: 3 \t\t\t Iteration 695 Loss Train: 0.40633970499038696\n",
      "Epoch: 3 \t\t\t Iteration 696 Loss Train: 0.47502243518829346\n",
      "Epoch: 3 \t\t\t Iteration 697 Loss Train: 0.3482109010219574\n",
      "Epoch: 3 \t\t\t Iteration 698 Loss Train: 0.27733394503593445\n",
      "Epoch: 3 \t\t\t Iteration 699 Loss Train: 0.2416674941778183\n",
      "Epoch: 3 \t\t\t Iteration 700 Loss Train: 0.477708101272583\n",
      "Epoch: 3 \t\t\t Iteration 701 Loss Train: 0.44156986474990845\n",
      "Epoch: 3 \t\t\t Iteration 702 Loss Train: 0.4944442808628082\n",
      "Epoch: 3 \t\t\t Iteration 703 Loss Train: 0.5725535154342651\n",
      "Epoch: 3 \t\t\t Iteration 704 Loss Train: 0.5530509948730469\n",
      "Epoch: 3 \t\t\t Iteration 705 Loss Train: 0.5681549310684204\n",
      "Epoch: 3 \t\t\t Iteration 706 Loss Train: 0.5058039426803589\n",
      "Epoch: 3 \t\t\t Iteration 707 Loss Train: 0.4041031002998352\n",
      "Epoch: 3 \t\t\t Iteration 708 Loss Train: 0.41105911135673523\n",
      "Epoch: 3 \t\t\t Iteration 709 Loss Train: 0.3845676779747009\n",
      "Epoch: 3 \t\t\t Iteration 710 Loss Train: 0.36084654927253723\n",
      "Epoch: 3 \t\t\t Iteration 711 Loss Train: 0.27831128239631653\n",
      "Epoch: 3 \t\t\t Iteration 712 Loss Train: 0.34483569860458374\n",
      "Epoch: 3 \t\t\t Iteration 713 Loss Train: 0.42237356305122375\n",
      "Epoch: 3 \t\t\t Iteration 714 Loss Train: 0.4569840133190155\n",
      "Epoch: 3 \t\t\t Iteration 715 Loss Train: 0.2801614999771118\n",
      "Epoch: 3 \t\t\t Iteration 716 Loss Train: 0.3841628432273865\n",
      "Epoch: 3 \t\t\t Iteration 717 Loss Train: 0.4341275691986084\n",
      "Epoch: 3 \t\t\t Iteration 718 Loss Train: 0.39893800020217896\n",
      "Epoch: 3 \t\t\t Iteration 719 Loss Train: 0.5182595252990723\n",
      "Epoch: 3 \t\t\t Iteration 720 Loss Train: 0.4932985305786133\n",
      "Epoch: 3 \t\t\t Iteration 721 Loss Train: 0.50975102186203\n",
      "Epoch: 3 \t\t\t Iteration 722 Loss Train: 0.356609046459198\n",
      "Epoch: 3 \t\t\t Iteration 723 Loss Train: 0.3496320843696594\n",
      "Epoch: 3 \t\t\t Iteration 724 Loss Train: 0.4069175720214844\n",
      "Epoch: 3 \t\t\t Iteration 725 Loss Train: 0.287402480840683\n",
      "Epoch: 3 \t\t\t Iteration 726 Loss Train: 0.5164684653282166\n",
      "Epoch: 3 \t\t\t Iteration 727 Loss Train: 0.46926623582839966\n",
      "Epoch: 3 \t\t\t Iteration 728 Loss Train: 0.5163216590881348\n",
      "Epoch: 3 \t\t\t Iteration 729 Loss Train: 0.3541153073310852\n",
      "Epoch: 3 \t\t\t Iteration 730 Loss Train: 0.31945812702178955\n",
      "Epoch: 3 \t\t\t Iteration 731 Loss Train: 0.40610790252685547\n",
      "Epoch: 3 \t\t\t Iteration 732 Loss Train: 0.2501351237297058\n",
      "Epoch: 3 \t\t\t Iteration 733 Loss Train: 0.3541161119937897\n",
      "Epoch: 3 \t\t\t Iteration 734 Loss Train: 0.7006958723068237\n",
      "Epoch: 3 \t\t\t Iteration 735 Loss Train: 0.504048764705658\n",
      "Epoch: 3 \t\t\t Iteration 736 Loss Train: 0.5370674729347229\n",
      "Epoch: 3 \t\t\t Iteration 737 Loss Train: 0.4060650169849396\n",
      "Epoch: 3 \t\t\t Iteration 738 Loss Train: 0.3606226146221161\n",
      "Epoch: 3 \t\t\t Iteration 739 Loss Train: 1.059541940689087\n",
      "Epoch: 3 \t\t\t Iteration 740 Loss Train: 0.5666600465774536\n",
      "Epoch: 3 \t\t\t Iteration 741 Loss Train: 0.4598216414451599\n",
      "Epoch: 3 \t\t\t Iteration 742 Loss Train: 0.5319622755050659\n",
      "Epoch: 3 \t\t\t Iteration 743 Loss Train: 0.46489354968070984\n",
      "Epoch: 3 \t\t\t Iteration 744 Loss Train: 0.40116727352142334\n",
      "Epoch: 3 \t\t\t Iteration 745 Loss Train: 0.4764488935470581\n",
      "Epoch: 3 \t\t\t Iteration 746 Loss Train: 0.45400986075401306\n",
      "Epoch: 3 \t\t\t Iteration 747 Loss Train: 0.4202336370944977\n",
      "Epoch: 3 \t\t\t Iteration 748 Loss Train: 0.34289854764938354\n",
      "Epoch: 3 \t\t\t Iteration 749 Loss Train: 0.3639451265335083\n",
      "Epoch: 3 \t\t\t Iteration 750 Loss Train: 0.4061160683631897\n",
      "Epoch: 3 \t\t\t Iteration 751 Loss Train: 0.428457647562027\n",
      "Epoch: 3 \t\t\t Iteration 752 Loss Train: 0.20748582482337952\n",
      "Epoch: 3 \t\t\t Iteration 753 Loss Train: 0.41107720136642456\n",
      "Epoch: 3 \t\t\t Iteration 754 Loss Train: 0.396506667137146\n",
      "Epoch: 3 \t\t\t Iteration 755 Loss Train: 0.6223291158676147\n",
      "Epoch: 3 \t\t\t Iteration 756 Loss Train: 0.4464011788368225\n",
      "Epoch: 3 \t\t\t Iteration 757 Loss Train: 0.43926292657852173\n",
      "Epoch: 3 \t\t\t Iteration 758 Loss Train: 0.3538799285888672\n",
      "Epoch: 3 \t\t\t Iteration 759 Loss Train: 0.42348194122314453\n",
      "Epoch: 3 \t\t\t Iteration 760 Loss Train: 0.47131097316741943\n",
      "Epoch: 3 \t\t\t Iteration 761 Loss Train: 0.5349187850952148\n",
      "Epoch: 3 \t\t\t Iteration 762 Loss Train: 0.29888710379600525\n",
      "Epoch: 3 \t\t\t Iteration 763 Loss Train: 0.4027051329612732\n",
      "Epoch: 3 \t\t\t Iteration 764 Loss Train: 0.4550653100013733\n",
      "Epoch: 3 \t\t\t Iteration 765 Loss Train: 0.23538289964199066\n",
      "Epoch: 3 \t\t\t Iteration 766 Loss Train: 0.47852200269699097\n",
      "Epoch: 3 \t\t\t Iteration 767 Loss Train: 0.3870978057384491\n",
      "Epoch: 3 \t\t\t Iteration 768 Loss Train: 0.7562506198883057\n",
      "Epoch: 3 \t\t\t Iteration 769 Loss Train: 0.587913453578949\n",
      "Epoch: 3 \t\t\t Iteration 770 Loss Train: 0.46497491002082825\n",
      "Epoch: 3 \t\t\t Iteration 771 Loss Train: 0.3882322907447815\n",
      "Epoch: 3 \t\t\t Iteration 772 Loss Train: 0.6073481440544128\n",
      "Epoch: 3 \t\t\t Iteration 773 Loss Train: 0.392802357673645\n",
      "Epoch: 3 \t\t\t Iteration 774 Loss Train: 0.30180370807647705\n",
      "Epoch: 3 \t\t\t Iteration 775 Loss Train: 0.3832399845123291\n",
      "Epoch: 3 \t\t\t Iteration 776 Loss Train: 0.602922797203064\n",
      "Epoch: 3 \t\t\t Iteration 777 Loss Train: 0.531442403793335\n",
      "Epoch: 3 \t\t\t Iteration 778 Loss Train: 0.48850542306900024\n",
      "Epoch: 3 \t\t\t Iteration 779 Loss Train: 0.399810791015625\n",
      "Epoch: 3 \t\t\t Iteration 780 Loss Train: 0.4229549765586853\n",
      "Epoch: 3 \t\t\t Iteration 781 Loss Train: 0.5253891944885254\n",
      "Epoch: 3 \t\t\t Iteration 782 Loss Train: 0.4941404461860657\n",
      "Epoch: 3 \t\t\t Iteration 783 Loss Train: 0.40557771921157837\n",
      "Epoch: 3 \t\t\t Iteration 784 Loss Train: 0.40718352794647217\n",
      "Epoch: 3 \t\t\t Iteration 785 Loss Train: 0.48368117213249207\n",
      "Epoch: 3 \t\t\t Iteration 786 Loss Train: 0.42347174882888794\n",
      "Epoch: 3 \t\t\t Iteration 787 Loss Train: 0.3501647412776947\n",
      "Epoch: 3 \t\t\t Iteration 788 Loss Train: 0.24051828682422638\n",
      "Epoch: 3 \t\t\t Iteration 789 Loss Train: 0.37325620651245117\n",
      "Epoch: 3 \t\t\t Iteration 790 Loss Train: 0.6475772261619568\n",
      "Epoch: 3 \t\t\t Iteration 791 Loss Train: 0.3702889680862427\n",
      "Epoch: 3 \t\t\t Iteration 792 Loss Train: 0.4182204604148865\n",
      "Epoch: 3 \t\t\t Iteration 793 Loss Train: 0.4169168472290039\n",
      "Epoch: 3 \t\t\t Iteration 794 Loss Train: 0.5006700754165649\n",
      "Epoch: 3 \t\t\t Iteration 795 Loss Train: 0.4431925415992737\n",
      "Epoch: 3 \t\t\t Iteration 796 Loss Train: 0.3628288805484772\n",
      "Epoch: 3 \t\t\t Iteration 797 Loss Train: 0.2693333327770233\n",
      "Epoch: 3 \t\t\t Iteration 798 Loss Train: 0.39185988903045654\n",
      "Epoch: 3 \t\t\t Iteration 799 Loss Train: 0.3314894437789917\n",
      "Epoch: 3 \t\t\t Iteration 800 Loss Train: 0.3778747320175171\n",
      "Epoch: 3 \t\t\t Iteration 801 Loss Train: 0.25538891553878784\n",
      "Epoch: 3 \t\t\t Iteration 802 Loss Train: 0.23921698331832886\n",
      "Epoch: 3 \t\t\t Iteration 803 Loss Train: 0.6395465135574341\n",
      "Epoch: 3 \t\t\t Iteration 804 Loss Train: 0.41006308794021606\n",
      "Epoch: 3 \t\t\t Iteration 805 Loss Train: 0.4586511254310608\n",
      "Epoch: 3 \t\t\t Iteration 806 Loss Train: 0.32211485505104065\n",
      "Epoch: 3 \t\t\t Iteration 807 Loss Train: 0.4711343050003052\n",
      "Epoch: 3 \t\t\t Iteration 808 Loss Train: 0.4931320548057556\n",
      "Epoch: 3 \t\t\t Iteration 809 Loss Train: 0.617416262626648\n",
      "Epoch: 3 \t\t\t Iteration 810 Loss Train: 0.48333001136779785\n",
      "Epoch: 3 \t\t\t Iteration 811 Loss Train: 0.3436861038208008\n",
      "Epoch: 3 \t\t\t Iteration 812 Loss Train: 0.2726131081581116\n",
      "Epoch: 3 \t\t\t Iteration 813 Loss Train: 0.25991249084472656\n",
      "Epoch: 3 \t\t\t Iteration 814 Loss Train: 0.481864869594574\n",
      "Epoch: 3 \t\t\t Iteration 815 Loss Train: 0.5128437876701355\n",
      "Epoch: 3 \t\t\t Iteration 816 Loss Train: 0.3280119001865387\n",
      "Epoch: 3 \t\t\t Iteration 817 Loss Train: 0.37137433886528015\n",
      "Epoch: 3 \t\t\t Iteration 818 Loss Train: 0.4518510699272156\n",
      "Epoch: 3 \t\t\t Iteration 819 Loss Train: 0.3770430088043213\n",
      "Epoch: 3 \t\t\t Iteration 820 Loss Train: 0.4744516909122467\n",
      "Epoch: 3 \t\t\t Iteration 821 Loss Train: 0.4762709140777588\n",
      "Epoch: 3 \t\t\t Iteration 822 Loss Train: 0.478619784116745\n",
      "Epoch: 3 \t\t\t Iteration 823 Loss Train: 0.501952052116394\n",
      "Epoch: 3 \t\t\t Iteration 824 Loss Train: 0.4629032611846924\n",
      "Epoch: 3 \t\t\t Iteration 825 Loss Train: 0.34964436292648315\n",
      "Epoch: 3 \t\t\t Iteration 826 Loss Train: 0.3172752857208252\n",
      "Epoch: 3 \t\t\t Iteration 827 Loss Train: 0.3747566342353821\n",
      "Epoch: 3 \t\t\t Iteration 828 Loss Train: 0.6353096961975098\n",
      "Epoch: 3 \t\t\t Iteration 829 Loss Train: 0.7831465601921082\n",
      "Epoch: 3 \t\t\t Iteration 830 Loss Train: 0.6290396451950073\n",
      "Epoch: 3 \t\t\t Iteration 831 Loss Train: 0.3080589771270752\n",
      "Epoch: 3 \t\t\t Iteration 832 Loss Train: 0.2445739209651947\n",
      "Epoch: 3 \t\t\t Iteration 833 Loss Train: 0.4667101204395294\n",
      "Epoch: 3 \t\t\t Iteration 834 Loss Train: 0.26679664850234985\n",
      "Epoch: 3 \t\t\t Iteration 835 Loss Train: 0.3539312481880188\n",
      "Epoch: 3 \t\t\t Iteration 836 Loss Train: 0.33293694257736206\n",
      "Epoch: 3 \t\t\t Iteration 837 Loss Train: 0.6123852729797363\n",
      "Epoch: 3 \t\t\t Iteration 838 Loss Train: 0.4849953353404999\n",
      "Epoch: 3 \t\t\t Iteration 839 Loss Train: 0.4821298122406006\n",
      "Epoch: 3 \t\t\t Iteration 840 Loss Train: 0.42740529775619507\n",
      "Epoch: 3 \t\t\t Iteration 841 Loss Train: 0.5203215479850769\n",
      "Epoch: 3 \t\t\t Iteration 842 Loss Train: 0.24446748197078705\n",
      "Epoch: 3 \t\t\t Iteration 843 Loss Train: 0.2893298268318176\n",
      "Epoch: 3 \t\t\t Iteration 844 Loss Train: 0.5076287984848022\n",
      "Epoch: 3 \t\t\t Iteration 845 Loss Train: 0.5420172810554504\n",
      "Epoch: 3 \t\t\t Iteration 846 Loss Train: 0.33960220217704773\n",
      "Epoch: 3 \t\t\t Iteration 847 Loss Train: 0.34146666526794434\n",
      "Epoch: 3 \t\t\t Iteration 848 Loss Train: 0.42730164527893066\n",
      "Epoch: 3 \t\t\t Iteration 849 Loss Train: 0.3424442410469055\n",
      "Epoch: 3 \t\t\t Iteration 850 Loss Train: 0.30820968747138977\n",
      "Epoch: 3 \t\t\t Iteration 851 Loss Train: 0.4913015365600586\n",
      "Epoch: 3 \t\t\t Iteration 852 Loss Train: 0.45055288076400757\n",
      "Epoch: 3 \t\t\t Iteration 853 Loss Train: 0.38112497329711914\n",
      "Epoch: 3 \t\t\t Iteration 854 Loss Train: 0.4384545683860779\n",
      "Epoch: 3 \t\t\t Iteration 855 Loss Train: 0.31471681594848633\n",
      "Epoch: 3 \t\t\t Iteration 856 Loss Train: 0.49190375208854675\n",
      "Epoch: 3 \t\t\t Iteration 857 Loss Train: 0.3530246615409851\n",
      "Epoch: 3 \t\t\t Iteration 858 Loss Train: 0.3282065987586975\n",
      "Epoch: 3 \t\t\t Iteration 859 Loss Train: 0.3218284547328949\n",
      "Epoch: 3 \t\t\t Iteration 860 Loss Train: 0.41726425290107727\n",
      "Epoch: 3 \t\t\t Iteration 861 Loss Train: 0.3231082558631897\n",
      "Epoch: 3 \t\t\t Iteration 862 Loss Train: 0.274517297744751\n",
      "Epoch: 3 \t\t\t Iteration 863 Loss Train: 0.43829309940338135\n",
      "Epoch: 3 \t\t\t Iteration 864 Loss Train: 0.7034055590629578\n",
      "Epoch: 3 \t\t\t Iteration 865 Loss Train: 0.42535680532455444\n",
      "Epoch: 3 \t\t\t Iteration 866 Loss Train: 0.27012068033218384\n",
      "Epoch: 3 \t\t\t Iteration 867 Loss Train: 0.4156160354614258\n",
      "Epoch: 3 \t\t\t Iteration 868 Loss Train: 0.6153261661529541\n",
      "Epoch: 3 \t\t\t Iteration 869 Loss Train: 0.43931567668914795\n",
      "Epoch: 3 \t\t\t Iteration 870 Loss Train: 0.44401881098747253\n",
      "Epoch: 3 \t\t\t Iteration 871 Loss Train: 0.41910117864608765\n",
      "Epoch: 3 \t\t\t Iteration 872 Loss Train: 0.39977073669433594\n",
      "Epoch: 3 \t\t\t Iteration 873 Loss Train: 0.593163251876831\n",
      "Epoch: 3 \t\t\t Iteration 874 Loss Train: 0.5637118816375732\n",
      "Epoch: 3 \t\t\t Iteration 875 Loss Train: 0.3829694986343384\n",
      "Epoch: 3 \t\t\t Iteration 876 Loss Train: 0.2752020061016083\n",
      "Epoch: 3 \t\t\t Iteration 877 Loss Train: 0.5455591678619385\n",
      "Epoch: 3 \t\t\t Iteration 878 Loss Train: 0.4104912281036377\n",
      "Epoch: 3 \t\t\t Iteration 879 Loss Train: 0.5161694288253784\n",
      "Epoch: 3 \t\t\t Iteration 880 Loss Train: 0.5950254797935486\n",
      "Epoch: 3 \t\t\t Iteration 881 Loss Train: 0.47198620438575745\n",
      "Epoch: 3 \t\t\t Iteration 882 Loss Train: 0.48709046840667725\n",
      "Epoch: 3 \t\t\t Iteration 883 Loss Train: 0.35993725061416626\n",
      "Epoch: 3 \t\t\t Iteration 884 Loss Train: 0.6738517880439758\n",
      "Epoch: 3 \t\t\t Iteration 885 Loss Train: 0.48001137375831604\n",
      "Epoch: 3 \t\t\t Iteration 886 Loss Train: 0.3165944516658783\n",
      "Epoch: 3 \t\t\t Iteration 887 Loss Train: 0.37226617336273193\n",
      "Epoch: 3 \t\t\t Iteration 888 Loss Train: 0.37419235706329346\n",
      "Epoch: 3 \t\t\t Iteration 889 Loss Train: 0.3412795066833496\n",
      "Epoch: 3 \t\t\t Iteration 890 Loss Train: 0.7591485977172852\n",
      "Epoch: 3 \t\t\t Iteration 891 Loss Train: 0.4782106578350067\n",
      "Epoch: 3 \t\t\t Iteration 892 Loss Train: 0.4588586688041687\n",
      "Epoch: 3 \t\t\t Iteration 893 Loss Train: 0.2957703471183777\n",
      "Epoch: 3 \t\t\t Iteration 894 Loss Train: 0.42560628056526184\n",
      "Epoch: 3 \t\t\t Iteration 895 Loss Train: 0.5430463552474976\n",
      "Epoch: 3 \t\t\t Iteration 896 Loss Train: 0.30480965971946716\n",
      "Epoch: 3 \t\t\t Iteration 897 Loss Train: 0.45253995060920715\n",
      "Epoch: 3 \t\t\t Iteration 898 Loss Train: 0.40960538387298584\n",
      "Epoch: 3 \t\t\t Iteration 899 Loss Train: 0.5209075212478638\n",
      "Epoch: 3 \t\t\t Iteration 900 Loss Train: 0.2629154324531555\n",
      "Epoch: 3 \t\t\t Iteration 901 Loss Train: 0.581264078617096\n",
      "Epoch: 3 \t\t\t Iteration 902 Loss Train: 0.4359925091266632\n",
      "Epoch: 3 \t\t\t Iteration 903 Loss Train: 0.45899176597595215\n",
      "Epoch: 3 \t\t\t Iteration 904 Loss Train: 0.4083907902240753\n",
      "Epoch: 3 \t\t\t Iteration 905 Loss Train: 0.41838645935058594\n",
      "Epoch: 3 \t\t\t Iteration 906 Loss Train: 0.2720067799091339\n",
      "Epoch: 3 \t\t\t Iteration 907 Loss Train: 0.556637704372406\n",
      "Epoch: 3 \t\t\t Iteration 908 Loss Train: 0.4468691051006317\n",
      "Epoch: 3 \t\t\t Iteration 909 Loss Train: 0.3881300687789917\n",
      "Epoch: 3 \t\t\t Iteration 910 Loss Train: 0.6451034545898438\n",
      "Epoch: 3 \t\t\t Iteration 911 Loss Train: 0.5408977270126343\n",
      "Epoch: 3 \t\t\t Iteration 912 Loss Train: 0.4522438645362854\n",
      "Epoch: 3 \t\t\t Iteration 913 Loss Train: 0.35006919503211975\n",
      "Epoch: 3 \t\t\t Iteration 914 Loss Train: 0.3161381483078003\n",
      "Epoch: 3 \t\t\t Iteration 915 Loss Train: 0.46788954734802246\n",
      "Epoch: 3 \t\t\t Iteration 916 Loss Train: 0.4188983142375946\n",
      "Epoch: 3 \t\t\t Iteration 917 Loss Train: 0.4929596781730652\n",
      "Epoch: 3 \t\t\t Iteration 918 Loss Train: 0.41353780031204224\n",
      "Epoch: 3 \t\t\t Iteration 919 Loss Train: 0.3900471329689026\n",
      "Epoch: 3 \t\t\t Iteration 920 Loss Train: 0.4405550956726074\n",
      "Epoch: 3 \t\t\t Iteration 921 Loss Train: 0.4150920510292053\n",
      "Epoch: 3 \t\t\t Iteration 922 Loss Train: 0.27357542514801025\n",
      "Epoch: 3 \t\t\t Iteration 923 Loss Train: 0.5371385812759399\n",
      "Epoch: 3 \t\t\t Iteration 924 Loss Train: 0.45971381664276123\n",
      "Epoch: 3 \t\t\t Iteration 925 Loss Train: 0.46236491203308105\n",
      "Epoch: 3 \t\t\t Iteration 926 Loss Train: 0.34855252504348755\n",
      "Epoch: 3 \t\t\t Iteration 927 Loss Train: 0.5000688433647156\n",
      "Epoch: 3 \t\t\t Iteration 928 Loss Train: 0.5311567783355713\n",
      "Epoch: 3 \t\t\t Iteration 929 Loss Train: 0.4918898344039917\n",
      "Epoch: 3 \t\t\t Iteration 930 Loss Train: 0.3426305651664734\n",
      "Epoch: 3 \t\t\t Iteration 931 Loss Train: 0.433495432138443\n",
      "Epoch: 3 \t\t\t Iteration 932 Loss Train: 0.3277580738067627\n",
      "Epoch: 3 \t\t\t Iteration 933 Loss Train: 0.341175377368927\n",
      "Epoch: 3 \t\t\t Iteration 934 Loss Train: 0.5597375631332397\n",
      "Epoch: 3 \t\t\t Iteration 935 Loss Train: 0.4728577733039856\n",
      "Epoch: 3 \t\t\t Iteration 936 Loss Train: 0.551056981086731\n",
      "Epoch: 3 \t\t\t Iteration 937 Loss Train: 0.3744031488895416\n",
      "Epoch: 3 \t\t\t Iteration 938 Loss Train: 0.550940752029419\n",
      "Epoch: 3 \t\t\t Iteration 939 Loss Train: 0.4204963445663452\n",
      "Epoch: 3 \t\t\t Iteration 940 Loss Train: 0.4307289123535156\n",
      "Epoch: 3 \t\t\t Iteration 941 Loss Train: 0.3671993017196655\n",
      "Epoch: 3 \t\t\t Iteration 942 Loss Train: 0.4602947533130646\n",
      "Epoch: 3 \t\t\t Iteration 943 Loss Train: 0.4242621064186096\n",
      "Epoch: 3 \t\t\t Iteration 944 Loss Train: 0.3581661581993103\n",
      "Epoch: 3 \t\t\t Iteration 945 Loss Train: 0.38984864950180054\n",
      "Epoch: 3 \t\t\t Iteration 946 Loss Train: 0.3197295069694519\n",
      "Epoch: 3 \t\t\t Iteration 947 Loss Train: 0.2072533667087555\n",
      "Epoch: 3 \t\t\t Iteration 948 Loss Train: 0.10076265037059784\n",
      "Epoch: 3 \t\t\t Iteration 949 Loss Train: 0.3427160382270813\n",
      "Epoch: 3 \t\t\t Iteration 950 Loss Train: 0.38462311029434204\n",
      "Epoch: 3 \t\t\t Iteration 951 Loss Train: 0.42349451780319214\n",
      "Epoch: 3 \t\t\t Iteration 952 Loss Train: 0.4149612784385681\n",
      "Epoch: 3 \t\t\t Iteration 953 Loss Train: 0.21632057428359985\n",
      "Epoch: 3 \t\t\t Iteration 954 Loss Train: 0.5892417430877686\n",
      "Epoch: 3 \t\t\t Iteration 955 Loss Train: 0.5583752393722534\n",
      "Epoch: 3 \t\t\t Iteration 956 Loss Train: 0.4235801696777344\n",
      "Epoch: 3 \t\t\t Iteration 957 Loss Train: 0.5264822244644165\n",
      "Epoch: 3 \t\t\t Iteration 958 Loss Train: 0.5615614652633667\n",
      "Epoch: 3 \t\t\t Iteration 959 Loss Train: 0.48798879981040955\n",
      "Epoch: 3 \t\t\t Iteration 960 Loss Train: 0.33634281158447266\n",
      "Epoch: 3 \t\t\t Iteration 961 Loss Train: 0.806157112121582\n",
      "Epoch: 3 \t\t\t Iteration 962 Loss Train: 0.4333735704421997\n",
      "Epoch: 3 \t\t\t Iteration 963 Loss Train: 0.4339468777179718\n",
      "Epoch: 3 \t\t\t Iteration 964 Loss Train: 0.3878384232521057\n",
      "Epoch: 3 \t\t\t Iteration 965 Loss Train: 0.4501911699771881\n",
      "Epoch: 3 \t\t\t Iteration 966 Loss Train: 0.37770503759384155\n",
      "Epoch: 3 \t\t\t Iteration 967 Loss Train: 0.22303247451782227\n",
      "Epoch: 3 \t\t\t Iteration 968 Loss Train: 0.36770811676979065\n",
      "Epoch: 3 \t\t\t Iteration 969 Loss Train: 0.5634119510650635\n",
      "Epoch: 3 \t\t\t Iteration 970 Loss Train: 0.29745063185691833\n",
      "Epoch: 3 \t\t\t Iteration 971 Loss Train: 0.44659125804901123\n",
      "Epoch: 3 \t\t\t Iteration 972 Loss Train: 0.6127486824989319\n",
      "Epoch: 3 \t\t\t Iteration 973 Loss Train: 0.5575653314590454\n",
      "Epoch: 3 \t\t\t Iteration 974 Loss Train: 0.4200790226459503\n",
      "Epoch: 3 \t\t\t Iteration 975 Loss Train: 0.485279381275177\n",
      "Epoch: 3 \t\t\t Iteration 976 Loss Train: 0.41268759965896606\n",
      "Epoch: 3 \t\t\t Iteration 977 Loss Train: 0.41973116993904114\n",
      "Epoch: 3 \t\t\t Iteration 978 Loss Train: 0.5063769221305847\n",
      "Epoch: 3 \t\t\t Iteration 979 Loss Train: 0.4246255159378052\n",
      "Epoch: 3 \t\t\t Iteration 980 Loss Train: 0.3381876051425934\n",
      "Epoch: 3 \t\t\t Iteration 981 Loss Train: 0.2005520462989807\n",
      "Epoch: 3 \t\t\t Iteration 982 Loss Train: 0.40800899267196655\n",
      "Epoch: 3 \t\t\t Iteration 983 Loss Train: 0.39609307050704956\n",
      "Epoch: 3 \t\t\t Iteration 984 Loss Train: 0.5864098072052002\n",
      "Epoch: 3 \t\t\t Iteration 985 Loss Train: 0.4487057328224182\n",
      "Epoch: 3 \t\t\t Iteration 986 Loss Train: 0.5142068862915039\n",
      "Epoch: 3 \t\t\t Iteration 987 Loss Train: 0.47867363691329956\n",
      "Epoch: 3 \t\t\t Iteration 988 Loss Train: 0.2621552050113678\n",
      "Epoch: 3 \t\t\t Iteration 989 Loss Train: 0.3242005407810211\n",
      "Epoch: 3 \t\t\t Iteration 990 Loss Train: 0.3554227948188782\n",
      "Epoch: 3 \t\t\t Iteration 991 Loss Train: 0.3143655061721802\n",
      "Epoch: 3 \t\t\t Iteration 992 Loss Train: 0.47253817319869995\n",
      "Epoch: 3 \t\t\t Iteration 993 Loss Train: 0.44225823879241943\n",
      "Epoch: 3 \t\t\t Iteration 994 Loss Train: 0.34610217809677124\n",
      "Epoch: 3 \t\t\t Iteration 995 Loss Train: 0.3313133716583252\n",
      "Epoch: 3 \t\t\t Iteration 996 Loss Train: 0.639603853225708\n",
      "Epoch: 3 \t\t\t Iteration 997 Loss Train: 0.45393267273902893\n",
      "Epoch: 3 \t\t\t Iteration 998 Loss Train: 0.4596855044364929\n",
      "Epoch: 3 \t\t\t Iteration 999 Loss Train: 0.6531270146369934\n",
      "Epoch: 3 \t\t\t Iteration 1000 Loss Train: 0.455708384513855\n",
      "Epoch: 3 \t\t\t Iteration 1001 Loss Train: 0.47040706872940063\n",
      "Epoch: 3 \t\t\t Iteration 1002 Loss Train: 0.5130528211593628\n",
      "Epoch: 3 \t\t\t Iteration 1003 Loss Train: 0.21685411036014557\n",
      "Epoch: 3 \t\t\t Iteration 1004 Loss Train: 0.32302767038345337\n",
      "Epoch: 3 \t\t\t Iteration 1005 Loss Train: 0.5628145933151245\n",
      "Epoch: 3 \t\t\t Iteration 1006 Loss Train: 0.2994503378868103\n",
      "Epoch: 3 \t\t\t Iteration 1007 Loss Train: 0.45576757192611694\n",
      "Epoch: 3 \t\t\t Iteration 1008 Loss Train: 0.3903142809867859\n",
      "Epoch: 3 \t\t\t Iteration 1009 Loss Train: 0.37450075149536133\n",
      "Epoch: 3 \t\t\t Iteration 1010 Loss Train: 0.4895520508289337\n",
      "Epoch: 3 \t\t\t Iteration 1011 Loss Train: 0.4775150418281555\n",
      "Epoch: 3 \t\t\t Iteration 1012 Loss Train: 0.4780670404434204\n",
      "Epoch: 3 \t\t\t Iteration 1013 Loss Train: 0.2931936979293823\n",
      "Epoch: 3 \t\t\t Iteration 1014 Loss Train: 0.4952508807182312\n",
      "Epoch: 3 \t\t\t Iteration 1015 Loss Train: 0.33904922008514404\n",
      "Epoch: 3 \t\t\t Iteration 1016 Loss Train: 0.39108872413635254\n",
      "Epoch: 3 \t\t\t Iteration 1017 Loss Train: 0.547276496887207\n",
      "Epoch: 3 \t\t\t Iteration 1018 Loss Train: 0.5416646599769592\n",
      "Epoch: 3 \t\t\t Iteration 1019 Loss Train: 0.3210254907608032\n",
      "Epoch: 3 \t\t\t Iteration 1020 Loss Train: 0.4453357458114624\n",
      "Epoch: 3 \t\t\t Iteration 1021 Loss Train: 0.4372164309024811\n",
      "Epoch: 3 \t\t\t Iteration 1022 Loss Train: 0.5979903936386108\n",
      "Epoch: 3 \t\t\t Iteration 1023 Loss Train: 0.4136236906051636\n",
      "Epoch: 3 \t\t\t Iteration 1024 Loss Train: 0.24749772250652313\n",
      "Epoch: 3 \t\t\t Iteration 1025 Loss Train: 0.3724789023399353\n",
      "Epoch: 3 \t\t\t Iteration 1026 Loss Train: 0.41571590304374695\n",
      "Epoch: 3 \t\t\t Iteration 1027 Loss Train: 0.5389912128448486\n",
      "Epoch: 3 \t\t\t Iteration 1028 Loss Train: 0.2898455560207367\n",
      "Epoch: 3 \t\t\t Iteration 1029 Loss Train: 0.13744807243347168\n",
      "Epoch: 3 \t\t\t Iteration 1030 Loss Train: 0.4810735583305359\n",
      "Epoch: 3 \t\t\t Iteration 1031 Loss Train: 0.3650222420692444\n",
      "Epoch: 3 \t\t\t Iteration 1032 Loss Train: 0.4929679036140442\n",
      "Epoch: 3 \t\t\t Iteration 1033 Loss Train: 0.4328388571739197\n",
      "Epoch: 3 \t\t\t Iteration 1034 Loss Train: 0.4569252133369446\n",
      "Epoch: 3 \t\t\t Iteration 1035 Loss Train: 0.3701215386390686\n",
      "Epoch: 3 \t\t\t Iteration 1036 Loss Train: 0.23519763350486755\n",
      "Epoch: 3 \t\t\t Iteration 1037 Loss Train: 0.21562278270721436\n",
      "Epoch: 3 \t\t\t Iteration 1038 Loss Train: 0.46408358216285706\n",
      "Epoch: 3 \t\t\t Iteration 1039 Loss Train: 0.3743767738342285\n",
      "Epoch: 3 \t\t\t Iteration 1040 Loss Train: 0.39229169487953186\n",
      "Epoch: 3 \t\t\t Iteration 1041 Loss Train: 0.5710958242416382\n",
      "Epoch: 3 \t\t\t Iteration 1042 Loss Train: 0.5175104737281799\n",
      "Epoch: 3 \t\t\t Iteration 1043 Loss Train: 0.627955436706543\n",
      "Epoch: 3 \t\t\t Iteration 1044 Loss Train: 0.44130939245224\n",
      "Epoch: 3 \t\t\t Iteration 1045 Loss Train: 0.3898583650588989\n",
      "Epoch: 3 \t\t\t Iteration 1046 Loss Train: 0.3440842032432556\n",
      "Epoch: 3 \t\t\t Iteration 1047 Loss Train: 0.278007835149765\n",
      "Epoch: 3 \t\t\t Iteration 1048 Loss Train: 0.5816137194633484\n",
      "Epoch: 3 \t\t\t Iteration 1049 Loss Train: 0.37321937084198\n",
      "Epoch: 3 \t\t\t Iteration 1050 Loss Train: 0.507838249206543\n",
      "Epoch: 3 \t\t\t Iteration 1051 Loss Train: 0.3666660785675049\n",
      "Epoch: 3 \t\t\t Iteration 1052 Loss Train: 0.5498526692390442\n",
      "Epoch: 3 \t\t\t Iteration 1053 Loss Train: 0.42861849069595337\n",
      "Epoch: 3 \t\t\t Iteration 1054 Loss Train: 0.34303444623947144\n",
      "Epoch: 3 \t\t\t Iteration 1055 Loss Train: 0.5341784954071045\n",
      "Epoch: 3 \t\t\t Iteration 1056 Loss Train: 0.3292362093925476\n",
      "Epoch: 3 \t\t\t Iteration 1057 Loss Train: 0.4135872721672058\n",
      "Epoch: 3 \t\t\t Iteration 1058 Loss Train: 0.5257810354232788\n",
      "Epoch: 3 \t\t\t Iteration 1059 Loss Train: 0.41854846477508545\n",
      "Epoch: 3 \t\t\t Iteration 1060 Loss Train: 0.370757520198822\n",
      "Epoch: 3 \t\t\t Iteration 1061 Loss Train: 0.2179276943206787\n",
      "Epoch: 3 \t\t\t Iteration 1062 Loss Train: 0.5022876262664795\n",
      "Epoch: 3 \t\t\t Iteration 1063 Loss Train: 0.48960691690444946\n",
      "Epoch: 3 \t\t\t Iteration 1064 Loss Train: 0.299113392829895\n",
      "Epoch: 3 \t\t\t Iteration 1065 Loss Train: 0.4318801164627075\n",
      "Epoch: 3 \t\t\t Iteration 1066 Loss Train: 0.5653295516967773\n",
      "Epoch: 3 \t\t\t Iteration 1067 Loss Train: 0.4382483959197998\n",
      "Epoch: 3 \t\t\t Iteration 1068 Loss Train: 0.40339452028274536\n",
      "Epoch: 3 \t\t\t Iteration 1069 Loss Train: 0.29884499311447144\n",
      "Epoch: 3 \t\t\t Iteration 1070 Loss Train: 0.3285338282585144\n",
      "Epoch: 3 \t\t\t Iteration 1071 Loss Train: 0.2023133784532547\n",
      "Epoch: 3 \t\t\t Iteration 1072 Loss Train: 0.40438219904899597\n",
      "Epoch: 3 \t\t\t Iteration 1073 Loss Train: 0.4710383415222168\n",
      "Epoch: 3 \t\t\t Iteration 1074 Loss Train: 0.6797879934310913\n",
      "Epoch: 3 \t\t\t Iteration 1075 Loss Train: 0.35160937905311584\n",
      "Epoch: 3 \t\t\t Iteration 1076 Loss Train: 0.3578856289386749\n",
      "Epoch: 3 \t\t\t Iteration 1077 Loss Train: 0.3426860570907593\n",
      "Epoch: 3 \t\t\t Iteration 1078 Loss Train: 0.402987003326416\n",
      "Epoch: 3 \t\t\t Iteration 1079 Loss Train: 0.34988582134246826\n",
      "Epoch: 3 \t\t\t Iteration 1080 Loss Train: 0.2728757858276367\n",
      "Epoch: 3 \t\t\t Iteration 1081 Loss Train: 0.31329837441444397\n",
      "Epoch: 3 \t\t\t Iteration 1082 Loss Train: 0.2680615186691284\n",
      "Epoch: 3 \t\t\t Iteration 1083 Loss Train: 0.46443766355514526\n",
      "Epoch: 3 \t\t\t Iteration 1084 Loss Train: 0.409273624420166\n",
      "Epoch: 3 \t\t\t Iteration 1085 Loss Train: 0.5564405918121338\n",
      "Epoch: 3 \t\t\t Iteration 1086 Loss Train: 0.4902905821800232\n",
      "Epoch: 3 \t\t\t Iteration 1087 Loss Train: 0.4888837933540344\n",
      "Epoch: 3 \t\t\t Iteration 1088 Loss Train: 0.5881099104881287\n",
      "Epoch: 3 \t\t\t Iteration 1089 Loss Train: 0.5393904447555542\n",
      "Epoch: 3 \t\t\t Iteration 1090 Loss Train: 0.4320325553417206\n",
      "Epoch: 3 \t\t\t Iteration 1091 Loss Train: 0.4130701422691345\n",
      "Epoch: 3 \t\t\t Iteration 1092 Loss Train: 0.41353464126586914\n",
      "Epoch: 3 \t\t\t Iteration 1093 Loss Train: 0.4391823709011078\n",
      "Epoch: 3 \t\t\t Iteration 1094 Loss Train: 0.4698643386363983\n",
      "Epoch: 3 \t\t\t Iteration 1095 Loss Train: 0.305375874042511\n",
      "Epoch: 3 \t\t\t Iteration 1096 Loss Train: 0.39422839879989624\n",
      "Epoch: 3 \t\t\t Iteration 1097 Loss Train: 0.8007829189300537\n",
      "Epoch: 3 \t\t\t Iteration 1098 Loss Train: 0.5045763254165649\n",
      "Epoch: 3 \t\t\t Iteration 1099 Loss Train: 0.40406104922294617\n",
      "Epoch: 3 \t\t\t Iteration 1100 Loss Train: 0.4143809676170349\n",
      "Epoch: 3 \t\t\t Iteration 1101 Loss Train: 0.2906695604324341\n",
      "Epoch: 3 \t\t\t Iteration 1102 Loss Train: 0.32737454771995544\n",
      "Epoch: 3 \t\t\t Iteration 1103 Loss Train: 0.4554464817047119\n",
      "Epoch: 3 \t\t\t Iteration 1104 Loss Train: 0.42426401376724243\n",
      "Epoch: 3 \t\t\t Iteration 1105 Loss Train: 0.379084050655365\n",
      "Epoch: 3 \t\t\t Iteration 1106 Loss Train: 0.3882165253162384\n",
      "Epoch: 3 \t\t\t Iteration 1107 Loss Train: 0.4044276773929596\n",
      "Epoch: 3 \t\t\t Iteration 1108 Loss Train: 0.22750620543956757\n",
      "Epoch: 3 \t\t\t Iteration 1109 Loss Train: 0.4988427758216858\n",
      "Epoch: 3 \t\t\t Iteration 1110 Loss Train: 0.4145996868610382\n",
      "Epoch: 3 \t\t\t Iteration 1111 Loss Train: 0.5243836641311646\n",
      "Epoch: 3 \t\t\t Iteration 1112 Loss Train: 0.31406158208847046\n",
      "Epoch: 3 \t\t\t Iteration 1113 Loss Train: 0.4545696973800659\n",
      "Epoch: 3 \t\t\t Iteration 1114 Loss Train: 0.3203272819519043\n",
      "Epoch: 3 \t\t\t Iteration 1115 Loss Train: 0.3579574525356293\n",
      "Epoch: 3 \t\t\t Iteration 1116 Loss Train: 0.46763426065444946\n",
      "Epoch: 3 \t\t\t Iteration 1117 Loss Train: 0.45663103461265564\n",
      "Epoch: 3 \t\t\t Iteration 1118 Loss Train: 0.33162033557891846\n",
      "Epoch: 3 \t\t\t Iteration 1119 Loss Train: 0.35718321800231934\n",
      "Epoch: 3 \t\t\t Iteration 1120 Loss Train: 0.42011308670043945\n",
      "Epoch: 3 \t\t\t Iteration 1121 Loss Train: 0.3677259087562561\n",
      "Epoch: 3 \t\t\t Iteration 1122 Loss Train: 0.44865262508392334\n",
      "Epoch: 3 \t\t\t Iteration 1123 Loss Train: 0.4513753652572632\n",
      "Epoch: 3 \t\t\t Iteration 1124 Loss Train: 0.49904710054397583\n",
      "Epoch: 3 \t\t\t Iteration 1125 Loss Train: 0.4304254949092865\n",
      "Epoch: 3 \t\t\t Iteration 1126 Loss Train: 0.4361909031867981\n",
      "Epoch: 3 \t\t\t Iteration 1127 Loss Train: 0.40434780716896057\n",
      "Epoch: 3 \t\t\t Iteration 1128 Loss Train: 0.255699098110199\n",
      "Epoch: 3 \t\t\t Iteration 1129 Loss Train: 0.6236245036125183\n",
      "Epoch: 3 \t\t\t Iteration 1130 Loss Train: 0.2214522659778595\n",
      "Epoch: 3 \t\t\t Iteration 1131 Loss Train: 0.5233262181282043\n",
      "Epoch: 3 \t\t\t Iteration 1132 Loss Train: 0.37399473786354065\n",
      "Epoch: 3 \t\t\t Iteration 1133 Loss Train: 0.531767725944519\n",
      "Epoch: 3 \t\t\t Iteration 1134 Loss Train: 0.3210463225841522\n",
      "Epoch: 3 \t\t\t Iteration 1135 Loss Train: 0.25886160135269165\n",
      "Epoch: 3 \t\t\t Iteration 1136 Loss Train: 0.6029683947563171\n",
      "Epoch: 3 \t\t\t Iteration 1137 Loss Train: 0.40802818536758423\n",
      "Epoch: 3 \t\t\t Iteration 1138 Loss Train: 0.32786157727241516\n",
      "Epoch: 3 \t\t\t Iteration 1139 Loss Train: 0.32505306601524353\n",
      "Epoch: 3 \t\t\t Iteration 1140 Loss Train: 0.47638970613479614\n",
      "Epoch: 3 \t\t\t Iteration 1141 Loss Train: 0.5357867479324341\n",
      "Epoch: 3 \t\t\t Iteration 1142 Loss Train: 0.35492968559265137\n",
      "Epoch: 3 \t\t\t Iteration 1143 Loss Train: 0.3710162937641144\n",
      "Epoch: 3 \t\t\t Iteration 1144 Loss Train: 0.5205363035202026\n",
      "Epoch: 3 \t\t\t Iteration 1145 Loss Train: 0.4900584816932678\n",
      "Epoch: 3 \t\t\t Iteration 1146 Loss Train: 0.45353081822395325\n",
      "Epoch: 3 \t\t\t Iteration 1147 Loss Train: 0.408455491065979\n",
      "Epoch: 3 \t\t\t Iteration 1148 Loss Train: 0.3989887237548828\n",
      "Epoch: 3 \t\t\t Iteration 1149 Loss Train: 0.3753780126571655\n",
      "Epoch: 3 \t\t\t Iteration 1150 Loss Train: 0.369829386472702\n",
      "Epoch: 3 \t\t\t Iteration 1151 Loss Train: 0.5394788980484009\n",
      "Epoch: 3 \t\t\t Iteration 1152 Loss Train: 0.5169678330421448\n",
      "Epoch: 3 \t\t\t Iteration 1153 Loss Train: 0.28863081336021423\n",
      "Epoch: 3 \t\t\t Iteration 1154 Loss Train: 0.5724636316299438\n",
      "Epoch: 3 \t\t\t Iteration 1155 Loss Train: 0.3627874255180359\n",
      "Epoch: 3 \t\t\t Iteration 1156 Loss Train: 0.4214359521865845\n",
      "Epoch: 3 \t\t\t Iteration 1157 Loss Train: 0.4128810465335846\n",
      "Epoch: 3 \t\t\t Iteration 1158 Loss Train: 0.36241406202316284\n",
      "Epoch: 3 \t\t\t Iteration 1159 Loss Train: 0.31535863876342773\n",
      "Epoch: 3 \t\t\t Iteration 1160 Loss Train: 0.33237555623054504\n",
      "Epoch: 3 \t\t\t Iteration 1161 Loss Train: 0.22174200415611267\n",
      "Epoch: 3 \t\t\t Iteration 1162 Loss Train: 0.25677159428596497\n",
      "Epoch: 3 \t\t\t Iteration 1163 Loss Train: 0.17445756494998932\n",
      "Epoch: 3 \t\t\t Iteration 1164 Loss Train: 0.641453742980957\n",
      "Epoch: 3 \t\t\t Iteration 1165 Loss Train: 0.4417976438999176\n",
      "Epoch: 3 \t\t\t Iteration 1166 Loss Train: 0.33620595932006836\n",
      "Epoch: 3 \t\t\t Iteration 1167 Loss Train: 0.31815212965011597\n",
      "Epoch: 3 \t\t\t Iteration 1168 Loss Train: 0.4174964129924774\n",
      "Epoch: 3 \t\t\t Iteration 1169 Loss Train: 0.3697221577167511\n",
      "Epoch: 3 \t\t\t Iteration 1170 Loss Train: 0.5542171001434326\n",
      "Epoch: 3 \t\t\t Iteration 1171 Loss Train: 0.35628294944763184\n",
      "Epoch: 3 \t\t\t Iteration 1172 Loss Train: 0.4250229299068451\n",
      "Epoch: 3 \t\t\t Iteration 1173 Loss Train: 0.41087105870246887\n",
      "Epoch: 3 \t\t\t Iteration 1174 Loss Train: 0.3252543807029724\n",
      "Epoch: 3 \t\t\t Iteration 1175 Loss Train: 0.40257731080055237\n",
      "Epoch: 3 \t\t\t Iteration 1176 Loss Train: 0.4723273813724518\n",
      "Epoch: 3 \t\t\t Iteration 1177 Loss Train: 0.5816640853881836\n",
      "Epoch: 3 \t\t\t Iteration 1178 Loss Train: 0.4389982521533966\n",
      "Epoch: 3 \t\t\t Iteration 1179 Loss Train: 0.33638912439346313\n",
      "Epoch: 3 \t\t\t Iteration 1180 Loss Train: 0.32780158519744873\n",
      "Epoch: 3 \t\t\t Iteration 1181 Loss Train: 0.6748300790786743\n",
      "Epoch: 3 \t\t\t Iteration 1182 Loss Train: 0.3972404897212982\n",
      "Epoch: 3 \t\t\t Iteration 1183 Loss Train: 0.4961143136024475\n",
      "Epoch: 3 \t\t\t Iteration 1184 Loss Train: 0.4784051775932312\n",
      "Epoch: 3 \t\t\t Iteration 1185 Loss Train: 0.5266558527946472\n",
      "Epoch: 3 \t\t\t Iteration 1186 Loss Train: 0.26186972856521606\n",
      "Epoch: 3 \t\t\t Iteration 1187 Loss Train: 0.32315030694007874\n",
      "Epoch: 3 \t\t\t Iteration 1188 Loss Train: 0.4557252526283264\n",
      "Epoch: 3 \t\t\t Iteration 1189 Loss Train: 0.4634418487548828\n",
      "Epoch: 3 \t\t\t Iteration 1190 Loss Train: 0.3749459385871887\n",
      "Epoch: 3 \t\t\t Iteration 1191 Loss Train: 0.3555135428905487\n",
      "Epoch: 3 \t\t\t Iteration 1192 Loss Train: 0.3548567295074463\n",
      "Epoch: 3 \t\t\t Iteration 1193 Loss Train: 0.3977174758911133\n",
      "Epoch: 3 \t\t\t Iteration 1194 Loss Train: 0.465678334236145\n",
      "Epoch: 3 \t\t\t Iteration 1195 Loss Train: 0.43247297406196594\n",
      "Epoch: 3 \t\t\t Iteration 1196 Loss Train: 0.6858763694763184\n",
      "Epoch: 3 \t\t\t Iteration 1197 Loss Train: 0.3890330195426941\n",
      "Epoch: 3 \t\t\t Iteration 1198 Loss Train: 0.5705912113189697\n",
      "Epoch: 3 \t\t\t Iteration 1199 Loss Train: 0.3583289384841919\n",
      "Epoch: 3 \t\t\t Iteration 1200 Loss Train: 0.3915099501609802\n",
      "Epoch: 3 \t\t\t Iteration 1201 Loss Train: 0.5084690451622009\n",
      "Epoch: 3 \t\t\t Iteration 1202 Loss Train: 0.27457544207572937\n",
      "Epoch: 3 \t\t\t Iteration 1203 Loss Train: 0.4181860089302063\n",
      "Epoch: 3 \t\t\t Iteration 1204 Loss Train: 0.5171886682510376\n",
      "Epoch: 3 \t\t\t Iteration 1205 Loss Train: 0.43319615721702576\n",
      "Epoch: 3 \t\t\t Iteration 1206 Loss Train: 0.5121558904647827\n",
      "Epoch: 3 \t\t\t Iteration 1207 Loss Train: 0.49617284536361694\n",
      "Epoch: 3 \t\t\t Iteration 1208 Loss Train: 0.2980918288230896\n",
      "Epoch: 3 \t\t\t Iteration 1209 Loss Train: 0.3360367715358734\n",
      "Epoch: 3 \t\t\t Iteration 1210 Loss Train: 0.2613409757614136\n",
      "Epoch: 3 \t\t\t Iteration 1211 Loss Train: 0.6373275518417358\n",
      "Epoch: 3 \t\t\t Iteration 1212 Loss Train: 0.4501599967479706\n",
      "Epoch: 3 \t\t\t Iteration 1213 Loss Train: 0.3997516632080078\n",
      "Epoch: 3 \t\t\t Iteration 1214 Loss Train: 0.46905049681663513\n",
      "Epoch: 3 \t\t\t Iteration 1215 Loss Train: 0.36899831891059875\n",
      "Epoch: 3 \t\t\t Iteration 1216 Loss Train: 0.29982203245162964\n",
      "Epoch: 3 \t\t\t Iteration 1217 Loss Train: 0.59017014503479\n",
      "Epoch: 3 \t\t\t Iteration 1218 Loss Train: 0.5350112915039062\n",
      "Epoch: 3 \t\t\t Iteration 1219 Loss Train: 0.29588016867637634\n",
      "Epoch: 3 \t\t\t Iteration 1220 Loss Train: 0.5505772829055786\n",
      "Epoch: 3 \t\t\t Iteration 1221 Loss Train: 0.3333531618118286\n",
      "Epoch: 3 \t\t\t Iteration 1222 Loss Train: 0.2835257649421692\n",
      "Epoch: 3 \t\t\t Iteration 1223 Loss Train: 0.4195476174354553\n",
      "Epoch: 3 \t\t\t Iteration 1224 Loss Train: 0.36318540573120117\n",
      "Epoch: 3 \t\t\t Iteration 1225 Loss Train: 0.2968047857284546\n",
      "Epoch: 3 \t\t\t Iteration 1226 Loss Train: 0.49450385570526123\n",
      "Epoch: 3 \t\t\t Iteration 1227 Loss Train: 0.4224873185157776\n",
      "Epoch: 3 \t\t\t Iteration 1228 Loss Train: 0.40923580527305603\n",
      "Epoch: 3 \t\t\t Iteration 1229 Loss Train: 0.5217742919921875\n",
      "Epoch: 3 \t\t\t Iteration 1230 Loss Train: 0.3286951780319214\n",
      "Epoch: 3 \t\t\t Iteration 1231 Loss Train: 0.32680949568748474\n",
      "Epoch: 3 \t\t\t Iteration 1232 Loss Train: 0.4454435408115387\n",
      "Epoch: 3 \t\t\t Iteration 1233 Loss Train: 0.29662376642227173\n",
      "Epoch: 3 \t\t\t Iteration 1234 Loss Train: 0.5028002262115479\n",
      "Epoch: 3 \t\t\t Iteration 1235 Loss Train: 0.34677451848983765\n",
      "Epoch: 3 \t\t\t Iteration 1236 Loss Train: 0.36569058895111084\n",
      "Epoch: 3 \t\t\t Iteration 1237 Loss Train: 0.37537500262260437\n",
      "Epoch: 3 \t\t\t Iteration 1238 Loss Train: 0.6637677550315857\n",
      "Epoch: 3 \t\t\t Iteration 1239 Loss Train: 0.5490002036094666\n",
      "Epoch: 3 \t\t\t Iteration 1240 Loss Train: 0.46292373538017273\n",
      "Epoch: 3 \t\t\t Iteration 1241 Loss Train: 0.3605538606643677\n",
      "Epoch: 3 \t\t\t Iteration 1242 Loss Train: 0.4179508090019226\n",
      "Epoch: 3 \t\t\t Iteration 1243 Loss Train: 0.38372623920440674\n",
      "Epoch: 3 \t\t\t Iteration 1244 Loss Train: 0.5206232666969299\n",
      "Epoch: 3 \t\t\t Iteration 1245 Loss Train: 0.19742679595947266\n",
      "Epoch: 3 \t\t\t Iteration 1246 Loss Train: 0.44598037004470825\n",
      "Epoch: 3 \t\t\t Iteration 1247 Loss Train: 0.3535817861557007\n",
      "Epoch: 3 \t\t\t Iteration 1248 Loss Train: 0.21544687449932098\n",
      "Epoch: 3 \t\t\t Iteration 1249 Loss Train: 0.6803410649299622\n",
      "Epoch: 3 \t\t\t Iteration 1250 Loss Train: 0.4165976047515869\n",
      "Epoch: 3 \t\t\t Iteration 1251 Loss Train: 0.38359546661376953\n",
      "Epoch: 3 \t\t\t Iteration 1252 Loss Train: 0.4734523594379425\n",
      "Epoch: 3 \t\t\t Iteration 1253 Loss Train: 0.3872107267379761\n",
      "Epoch: 3 \t\t\t Iteration 1254 Loss Train: 0.43288516998291016\n",
      "Epoch: 3 \t\t\t Iteration 1255 Loss Train: 0.267677903175354\n",
      "Epoch: 3 \t\t\t Iteration 1256 Loss Train: 0.3811405301094055\n",
      "Epoch: 3 \t\t\t Iteration 1257 Loss Train: 0.40876883268356323\n",
      "Epoch: 3 \t\t\t Iteration 1258 Loss Train: 0.35548385977745056\n",
      "Epoch: 3 \t\t\t Iteration 1259 Loss Train: 0.4987424910068512\n",
      "Epoch: 3 \t\t\t Iteration 1260 Loss Train: 0.3980020582675934\n",
      "Epoch: 3 \t\t\t Iteration 1261 Loss Train: 0.16408318281173706\n",
      "Epoch: 3 \t\t\t Iteration 1262 Loss Train: 0.2736823856830597\n",
      "Epoch: 3 \t\t\t Iteration 1263 Loss Train: 0.4206923246383667\n",
      "Epoch: 3 \t\t\t Iteration 1264 Loss Train: 0.4271910488605499\n",
      "Epoch: 3 \t\t\t Iteration 1265 Loss Train: 0.4716895520687103\n",
      "Epoch: 3 \t\t\t Iteration 1266 Loss Train: 0.5525693893432617\n",
      "Epoch: 3 \t\t\t Iteration 1267 Loss Train: 0.4036116600036621\n",
      "Epoch: 3 \t\t\t Iteration 1268 Loss Train: 0.3254646360874176\n",
      "Epoch: 3 \t\t\t Iteration 1269 Loss Train: 0.3597925901412964\n",
      "Epoch: 3 \t\t\t Iteration 1270 Loss Train: 0.45261991024017334\n",
      "Epoch: 3 \t\t\t Iteration 1271 Loss Train: 0.3808738887310028\n",
      "Epoch: 3 \t\t\t Iteration 1272 Loss Train: 0.26275771856307983\n",
      "Epoch: 3 \t\t\t Iteration 1273 Loss Train: 0.39283910393714905\n",
      "Epoch: 3 \t\t\t Iteration 1274 Loss Train: 0.34020429849624634\n",
      "Epoch: 3 \t\t\t Iteration 1275 Loss Train: 0.3352392315864563\n",
      "Epoch: 3 \t\t\t Iteration 1276 Loss Train: 0.4410516321659088\n",
      "Epoch: 3 \t\t\t Iteration 1277 Loss Train: 0.3670594096183777\n",
      "Epoch: 3 \t\t\t Iteration 1278 Loss Train: 0.35304903984069824\n",
      "Epoch: 3 \t\t\t Iteration 1279 Loss Train: 0.356799453496933\n",
      "Epoch: 3 \t\t\t Iteration 1280 Loss Train: 0.35819852352142334\n",
      "Epoch: 3 \t\t\t Iteration 1281 Loss Train: 0.39306914806365967\n",
      "Epoch: 3 \t\t\t Iteration 1282 Loss Train: 0.22749920189380646\n",
      "Epoch: 3 \t\t\t Iteration 1283 Loss Train: 0.33089977502822876\n",
      "Epoch: 3 \t\t\t Iteration 1284 Loss Train: 0.5893939733505249\n",
      "Epoch: 3 \t\t\t Iteration 1285 Loss Train: 0.37500810623168945\n",
      "Epoch: 3 \t\t\t Iteration 1286 Loss Train: 0.5654214024543762\n",
      "Epoch: 3 \t\t\t Iteration 1287 Loss Train: 0.43455958366394043\n",
      "Epoch: 3 \t\t\t Iteration 1288 Loss Train: 0.37270215153694153\n",
      "Epoch: 3 \t\t\t Iteration 1289 Loss Train: 0.3023702800273895\n",
      "Epoch: 3 \t\t\t Iteration 1290 Loss Train: 0.4026188254356384\n",
      "Epoch: 3 \t\t\t Iteration 1291 Loss Train: 0.595837414264679\n",
      "Epoch: 3 \t\t\t Iteration 1292 Loss Train: 0.368622362613678\n",
      "Epoch: 3 \t\t\t Iteration 1293 Loss Train: 0.4184900224208832\n",
      "Epoch: 3 \t\t\t Iteration 1294 Loss Train: 0.35807105898857117\n",
      "Epoch: 3 \t\t\t Iteration 1295 Loss Train: 0.39469826221466064\n",
      "Epoch: 3 \t\t\t Iteration 1296 Loss Train: 0.3233884871006012\n",
      "Epoch: 3 \t\t\t Iteration 1297 Loss Train: 0.432117223739624\n",
      "Epoch: 3 \t\t\t Iteration 1298 Loss Train: 0.4336756765842438\n",
      "Epoch: 3 \t\t\t Iteration 1299 Loss Train: 0.6920583248138428\n",
      "Epoch: 3 \t\t\t Iteration 1300 Loss Train: 0.45666396617889404\n",
      "Epoch: 3 \t\t\t Iteration 1301 Loss Train: 0.45627957582473755\n",
      "Epoch: 3 \t\t\t Iteration 1302 Loss Train: 0.5571357011795044\n",
      "Epoch: 3 \t\t\t Iteration 1303 Loss Train: 0.3896469473838806\n",
      "Epoch: 3 \t\t\t Iteration 1304 Loss Train: 0.27092230319976807\n",
      "Epoch: 3 \t\t\t Iteration 1305 Loss Train: 0.28644096851348877\n",
      "Epoch: 3 \t\t\t Iteration 1306 Loss Train: 0.31965065002441406\n",
      "Epoch: 3 \t\t\t Iteration 1307 Loss Train: 0.373324990272522\n",
      "Epoch: 3 \t\t\t Iteration 1308 Loss Train: 0.34737884998321533\n",
      "Epoch: 3 \t\t\t Iteration 1309 Loss Train: 0.5775450468063354\n",
      "Epoch: 3 \t\t\t Iteration 1310 Loss Train: 0.7269768714904785\n",
      "Epoch: 3 \t\t\t Iteration 1311 Loss Train: 0.3601107597351074\n",
      "Epoch: 3 \t\t\t Iteration 1312 Loss Train: 0.3708302974700928\n",
      "Epoch: 3 \t\t\t Iteration 1313 Loss Train: 0.7377082109451294\n",
      "Epoch: 3 \t\t\t Iteration 1314 Loss Train: 0.32253050804138184\n",
      "Epoch: 3 \t\t\t Iteration 1315 Loss Train: 0.38183528184890747\n",
      "Epoch: 3 \t\t\t Iteration 1316 Loss Train: 0.4424220323562622\n",
      "Epoch: 3 \t\t\t Iteration 1317 Loss Train: 0.34322047233581543\n",
      "Epoch: 3 \t\t\t Iteration 1318 Loss Train: 0.5488849878311157\n",
      "Epoch: 3 \t\t\t Iteration 1319 Loss Train: 0.5352655649185181\n",
      "Epoch: 3 \t\t\t Iteration 1320 Loss Train: 0.33521324396133423\n",
      "Epoch: 3 \t\t\t Iteration 1321 Loss Train: 0.40403181314468384\n",
      "Epoch: 3 \t\t\t Iteration 1322 Loss Train: 0.44272178411483765\n",
      "Epoch: 3 \t\t\t Iteration 1323 Loss Train: 0.2751674950122833\n",
      "Epoch: 3 \t\t\t Iteration 1324 Loss Train: 0.4603637456893921\n",
      "Epoch: 3 \t\t\t Iteration 1325 Loss Train: 0.5620912313461304\n",
      "Epoch: 3 \t\t\t Iteration 1326 Loss Train: 0.44519323110580444\n",
      "Epoch: 3 \t\t\t Iteration 1327 Loss Train: 0.45587998628616333\n",
      "Epoch: 3 \t\t\t Iteration 1328 Loss Train: 0.4729645848274231\n",
      "Epoch: 3 \t\t\t Iteration 1329 Loss Train: 0.4612794816493988\n",
      "Epoch: 3 \t\t\t Iteration 1330 Loss Train: 0.38199296593666077\n",
      "Epoch: 3 \t\t\t Iteration 1331 Loss Train: 0.4155105948448181\n",
      "Epoch: 3 \t\t\t Iteration 1332 Loss Train: 0.49974551796913147\n",
      "Epoch: 3 \t\t\t Iteration 1333 Loss Train: 0.45950236916542053\n",
      "Epoch: 3 \t\t\t Iteration 1334 Loss Train: 0.4169013202190399\n",
      "Epoch: 3 \t\t\t Iteration 1335 Loss Train: 0.3313462734222412\n",
      "Epoch: 3 \t\t\t Iteration 1336 Loss Train: 0.1878531277179718\n",
      "Epoch: 3 \t\t\t Iteration 1337 Loss Train: 0.7539749145507812\n",
      "Epoch: 3 \t\t\t Iteration 1338 Loss Train: 0.4791470468044281\n",
      "Epoch: 3 \t\t\t Iteration 1339 Loss Train: 0.4293302297592163\n",
      "Epoch: 3 \t\t\t Iteration 1340 Loss Train: 0.5512241721153259\n",
      "Epoch: 3 \t\t\t Iteration 1341 Loss Train: 0.4241994023323059\n",
      "Epoch: 3 \t\t\t Iteration 1342 Loss Train: 0.36926472187042236\n",
      "Epoch: 3 \t\t\t Iteration 1343 Loss Train: 0.31958243250846863\n",
      "Epoch: 3 \t\t\t Iteration 1344 Loss Train: 0.1760822981595993\n",
      "Epoch: 3 \t\t\t Iteration 1345 Loss Train: 0.4010370969772339\n",
      "Epoch: 3 \t\t\t Iteration 1346 Loss Train: 0.47302696108818054\n",
      "Epoch: 3 \t\t\t Iteration 1347 Loss Train: 0.47697019577026367\n",
      "Epoch: 3 \t\t\t Iteration 1348 Loss Train: 0.5710145831108093\n",
      "Epoch: 3 \t\t\t Iteration 1349 Loss Train: 0.45430442690849304\n",
      "Epoch: 3 \t\t\t Iteration 1350 Loss Train: 0.2666214108467102\n",
      "Epoch: 3 \t\t\t Iteration 1351 Loss Train: 0.46990931034088135\n",
      "Epoch: 3 \t\t\t Iteration 1352 Loss Train: 0.29891663789749146\n",
      "Epoch: 3 \t\t\t Iteration 1353 Loss Train: 0.4540210962295532\n",
      "Epoch: 3 \t\t\t Iteration 1354 Loss Train: 0.5012756586074829\n",
      "Epoch: 3 \t\t\t Iteration 1355 Loss Train: 0.507032036781311\n",
      "Epoch: 3 \t\t\t Iteration 1356 Loss Train: 0.4761378765106201\n",
      "Epoch: 3 \t\t\t Iteration 1357 Loss Train: 0.32210198044776917\n",
      "Epoch: 3 \t\t\t Iteration 1358 Loss Train: 0.44860953092575073\n",
      "Epoch: 3 \t\t\t Iteration 1359 Loss Train: 0.41978850960731506\n",
      "Epoch: 3 \t\t\t Iteration 1360 Loss Train: 0.44868916273117065\n",
      "Epoch: 3 \t\t\t Iteration 1361 Loss Train: 0.2904468774795532\n",
      "Epoch: 3 \t\t\t Iteration 1362 Loss Train: 0.5780521631240845\n",
      "Epoch: 3 \t\t\t Iteration 1363 Loss Train: 0.2750110924243927\n",
      "Epoch: 3 \t\t\t Iteration 1364 Loss Train: 0.3033924102783203\n",
      "Epoch: 3 \t\t\t Iteration 1365 Loss Train: 0.23846927285194397\n",
      "Epoch: 3 \t\t\t Iteration 1366 Loss Train: 0.731812059879303\n",
      "Epoch: 3 \t\t\t Iteration 1367 Loss Train: 0.41482454538345337\n",
      "Epoch: 3 \t\t\t Iteration 1368 Loss Train: 0.37648510932922363\n",
      "Epoch: 3 \t\t\t Iteration 1369 Loss Train: 0.40052530169487\n",
      "Epoch: 3 \t\t\t Iteration 1370 Loss Train: 0.44387131929397583\n",
      "Epoch: 3 \t\t\t Iteration 1371 Loss Train: 0.3877033591270447\n",
      "Epoch: 3 \t\t\t Iteration 1372 Loss Train: 0.41333866119384766\n",
      "Epoch: 3 \t\t\t Iteration 1373 Loss Train: 0.39325979351997375\n",
      "Epoch: 3 \t\t\t Iteration 1374 Loss Train: 0.2918413281440735\n",
      "Epoch: 3 \t\t\t Iteration 1375 Loss Train: 0.2355043739080429\n",
      "Epoch: 3 \t\t\t Iteration 1376 Loss Train: 0.730133593082428\n",
      "Epoch: 3 \t\t\t Iteration 1377 Loss Train: 0.3857470154762268\n",
      "Epoch: 3 \t\t\t Iteration 1378 Loss Train: 0.3744790554046631\n",
      "Epoch: 3 \t\t\t Iteration 1379 Loss Train: 0.28986236453056335\n",
      "Epoch: 3 \t\t\t Iteration 1380 Loss Train: 0.32802242040634155\n",
      "Epoch: 3 \t\t\t Iteration 1381 Loss Train: 0.46763306856155396\n",
      "Epoch: 3 \t\t\t Iteration 1382 Loss Train: 0.25783222913742065\n",
      "Epoch: 3 \t\t\t Iteration 1383 Loss Train: 0.6764633655548096\n",
      "Epoch: 3 \t\t\t Iteration 1384 Loss Train: 0.33071985840797424\n",
      "Epoch: 3 \t\t\t Iteration 1385 Loss Train: 0.42109552025794983\n",
      "Epoch: 3 \t\t\t Iteration 1386 Loss Train: 0.41611090302467346\n",
      "Epoch: 3 \t\t\t Iteration 1387 Loss Train: 0.6032326221466064\n",
      "Epoch: 3 \t\t\t Iteration 1388 Loss Train: 0.3768506348133087\n",
      "Epoch: 3 \t\t\t Iteration 1389 Loss Train: 0.3589900732040405\n",
      "Epoch: 3 \t\t\t Iteration 1390 Loss Train: 0.28921636939048767\n",
      "Epoch: 3 \t\t\t Iteration 1391 Loss Train: 0.40676793456077576\n",
      "Epoch: 3 \t\t\t Iteration 1392 Loss Train: 0.36050131916999817\n",
      "Epoch: 3 \t\t\t Iteration 1393 Loss Train: 0.3342442810535431\n",
      "Epoch: 3 \t\t\t Iteration 1394 Loss Train: 0.2998751699924469\n",
      "Epoch: 3 \t\t\t Iteration 1395 Loss Train: 0.780833899974823\n",
      "Epoch: 3 / 5 \t\t\t Training Loss:0.42456891448587497\n",
      "Epoch: 4 \t\t\t Iteration 1 Loss Train: 0.8615245223045349\n",
      "Epoch: 4 \t\t\t Iteration 2 Loss Train: 0.4915933609008789\n",
      "Epoch: 4 \t\t\t Iteration 3 Loss Train: 0.898280680179596\n",
      "Epoch: 4 \t\t\t Iteration 4 Loss Train: 0.40602707862854004\n",
      "Epoch: 4 \t\t\t Iteration 5 Loss Train: 0.50887131690979\n",
      "Epoch: 4 \t\t\t Iteration 6 Loss Train: 0.45809367299079895\n",
      "Epoch: 4 \t\t\t Iteration 7 Loss Train: 0.3330124020576477\n",
      "Epoch: 4 \t\t\t Iteration 8 Loss Train: 0.2212218940258026\n",
      "Epoch: 4 \t\t\t Iteration 9 Loss Train: 0.5774526596069336\n",
      "Epoch: 4 \t\t\t Iteration 10 Loss Train: 0.3289031386375427\n",
      "Epoch: 4 \t\t\t Iteration 11 Loss Train: 0.3557441830635071\n",
      "Epoch: 4 \t\t\t Iteration 12 Loss Train: 0.1654772311449051\n",
      "Epoch: 4 \t\t\t Iteration 13 Loss Train: 0.5501580834388733\n",
      "Epoch: 4 \t\t\t Iteration 14 Loss Train: 0.3978137969970703\n",
      "Epoch: 4 \t\t\t Iteration 15 Loss Train: 0.3636018633842468\n",
      "Epoch: 4 \t\t\t Iteration 16 Loss Train: 0.2405361384153366\n",
      "Epoch: 4 \t\t\t Iteration 17 Loss Train: 0.23285362124443054\n",
      "Epoch: 4 \t\t\t Iteration 18 Loss Train: 0.39279666543006897\n",
      "Epoch: 4 \t\t\t Iteration 19 Loss Train: 0.3089527189731598\n",
      "Epoch: 4 \t\t\t Iteration 20 Loss Train: 0.3801334500312805\n",
      "Epoch: 4 \t\t\t Iteration 21 Loss Train: 0.4149242639541626\n",
      "Epoch: 4 \t\t\t Iteration 22 Loss Train: 0.4029412269592285\n",
      "Epoch: 4 \t\t\t Iteration 23 Loss Train: 0.30973929166793823\n",
      "Epoch: 4 \t\t\t Iteration 24 Loss Train: 0.4102727174758911\n",
      "Epoch: 4 \t\t\t Iteration 25 Loss Train: 0.3964439630508423\n",
      "Epoch: 4 \t\t\t Iteration 26 Loss Train: 0.4067586362361908\n",
      "Epoch: 4 \t\t\t Iteration 27 Loss Train: 0.9169945120811462\n",
      "Epoch: 4 \t\t\t Iteration 28 Loss Train: 0.5087774991989136\n",
      "Epoch: 4 \t\t\t Iteration 29 Loss Train: 0.418094664812088\n",
      "Epoch: 4 \t\t\t Iteration 30 Loss Train: 0.36106884479522705\n",
      "Epoch: 4 \t\t\t Iteration 31 Loss Train: 0.6398748159408569\n",
      "Epoch: 4 \t\t\t Iteration 32 Loss Train: 0.3871457278728485\n",
      "Epoch: 4 \t\t\t Iteration 33 Loss Train: 0.41881364583969116\n",
      "Epoch: 4 \t\t\t Iteration 34 Loss Train: 0.38165247440338135\n",
      "Epoch: 4 \t\t\t Iteration 35 Loss Train: 0.4692929983139038\n",
      "Epoch: 4 \t\t\t Iteration 36 Loss Train: 0.45640307664871216\n",
      "Epoch: 4 \t\t\t Iteration 37 Loss Train: 0.36663904786109924\n",
      "Epoch: 4 \t\t\t Iteration 38 Loss Train: 0.45729827880859375\n",
      "Epoch: 4 \t\t\t Iteration 39 Loss Train: 0.4552883505821228\n",
      "Epoch: 4 \t\t\t Iteration 40 Loss Train: 0.3886401653289795\n",
      "Epoch: 4 \t\t\t Iteration 41 Loss Train: 0.36247146129608154\n",
      "Epoch: 4 \t\t\t Iteration 42 Loss Train: 0.39684396982192993\n",
      "Epoch: 4 \t\t\t Iteration 43 Loss Train: 0.4907258450984955\n",
      "Epoch: 4 \t\t\t Iteration 44 Loss Train: 0.4177646040916443\n",
      "Epoch: 4 \t\t\t Iteration 45 Loss Train: 0.49743813276290894\n",
      "Epoch: 4 \t\t\t Iteration 46 Loss Train: 0.35956674814224243\n",
      "Epoch: 4 \t\t\t Iteration 47 Loss Train: 0.2747494578361511\n",
      "Epoch: 4 \t\t\t Iteration 48 Loss Train: 0.6460110545158386\n",
      "Epoch: 4 \t\t\t Iteration 49 Loss Train: 0.3890031576156616\n",
      "Epoch: 4 \t\t\t Iteration 50 Loss Train: 0.5064952373504639\n",
      "Epoch: 4 \t\t\t Iteration 51 Loss Train: 0.4207848906517029\n",
      "Epoch: 4 \t\t\t Iteration 52 Loss Train: 0.3802773356437683\n",
      "Epoch: 4 \t\t\t Iteration 53 Loss Train: 0.394347608089447\n",
      "Epoch: 4 \t\t\t Iteration 54 Loss Train: 0.5635005831718445\n",
      "Epoch: 4 \t\t\t Iteration 55 Loss Train: 0.33253729343414307\n",
      "Epoch: 4 \t\t\t Iteration 56 Loss Train: 0.278486967086792\n",
      "Epoch: 4 \t\t\t Iteration 57 Loss Train: 0.47929221391677856\n",
      "Epoch: 4 \t\t\t Iteration 58 Loss Train: 0.4296956956386566\n",
      "Epoch: 4 \t\t\t Iteration 59 Loss Train: 0.2806161344051361\n",
      "Epoch: 4 \t\t\t Iteration 60 Loss Train: 0.4066303074359894\n",
      "Epoch: 4 \t\t\t Iteration 61 Loss Train: 0.30609825253486633\n",
      "Epoch: 4 \t\t\t Iteration 62 Loss Train: 0.41608837246894836\n",
      "Epoch: 4 \t\t\t Iteration 63 Loss Train: 0.36945316195487976\n",
      "Epoch: 4 \t\t\t Iteration 64 Loss Train: 0.6204715967178345\n",
      "Epoch: 4 \t\t\t Iteration 65 Loss Train: 0.38493239879608154\n",
      "Epoch: 4 \t\t\t Iteration 66 Loss Train: 0.405112624168396\n",
      "Epoch: 4 \t\t\t Iteration 67 Loss Train: 0.6013705730438232\n",
      "Epoch: 4 \t\t\t Iteration 68 Loss Train: 0.44690006971359253\n",
      "Epoch: 4 \t\t\t Iteration 69 Loss Train: 0.5658310055732727\n",
      "Epoch: 4 \t\t\t Iteration 70 Loss Train: 0.31707632541656494\n",
      "Epoch: 4 \t\t\t Iteration 71 Loss Train: 0.5678703784942627\n",
      "Epoch: 4 \t\t\t Iteration 72 Loss Train: 0.442963182926178\n",
      "Epoch: 4 \t\t\t Iteration 73 Loss Train: 0.49395403265953064\n",
      "Epoch: 4 \t\t\t Iteration 74 Loss Train: 0.394536554813385\n",
      "Epoch: 4 \t\t\t Iteration 75 Loss Train: 0.3714737296104431\n",
      "Epoch: 4 \t\t\t Iteration 76 Loss Train: 0.3865697979927063\n",
      "Epoch: 4 \t\t\t Iteration 77 Loss Train: 0.5573784708976746\n",
      "Epoch: 4 \t\t\t Iteration 78 Loss Train: 0.4216505289077759\n",
      "Epoch: 4 \t\t\t Iteration 79 Loss Train: 0.363139271736145\n",
      "Epoch: 4 \t\t\t Iteration 80 Loss Train: 0.3342563509941101\n",
      "Epoch: 4 \t\t\t Iteration 81 Loss Train: 0.5985612869262695\n",
      "Epoch: 4 \t\t\t Iteration 82 Loss Train: 0.5068035125732422\n",
      "Epoch: 4 \t\t\t Iteration 83 Loss Train: 0.37120094895362854\n",
      "Epoch: 4 \t\t\t Iteration 84 Loss Train: 0.3164360225200653\n",
      "Epoch: 4 \t\t\t Iteration 85 Loss Train: 0.18677082657814026\n",
      "Epoch: 4 \t\t\t Iteration 86 Loss Train: 0.35391002893447876\n",
      "Epoch: 4 \t\t\t Iteration 87 Loss Train: 0.480354905128479\n",
      "Epoch: 4 \t\t\t Iteration 88 Loss Train: 0.5016952753067017\n",
      "Epoch: 4 \t\t\t Iteration 89 Loss Train: 0.40368813276290894\n",
      "Epoch: 4 \t\t\t Iteration 90 Loss Train: 0.4440400302410126\n",
      "Epoch: 4 \t\t\t Iteration 91 Loss Train: 0.39171814918518066\n",
      "Epoch: 4 \t\t\t Iteration 92 Loss Train: 0.44192230701446533\n",
      "Epoch: 4 \t\t\t Iteration 93 Loss Train: 0.336169958114624\n",
      "Epoch: 4 \t\t\t Iteration 94 Loss Train: 0.5450513362884521\n",
      "Epoch: 4 \t\t\t Iteration 95 Loss Train: 0.4467760920524597\n",
      "Epoch: 4 \t\t\t Iteration 96 Loss Train: 0.5388208627700806\n",
      "Epoch: 4 \t\t\t Iteration 97 Loss Train: 0.4939797520637512\n",
      "Epoch: 4 \t\t\t Iteration 98 Loss Train: 0.589194118976593\n",
      "Epoch: 4 \t\t\t Iteration 99 Loss Train: 0.5850767493247986\n",
      "Epoch: 4 \t\t\t Iteration 100 Loss Train: 0.36167123913764954\n",
      "Epoch: 4 \t\t\t Iteration 101 Loss Train: 0.4207519292831421\n",
      "Epoch: 4 \t\t\t Iteration 102 Loss Train: 0.36810895800590515\n",
      "Epoch: 4 \t\t\t Iteration 103 Loss Train: 0.32375824451446533\n",
      "Epoch: 4 \t\t\t Iteration 104 Loss Train: 0.3991817831993103\n",
      "Epoch: 4 \t\t\t Iteration 105 Loss Train: 0.4773271381855011\n",
      "Epoch: 4 \t\t\t Iteration 106 Loss Train: 0.43592071533203125\n",
      "Epoch: 4 \t\t\t Iteration 107 Loss Train: 0.5438694357872009\n",
      "Epoch: 4 \t\t\t Iteration 108 Loss Train: 0.4481431543827057\n",
      "Epoch: 4 \t\t\t Iteration 109 Loss Train: 0.29600751399993896\n",
      "Epoch: 4 \t\t\t Iteration 110 Loss Train: 0.23429635167121887\n",
      "Epoch: 4 \t\t\t Iteration 111 Loss Train: 0.46391260623931885\n",
      "Epoch: 4 \t\t\t Iteration 112 Loss Train: 0.43369966745376587\n",
      "Epoch: 4 \t\t\t Iteration 113 Loss Train: 0.4735487401485443\n",
      "Epoch: 4 \t\t\t Iteration 114 Loss Train: 0.3974596858024597\n",
      "Epoch: 4 \t\t\t Iteration 115 Loss Train: 0.522068977355957\n",
      "Epoch: 4 \t\t\t Iteration 116 Loss Train: 0.45449674129486084\n",
      "Epoch: 4 \t\t\t Iteration 117 Loss Train: 0.34070736169815063\n",
      "Epoch: 4 \t\t\t Iteration 118 Loss Train: 0.2320256382226944\n",
      "Epoch: 4 \t\t\t Iteration 119 Loss Train: 0.2978499233722687\n",
      "Epoch: 4 \t\t\t Iteration 120 Loss Train: 0.23905494809150696\n",
      "Epoch: 4 \t\t\t Iteration 121 Loss Train: 0.43743443489074707\n",
      "Epoch: 4 \t\t\t Iteration 122 Loss Train: 0.4360077977180481\n",
      "Epoch: 4 \t\t\t Iteration 123 Loss Train: 0.5263869762420654\n",
      "Epoch: 4 \t\t\t Iteration 124 Loss Train: 0.36533570289611816\n",
      "Epoch: 4 \t\t\t Iteration 125 Loss Train: 0.3544169068336487\n",
      "Epoch: 4 \t\t\t Iteration 126 Loss Train: 0.22026565670967102\n",
      "Epoch: 4 \t\t\t Iteration 127 Loss Train: 0.5069922208786011\n",
      "Epoch: 4 \t\t\t Iteration 128 Loss Train: 0.5109541416168213\n",
      "Epoch: 4 \t\t\t Iteration 129 Loss Train: 0.4444063901901245\n",
      "Epoch: 4 \t\t\t Iteration 130 Loss Train: 0.2677014172077179\n",
      "Epoch: 4 \t\t\t Iteration 131 Loss Train: 0.4729343354701996\n",
      "Epoch: 4 \t\t\t Iteration 132 Loss Train: 0.3542383909225464\n",
      "Epoch: 4 \t\t\t Iteration 133 Loss Train: 0.28430280089378357\n",
      "Epoch: 4 \t\t\t Iteration 134 Loss Train: 0.5495539307594299\n",
      "Epoch: 4 \t\t\t Iteration 135 Loss Train: 0.4046492874622345\n",
      "Epoch: 4 \t\t\t Iteration 136 Loss Train: 0.6062673330307007\n",
      "Epoch: 4 \t\t\t Iteration 137 Loss Train: 0.42837250232696533\n",
      "Epoch: 4 \t\t\t Iteration 138 Loss Train: 0.46934565901756287\n",
      "Epoch: 4 \t\t\t Iteration 139 Loss Train: 0.3546905517578125\n",
      "Epoch: 4 \t\t\t Iteration 140 Loss Train: 0.27750059962272644\n",
      "Epoch: 4 \t\t\t Iteration 141 Loss Train: 0.5686829686164856\n",
      "Epoch: 4 \t\t\t Iteration 142 Loss Train: 0.42687469720840454\n",
      "Epoch: 4 \t\t\t Iteration 143 Loss Train: 0.413118839263916\n",
      "Epoch: 4 \t\t\t Iteration 144 Loss Train: 0.4091399312019348\n",
      "Epoch: 4 \t\t\t Iteration 145 Loss Train: 0.44239342212677\n",
      "Epoch: 4 \t\t\t Iteration 146 Loss Train: 0.36237549781799316\n",
      "Epoch: 4 \t\t\t Iteration 147 Loss Train: 0.32680487632751465\n",
      "Epoch: 4 \t\t\t Iteration 148 Loss Train: 0.40879368782043457\n",
      "Epoch: 4 \t\t\t Iteration 149 Loss Train: 0.49977701902389526\n",
      "Epoch: 4 \t\t\t Iteration 150 Loss Train: 0.3849995732307434\n",
      "Epoch: 4 \t\t\t Iteration 151 Loss Train: 0.39677894115448\n",
      "Epoch: 4 \t\t\t Iteration 152 Loss Train: 0.4354930520057678\n",
      "Epoch: 4 \t\t\t Iteration 153 Loss Train: 0.42630285024642944\n",
      "Epoch: 4 \t\t\t Iteration 154 Loss Train: 0.35448020696640015\n",
      "Epoch: 4 \t\t\t Iteration 155 Loss Train: 0.3736085891723633\n",
      "Epoch: 4 \t\t\t Iteration 156 Loss Train: 0.42336174845695496\n",
      "Epoch: 4 \t\t\t Iteration 157 Loss Train: 0.37633776664733887\n",
      "Epoch: 4 \t\t\t Iteration 158 Loss Train: 0.5055181384086609\n",
      "Epoch: 4 \t\t\t Iteration 159 Loss Train: 0.3593928813934326\n",
      "Epoch: 4 \t\t\t Iteration 160 Loss Train: 0.20214004814624786\n",
      "Epoch: 4 \t\t\t Iteration 161 Loss Train: 0.8051223158836365\n",
      "Epoch: 4 \t\t\t Iteration 162 Loss Train: 0.44322657585144043\n",
      "Epoch: 4 \t\t\t Iteration 163 Loss Train: 0.3710857927799225\n",
      "Epoch: 4 \t\t\t Iteration 164 Loss Train: 0.3991807997226715\n",
      "Epoch: 4 \t\t\t Iteration 165 Loss Train: 0.47938668727874756\n",
      "Epoch: 4 \t\t\t Iteration 166 Loss Train: 0.4012223482131958\n",
      "Epoch: 4 \t\t\t Iteration 167 Loss Train: 0.4891118109226227\n",
      "Epoch: 4 \t\t\t Iteration 168 Loss Train: 0.3188924193382263\n",
      "Epoch: 4 \t\t\t Iteration 169 Loss Train: 0.40550893545150757\n",
      "Epoch: 4 \t\t\t Iteration 170 Loss Train: 0.5426865816116333\n",
      "Epoch: 4 \t\t\t Iteration 171 Loss Train: 0.41010433435440063\n",
      "Epoch: 4 \t\t\t Iteration 172 Loss Train: 0.43836790323257446\n",
      "Epoch: 4 \t\t\t Iteration 173 Loss Train: 0.4479500353336334\n",
      "Epoch: 4 \t\t\t Iteration 174 Loss Train: 0.4184987545013428\n",
      "Epoch: 4 \t\t\t Iteration 175 Loss Train: 0.43483567237854004\n",
      "Epoch: 4 \t\t\t Iteration 176 Loss Train: 0.551419198513031\n",
      "Epoch: 4 \t\t\t Iteration 177 Loss Train: 0.45436179637908936\n",
      "Epoch: 4 \t\t\t Iteration 178 Loss Train: 0.47188329696655273\n",
      "Epoch: 4 \t\t\t Iteration 179 Loss Train: 0.32623010873794556\n",
      "Epoch: 4 \t\t\t Iteration 180 Loss Train: 0.3254128694534302\n",
      "Epoch: 4 \t\t\t Iteration 181 Loss Train: 0.7828840017318726\n",
      "Epoch: 4 \t\t\t Iteration 182 Loss Train: 0.40909838676452637\n",
      "Epoch: 4 \t\t\t Iteration 183 Loss Train: 0.3327813148498535\n",
      "Epoch: 4 \t\t\t Iteration 184 Loss Train: 0.4357321560382843\n",
      "Epoch: 4 \t\t\t Iteration 185 Loss Train: 0.5594524145126343\n",
      "Epoch: 4 \t\t\t Iteration 186 Loss Train: 0.4412297010421753\n",
      "Epoch: 4 \t\t\t Iteration 187 Loss Train: 0.4427012801170349\n",
      "Epoch: 4 \t\t\t Iteration 188 Loss Train: 0.3598800301551819\n",
      "Epoch: 4 \t\t\t Iteration 189 Loss Train: 0.34329020977020264\n",
      "Epoch: 4 \t\t\t Iteration 190 Loss Train: 0.5608487129211426\n",
      "Epoch: 4 \t\t\t Iteration 191 Loss Train: 0.4772592782974243\n",
      "Epoch: 4 \t\t\t Iteration 192 Loss Train: 0.35254406929016113\n",
      "Epoch: 4 \t\t\t Iteration 193 Loss Train: 0.49287334084510803\n",
      "Epoch: 4 \t\t\t Iteration 194 Loss Train: 0.3319496810436249\n",
      "Epoch: 4 \t\t\t Iteration 195 Loss Train: 0.4500000476837158\n",
      "Epoch: 4 \t\t\t Iteration 196 Loss Train: 0.355510413646698\n",
      "Epoch: 4 \t\t\t Iteration 197 Loss Train: 0.2835713028907776\n",
      "Epoch: 4 \t\t\t Iteration 198 Loss Train: 0.46770721673965454\n",
      "Epoch: 4 \t\t\t Iteration 199 Loss Train: 0.3584980368614197\n",
      "Epoch: 4 \t\t\t Iteration 200 Loss Train: 0.5904549360275269\n",
      "Epoch: 4 \t\t\t Iteration 201 Loss Train: 0.5996638536453247\n",
      "Epoch: 4 \t\t\t Iteration 202 Loss Train: 0.47080832719802856\n",
      "Epoch: 4 \t\t\t Iteration 203 Loss Train: 0.5517826676368713\n",
      "Epoch: 4 \t\t\t Iteration 204 Loss Train: 0.5271104574203491\n",
      "Epoch: 4 \t\t\t Iteration 205 Loss Train: 0.41400146484375\n",
      "Epoch: 4 \t\t\t Iteration 206 Loss Train: 0.31474921107292175\n",
      "Epoch: 4 \t\t\t Iteration 207 Loss Train: 0.5173503160476685\n",
      "Epoch: 4 \t\t\t Iteration 208 Loss Train: 0.564724326133728\n",
      "Epoch: 4 \t\t\t Iteration 209 Loss Train: 0.29594945907592773\n",
      "Epoch: 4 \t\t\t Iteration 210 Loss Train: 0.2750721573829651\n",
      "Epoch: 4 \t\t\t Iteration 211 Loss Train: 0.437905490398407\n",
      "Epoch: 4 \t\t\t Iteration 212 Loss Train: 0.41714778542518616\n",
      "Epoch: 4 \t\t\t Iteration 213 Loss Train: 0.21608051657676697\n",
      "Epoch: 4 \t\t\t Iteration 214 Loss Train: 0.3285572826862335\n",
      "Epoch: 4 \t\t\t Iteration 215 Loss Train: 0.359077513217926\n",
      "Epoch: 4 \t\t\t Iteration 216 Loss Train: 0.3767145276069641\n",
      "Epoch: 4 \t\t\t Iteration 217 Loss Train: 0.5647961497306824\n",
      "Epoch: 4 \t\t\t Iteration 218 Loss Train: 0.4720222055912018\n",
      "Epoch: 4 \t\t\t Iteration 219 Loss Train: 0.42173874378204346\n",
      "Epoch: 4 \t\t\t Iteration 220 Loss Train: 0.42147475481033325\n",
      "Epoch: 4 \t\t\t Iteration 221 Loss Train: 0.26732921600341797\n",
      "Epoch: 4 \t\t\t Iteration 222 Loss Train: 0.6422716379165649\n",
      "Epoch: 4 \t\t\t Iteration 223 Loss Train: 0.429046094417572\n",
      "Epoch: 4 \t\t\t Iteration 224 Loss Train: 0.35518601536750793\n",
      "Epoch: 4 \t\t\t Iteration 225 Loss Train: 0.36480265855789185\n",
      "Epoch: 4 \t\t\t Iteration 226 Loss Train: 0.27275803685188293\n",
      "Epoch: 4 \t\t\t Iteration 227 Loss Train: 0.2706495225429535\n",
      "Epoch: 4 \t\t\t Iteration 228 Loss Train: 0.4852311313152313\n",
      "Epoch: 4 \t\t\t Iteration 229 Loss Train: 0.5165292024612427\n",
      "Epoch: 4 \t\t\t Iteration 230 Loss Train: 0.3788790702819824\n",
      "Epoch: 4 \t\t\t Iteration 231 Loss Train: 0.4327453672885895\n",
      "Epoch: 4 \t\t\t Iteration 232 Loss Train: 0.535895824432373\n",
      "Epoch: 4 \t\t\t Iteration 233 Loss Train: 0.465298056602478\n",
      "Epoch: 4 \t\t\t Iteration 234 Loss Train: 0.49059706926345825\n",
      "Epoch: 4 \t\t\t Iteration 235 Loss Train: 0.49991685152053833\n",
      "Epoch: 4 \t\t\t Iteration 236 Loss Train: 0.5125737190246582\n",
      "Epoch: 4 \t\t\t Iteration 237 Loss Train: 0.4849928617477417\n",
      "Epoch: 4 \t\t\t Iteration 238 Loss Train: 0.3204059600830078\n",
      "Epoch: 4 \t\t\t Iteration 239 Loss Train: 0.14426591992378235\n",
      "Epoch: 4 \t\t\t Iteration 240 Loss Train: 0.4177015423774719\n",
      "Epoch: 4 \t\t\t Iteration 241 Loss Train: 0.18538694083690643\n",
      "Epoch: 4 \t\t\t Iteration 242 Loss Train: 0.5927819013595581\n",
      "Epoch: 4 \t\t\t Iteration 243 Loss Train: 0.4203135371208191\n",
      "Epoch: 4 \t\t\t Iteration 244 Loss Train: 0.5279531478881836\n",
      "Epoch: 4 \t\t\t Iteration 245 Loss Train: 0.41695302724838257\n",
      "Epoch: 4 \t\t\t Iteration 246 Loss Train: 0.4863268733024597\n",
      "Epoch: 4 \t\t\t Iteration 247 Loss Train: 0.43763336539268494\n",
      "Epoch: 4 \t\t\t Iteration 248 Loss Train: 0.39913612604141235\n",
      "Epoch: 4 \t\t\t Iteration 249 Loss Train: 0.2911723256111145\n",
      "Epoch: 4 \t\t\t Iteration 250 Loss Train: 0.40116894245147705\n",
      "Epoch: 4 \t\t\t Iteration 251 Loss Train: 0.5006027221679688\n",
      "Epoch: 4 \t\t\t Iteration 252 Loss Train: 0.43288692831993103\n",
      "Epoch: 4 \t\t\t Iteration 253 Loss Train: 0.3522162139415741\n",
      "Epoch: 4 \t\t\t Iteration 254 Loss Train: 0.4241448938846588\n",
      "Epoch: 4 \t\t\t Iteration 255 Loss Train: 0.30159515142440796\n",
      "Epoch: 4 \t\t\t Iteration 256 Loss Train: 0.39514613151550293\n",
      "Epoch: 4 \t\t\t Iteration 257 Loss Train: 0.3261070251464844\n",
      "Epoch: 4 \t\t\t Iteration 258 Loss Train: 0.5642712116241455\n",
      "Epoch: 4 \t\t\t Iteration 259 Loss Train: 0.3061664402484894\n",
      "Epoch: 4 \t\t\t Iteration 260 Loss Train: 0.35562431812286377\n",
      "Epoch: 4 \t\t\t Iteration 261 Loss Train: 0.43776756525039673\n",
      "Epoch: 4 \t\t\t Iteration 262 Loss Train: 0.45272165536880493\n",
      "Epoch: 4 \t\t\t Iteration 263 Loss Train: 0.3101827800273895\n",
      "Epoch: 4 \t\t\t Iteration 264 Loss Train: 0.5528922080993652\n",
      "Epoch: 4 \t\t\t Iteration 265 Loss Train: 0.4435878098011017\n",
      "Epoch: 4 \t\t\t Iteration 266 Loss Train: 0.29693982005119324\n",
      "Epoch: 4 \t\t\t Iteration 267 Loss Train: 0.6194841265678406\n",
      "Epoch: 4 \t\t\t Iteration 268 Loss Train: 0.4482758939266205\n",
      "Epoch: 4 \t\t\t Iteration 269 Loss Train: 0.38481587171554565\n",
      "Epoch: 4 \t\t\t Iteration 270 Loss Train: 0.44593048095703125\n",
      "Epoch: 4 \t\t\t Iteration 271 Loss Train: 0.5051018595695496\n",
      "Epoch: 4 \t\t\t Iteration 272 Loss Train: 0.36429154872894287\n",
      "Epoch: 4 \t\t\t Iteration 273 Loss Train: 0.39064255356788635\n",
      "Epoch: 4 \t\t\t Iteration 274 Loss Train: 0.4828835427761078\n",
      "Epoch: 4 \t\t\t Iteration 275 Loss Train: 0.37825414538383484\n",
      "Epoch: 4 \t\t\t Iteration 276 Loss Train: 0.4400826394557953\n",
      "Epoch: 4 \t\t\t Iteration 277 Loss Train: 0.44659310579299927\n",
      "Epoch: 4 \t\t\t Iteration 278 Loss Train: 0.5496021509170532\n",
      "Epoch: 4 \t\t\t Iteration 279 Loss Train: 0.49888941645622253\n",
      "Epoch: 4 \t\t\t Iteration 280 Loss Train: 0.4677931070327759\n",
      "Epoch: 4 \t\t\t Iteration 281 Loss Train: 0.43377697467803955\n",
      "Epoch: 4 \t\t\t Iteration 282 Loss Train: 0.42430928349494934\n",
      "Epoch: 4 \t\t\t Iteration 283 Loss Train: 0.5864560008049011\n",
      "Epoch: 4 \t\t\t Iteration 284 Loss Train: 0.4585275650024414\n",
      "Epoch: 4 \t\t\t Iteration 285 Loss Train: 0.41686877608299255\n",
      "Epoch: 4 \t\t\t Iteration 286 Loss Train: 0.431218683719635\n",
      "Epoch: 4 \t\t\t Iteration 287 Loss Train: 0.28923919796943665\n",
      "Epoch: 4 \t\t\t Iteration 288 Loss Train: 0.3945883512496948\n",
      "Epoch: 4 \t\t\t Iteration 289 Loss Train: 0.2956700325012207\n",
      "Epoch: 4 \t\t\t Iteration 290 Loss Train: 0.2795870006084442\n",
      "Epoch: 4 \t\t\t Iteration 291 Loss Train: 0.4403109550476074\n",
      "Epoch: 4 \t\t\t Iteration 292 Loss Train: 0.4531501233577728\n",
      "Epoch: 4 \t\t\t Iteration 293 Loss Train: 0.3890525996685028\n",
      "Epoch: 4 \t\t\t Iteration 294 Loss Train: 0.4596743583679199\n",
      "Epoch: 4 \t\t\t Iteration 295 Loss Train: 0.47368359565734863\n",
      "Epoch: 4 \t\t\t Iteration 296 Loss Train: 0.3992983102798462\n",
      "Epoch: 4 \t\t\t Iteration 297 Loss Train: 0.49592074751853943\n",
      "Epoch: 4 \t\t\t Iteration 298 Loss Train: 0.42412734031677246\n",
      "Epoch: 4 \t\t\t Iteration 299 Loss Train: 0.5272708535194397\n",
      "Epoch: 4 \t\t\t Iteration 300 Loss Train: 0.37060749530792236\n",
      "Epoch: 4 \t\t\t Iteration 301 Loss Train: 0.3236081302165985\n",
      "Epoch: 4 \t\t\t Iteration 302 Loss Train: 0.3773139715194702\n",
      "Epoch: 4 \t\t\t Iteration 303 Loss Train: 0.3168538212776184\n",
      "Epoch: 4 \t\t\t Iteration 304 Loss Train: 0.39024439454078674\n",
      "Epoch: 4 \t\t\t Iteration 305 Loss Train: 0.32274481654167175\n",
      "Epoch: 4 \t\t\t Iteration 306 Loss Train: 0.5989294052124023\n",
      "Epoch: 4 \t\t\t Iteration 307 Loss Train: 0.43403467535972595\n",
      "Epoch: 4 \t\t\t Iteration 308 Loss Train: 0.4078301787376404\n",
      "Epoch: 4 \t\t\t Iteration 309 Loss Train: 0.45516401529312134\n",
      "Epoch: 4 \t\t\t Iteration 310 Loss Train: 0.4105086624622345\n",
      "Epoch: 4 \t\t\t Iteration 311 Loss Train: 0.3027920126914978\n",
      "Epoch: 4 \t\t\t Iteration 312 Loss Train: 0.7292852401733398\n",
      "Epoch: 4 \t\t\t Iteration 313 Loss Train: 0.43555599451065063\n",
      "Epoch: 4 \t\t\t Iteration 314 Loss Train: 0.5096706748008728\n",
      "Epoch: 4 \t\t\t Iteration 315 Loss Train: 0.27201807498931885\n",
      "Epoch: 4 \t\t\t Iteration 316 Loss Train: 0.4072874188423157\n",
      "Epoch: 4 \t\t\t Iteration 317 Loss Train: 0.2721251845359802\n",
      "Epoch: 4 \t\t\t Iteration 318 Loss Train: 0.3518063724040985\n",
      "Epoch: 4 \t\t\t Iteration 319 Loss Train: 0.4798892140388489\n",
      "Epoch: 4 \t\t\t Iteration 320 Loss Train: 0.3388700485229492\n",
      "Epoch: 4 \t\t\t Iteration 321 Loss Train: 0.33482661843299866\n",
      "Epoch: 4 \t\t\t Iteration 322 Loss Train: 0.3036334216594696\n",
      "Epoch: 4 \t\t\t Iteration 323 Loss Train: 0.6701101660728455\n",
      "Epoch: 4 \t\t\t Iteration 324 Loss Train: 0.47039133310317993\n",
      "Epoch: 4 \t\t\t Iteration 325 Loss Train: 0.42757582664489746\n",
      "Epoch: 4 \t\t\t Iteration 326 Loss Train: 0.41150104999542236\n",
      "Epoch: 4 \t\t\t Iteration 327 Loss Train: 0.422123521566391\n",
      "Epoch: 4 \t\t\t Iteration 328 Loss Train: 0.4052017331123352\n",
      "Epoch: 4 \t\t\t Iteration 329 Loss Train: 0.3468448519706726\n",
      "Epoch: 4 \t\t\t Iteration 330 Loss Train: 0.2948046326637268\n",
      "Epoch: 4 \t\t\t Iteration 331 Loss Train: 0.32580143213272095\n",
      "Epoch: 4 \t\t\t Iteration 332 Loss Train: 0.5692053437232971\n",
      "Epoch: 4 \t\t\t Iteration 333 Loss Train: 0.3782854974269867\n",
      "Epoch: 4 \t\t\t Iteration 334 Loss Train: 0.4549817144870758\n",
      "Epoch: 4 \t\t\t Iteration 335 Loss Train: 0.39045676589012146\n",
      "Epoch: 4 \t\t\t Iteration 336 Loss Train: 0.4182298183441162\n",
      "Epoch: 4 \t\t\t Iteration 337 Loss Train: 0.5733436346054077\n",
      "Epoch: 4 \t\t\t Iteration 338 Loss Train: 0.44288063049316406\n",
      "Epoch: 4 \t\t\t Iteration 339 Loss Train: 0.6169477701187134\n",
      "Epoch: 4 \t\t\t Iteration 340 Loss Train: 0.4966588020324707\n",
      "Epoch: 4 \t\t\t Iteration 341 Loss Train: 0.575908362865448\n",
      "Epoch: 4 \t\t\t Iteration 342 Loss Train: 0.43008530139923096\n",
      "Epoch: 4 \t\t\t Iteration 343 Loss Train: 0.35177648067474365\n",
      "Epoch: 4 \t\t\t Iteration 344 Loss Train: 0.4880962073802948\n",
      "Epoch: 4 \t\t\t Iteration 345 Loss Train: 0.581104040145874\n",
      "Epoch: 4 \t\t\t Iteration 346 Loss Train: 0.5700819492340088\n",
      "Epoch: 4 \t\t\t Iteration 347 Loss Train: 0.3510096073150635\n",
      "Epoch: 4 \t\t\t Iteration 348 Loss Train: 0.38180509209632874\n",
      "Epoch: 4 \t\t\t Iteration 349 Loss Train: 0.38646411895751953\n",
      "Epoch: 4 \t\t\t Iteration 350 Loss Train: 0.3985935151576996\n",
      "Epoch: 4 \t\t\t Iteration 351 Loss Train: 0.3898737132549286\n",
      "Epoch: 4 \t\t\t Iteration 352 Loss Train: 0.34792575240135193\n",
      "Epoch: 4 \t\t\t Iteration 353 Loss Train: 0.30480054020881653\n",
      "Epoch: 4 \t\t\t Iteration 354 Loss Train: 0.4392390251159668\n",
      "Epoch: 4 \t\t\t Iteration 355 Loss Train: 0.40040910243988037\n",
      "Epoch: 4 \t\t\t Iteration 356 Loss Train: 0.3783143162727356\n",
      "Epoch: 4 \t\t\t Iteration 357 Loss Train: 0.4387390911579132\n",
      "Epoch: 4 \t\t\t Iteration 358 Loss Train: 0.4353809654712677\n",
      "Epoch: 4 \t\t\t Iteration 359 Loss Train: 0.3771553039550781\n",
      "Epoch: 4 \t\t\t Iteration 360 Loss Train: 0.4391632676124573\n",
      "Epoch: 4 \t\t\t Iteration 361 Loss Train: 0.49137061834335327\n",
      "Epoch: 4 \t\t\t Iteration 362 Loss Train: 0.3722386360168457\n",
      "Epoch: 4 \t\t\t Iteration 363 Loss Train: 0.519649088382721\n",
      "Epoch: 4 \t\t\t Iteration 364 Loss Train: 0.3216499090194702\n",
      "Epoch: 4 \t\t\t Iteration 365 Loss Train: 0.2534385919570923\n",
      "Epoch: 4 \t\t\t Iteration 366 Loss Train: 0.3148592710494995\n",
      "Epoch: 4 \t\t\t Iteration 367 Loss Train: 0.35555121302604675\n",
      "Epoch: 4 \t\t\t Iteration 368 Loss Train: 0.365665078163147\n",
      "Epoch: 4 \t\t\t Iteration 369 Loss Train: 0.3806731700897217\n",
      "Epoch: 4 \t\t\t Iteration 370 Loss Train: 0.5345293283462524\n",
      "Epoch: 4 \t\t\t Iteration 371 Loss Train: 0.40820372104644775\n",
      "Epoch: 4 \t\t\t Iteration 372 Loss Train: 0.38218361139297485\n",
      "Epoch: 4 \t\t\t Iteration 373 Loss Train: 0.32111313939094543\n",
      "Epoch: 4 \t\t\t Iteration 374 Loss Train: 0.3970213532447815\n",
      "Epoch: 4 \t\t\t Iteration 375 Loss Train: 0.3542979657649994\n",
      "Epoch: 4 \t\t\t Iteration 376 Loss Train: 0.31152278184890747\n",
      "Epoch: 4 \t\t\t Iteration 377 Loss Train: 0.4718577265739441\n",
      "Epoch: 4 \t\t\t Iteration 378 Loss Train: 0.3716248869895935\n",
      "Epoch: 4 \t\t\t Iteration 379 Loss Train: 0.46736201643943787\n",
      "Epoch: 4 \t\t\t Iteration 380 Loss Train: 0.34096404910087585\n",
      "Epoch: 4 \t\t\t Iteration 381 Loss Train: 0.395382821559906\n",
      "Epoch: 4 \t\t\t Iteration 382 Loss Train: 0.5405625700950623\n",
      "Epoch: 4 \t\t\t Iteration 383 Loss Train: 0.43072009086608887\n",
      "Epoch: 4 \t\t\t Iteration 384 Loss Train: 0.4281906187534332\n",
      "Epoch: 4 \t\t\t Iteration 385 Loss Train: 0.5678409337997437\n",
      "Epoch: 4 \t\t\t Iteration 386 Loss Train: 0.3049429655075073\n",
      "Epoch: 4 \t\t\t Iteration 387 Loss Train: 0.4476347267627716\n",
      "Epoch: 4 \t\t\t Iteration 388 Loss Train: 0.4197748303413391\n",
      "Epoch: 4 \t\t\t Iteration 389 Loss Train: 0.3870235085487366\n",
      "Epoch: 4 \t\t\t Iteration 390 Loss Train: 0.5369478464126587\n",
      "Epoch: 4 \t\t\t Iteration 391 Loss Train: 0.33354827761650085\n",
      "Epoch: 4 \t\t\t Iteration 392 Loss Train: 0.32144761085510254\n",
      "Epoch: 4 \t\t\t Iteration 393 Loss Train: 0.40928637981414795\n",
      "Epoch: 4 \t\t\t Iteration 394 Loss Train: 0.42974168062210083\n",
      "Epoch: 4 \t\t\t Iteration 395 Loss Train: 0.4099619686603546\n",
      "Epoch: 4 \t\t\t Iteration 396 Loss Train: 0.389376163482666\n",
      "Epoch: 4 \t\t\t Iteration 397 Loss Train: 0.6232154369354248\n",
      "Epoch: 4 \t\t\t Iteration 398 Loss Train: 0.3521866202354431\n",
      "Epoch: 4 \t\t\t Iteration 399 Loss Train: 0.3604297637939453\n",
      "Epoch: 4 \t\t\t Iteration 400 Loss Train: 0.2768218517303467\n",
      "Epoch: 4 \t\t\t Iteration 401 Loss Train: 0.4486907720565796\n",
      "Epoch: 4 \t\t\t Iteration 402 Loss Train: 0.28919804096221924\n",
      "Epoch: 4 \t\t\t Iteration 403 Loss Train: 0.40911197662353516\n",
      "Epoch: 4 \t\t\t Iteration 404 Loss Train: 0.4266178607940674\n",
      "Epoch: 4 \t\t\t Iteration 405 Loss Train: 0.4434264302253723\n",
      "Epoch: 4 \t\t\t Iteration 406 Loss Train: 0.4664664566516876\n",
      "Epoch: 4 \t\t\t Iteration 407 Loss Train: 0.5861388444900513\n",
      "Epoch: 4 \t\t\t Iteration 408 Loss Train: 0.46819883584976196\n",
      "Epoch: 4 \t\t\t Iteration 409 Loss Train: 0.4397573471069336\n",
      "Epoch: 4 \t\t\t Iteration 410 Loss Train: 0.3548654317855835\n",
      "Epoch: 4 \t\t\t Iteration 411 Loss Train: 0.516616940498352\n",
      "Epoch: 4 \t\t\t Iteration 412 Loss Train: 0.5346695184707642\n",
      "Epoch: 4 \t\t\t Iteration 413 Loss Train: 0.4706629812717438\n",
      "Epoch: 4 \t\t\t Iteration 414 Loss Train: 0.45572274923324585\n",
      "Epoch: 4 \t\t\t Iteration 415 Loss Train: 0.29626092314720154\n",
      "Epoch: 4 \t\t\t Iteration 416 Loss Train: 0.4347190260887146\n",
      "Epoch: 4 \t\t\t Iteration 417 Loss Train: 0.5703071355819702\n",
      "Epoch: 4 \t\t\t Iteration 418 Loss Train: 0.5744770765304565\n",
      "Epoch: 4 \t\t\t Iteration 419 Loss Train: 0.47844311594963074\n",
      "Epoch: 4 \t\t\t Iteration 420 Loss Train: 0.47609731554985046\n",
      "Epoch: 4 \t\t\t Iteration 421 Loss Train: 0.330911785364151\n",
      "Epoch: 4 \t\t\t Iteration 422 Loss Train: 0.44801056385040283\n",
      "Epoch: 4 \t\t\t Iteration 423 Loss Train: 0.37506842613220215\n",
      "Epoch: 4 \t\t\t Iteration 424 Loss Train: 0.33253028988838196\n",
      "Epoch: 4 \t\t\t Iteration 425 Loss Train: 0.3427262306213379\n",
      "Epoch: 4 \t\t\t Iteration 426 Loss Train: 0.6247583031654358\n",
      "Epoch: 4 \t\t\t Iteration 427 Loss Train: 0.3618314266204834\n",
      "Epoch: 4 \t\t\t Iteration 428 Loss Train: 0.3546464443206787\n",
      "Epoch: 4 \t\t\t Iteration 429 Loss Train: 0.30674147605895996\n",
      "Epoch: 4 \t\t\t Iteration 430 Loss Train: 0.40394845604896545\n",
      "Epoch: 4 \t\t\t Iteration 431 Loss Train: 0.22281962633132935\n",
      "Epoch: 4 \t\t\t Iteration 432 Loss Train: 0.32298678159713745\n",
      "Epoch: 4 \t\t\t Iteration 433 Loss Train: 0.40523219108581543\n",
      "Epoch: 4 \t\t\t Iteration 434 Loss Train: 0.17701058089733124\n",
      "Epoch: 4 \t\t\t Iteration 435 Loss Train: 0.3711797297000885\n",
      "Epoch: 4 \t\t\t Iteration 436 Loss Train: 0.4305826723575592\n",
      "Epoch: 4 \t\t\t Iteration 437 Loss Train: 0.35123002529144287\n",
      "Epoch: 4 \t\t\t Iteration 438 Loss Train: 0.5543425679206848\n",
      "Epoch: 4 \t\t\t Iteration 439 Loss Train: 0.36613142490386963\n",
      "Epoch: 4 \t\t\t Iteration 440 Loss Train: 0.21185721457004547\n",
      "Epoch: 4 \t\t\t Iteration 441 Loss Train: 0.29371345043182373\n",
      "Epoch: 4 \t\t\t Iteration 442 Loss Train: 0.6506504416465759\n",
      "Epoch: 4 \t\t\t Iteration 443 Loss Train: 0.5231127738952637\n",
      "Epoch: 4 \t\t\t Iteration 444 Loss Train: 0.41707831621170044\n",
      "Epoch: 4 \t\t\t Iteration 445 Loss Train: 0.4745161831378937\n",
      "Epoch: 4 \t\t\t Iteration 446 Loss Train: 0.4128066599369049\n",
      "Epoch: 4 \t\t\t Iteration 447 Loss Train: 0.2840101718902588\n",
      "Epoch: 4 \t\t\t Iteration 448 Loss Train: 0.39714354276657104\n",
      "Epoch: 4 \t\t\t Iteration 449 Loss Train: 0.3775070011615753\n",
      "Epoch: 4 \t\t\t Iteration 450 Loss Train: 0.39691269397735596\n",
      "Epoch: 4 \t\t\t Iteration 451 Loss Train: 0.5195126533508301\n",
      "Epoch: 4 \t\t\t Iteration 452 Loss Train: 0.34327471256256104\n",
      "Epoch: 4 \t\t\t Iteration 453 Loss Train: 0.28031057119369507\n",
      "Epoch: 4 \t\t\t Iteration 454 Loss Train: 0.37556058168411255\n",
      "Epoch: 4 \t\t\t Iteration 455 Loss Train: 0.25774115324020386\n",
      "Epoch: 4 \t\t\t Iteration 456 Loss Train: 0.4867159426212311\n",
      "Epoch: 4 \t\t\t Iteration 457 Loss Train: 0.4078393578529358\n",
      "Epoch: 4 \t\t\t Iteration 458 Loss Train: 0.39852049946784973\n",
      "Epoch: 4 \t\t\t Iteration 459 Loss Train: 0.4958699941635132\n",
      "Epoch: 4 \t\t\t Iteration 460 Loss Train: 0.4275134801864624\n",
      "Epoch: 4 \t\t\t Iteration 461 Loss Train: 0.40180671215057373\n",
      "Epoch: 4 \t\t\t Iteration 462 Loss Train: 0.395024836063385\n",
      "Epoch: 4 \t\t\t Iteration 463 Loss Train: 0.5639820098876953\n",
      "Epoch: 4 \t\t\t Iteration 464 Loss Train: 0.2756667733192444\n",
      "Epoch: 4 \t\t\t Iteration 465 Loss Train: 0.3468175530433655\n",
      "Epoch: 4 \t\t\t Iteration 466 Loss Train: 0.2670367658138275\n",
      "Epoch: 4 \t\t\t Iteration 467 Loss Train: 0.2725338935852051\n",
      "Epoch: 4 \t\t\t Iteration 468 Loss Train: 0.2436063587665558\n",
      "Epoch: 4 \t\t\t Iteration 469 Loss Train: 0.11570093035697937\n",
      "Epoch: 4 \t\t\t Iteration 470 Loss Train: 0.7364649772644043\n",
      "Epoch: 4 \t\t\t Iteration 471 Loss Train: 0.3924466669559479\n",
      "Epoch: 4 \t\t\t Iteration 472 Loss Train: 0.35280826687812805\n",
      "Epoch: 4 \t\t\t Iteration 473 Loss Train: 0.3104887008666992\n",
      "Epoch: 4 \t\t\t Iteration 474 Loss Train: 0.3157542049884796\n",
      "Epoch: 4 \t\t\t Iteration 475 Loss Train: 0.4264500141143799\n",
      "Epoch: 4 \t\t\t Iteration 476 Loss Train: 0.4297533333301544\n",
      "Epoch: 4 \t\t\t Iteration 477 Loss Train: 0.3454938530921936\n",
      "Epoch: 4 \t\t\t Iteration 478 Loss Train: 0.30995383858680725\n",
      "Epoch: 4 \t\t\t Iteration 479 Loss Train: 0.7741378545761108\n",
      "Epoch: 4 \t\t\t Iteration 480 Loss Train: 0.4343634843826294\n",
      "Epoch: 4 \t\t\t Iteration 481 Loss Train: 0.3194662630558014\n",
      "Epoch: 4 \t\t\t Iteration 482 Loss Train: 0.3547702431678772\n",
      "Epoch: 4 \t\t\t Iteration 483 Loss Train: 0.19879895448684692\n",
      "Epoch: 4 \t\t\t Iteration 484 Loss Train: 0.43947356939315796\n",
      "Epoch: 4 \t\t\t Iteration 485 Loss Train: 0.38787519931793213\n",
      "Epoch: 4 \t\t\t Iteration 486 Loss Train: 0.4748126268386841\n",
      "Epoch: 4 \t\t\t Iteration 487 Loss Train: 0.5206263661384583\n",
      "Epoch: 4 \t\t\t Iteration 488 Loss Train: 0.38222065567970276\n",
      "Epoch: 4 \t\t\t Iteration 489 Loss Train: 0.22577375173568726\n",
      "Epoch: 4 \t\t\t Iteration 490 Loss Train: 0.41767269372940063\n",
      "Epoch: 4 \t\t\t Iteration 491 Loss Train: 0.3419601321220398\n",
      "Epoch: 4 \t\t\t Iteration 492 Loss Train: 0.5716332197189331\n",
      "Epoch: 4 \t\t\t Iteration 493 Loss Train: 0.43504205346107483\n",
      "Epoch: 4 \t\t\t Iteration 494 Loss Train: 0.4233323335647583\n",
      "Epoch: 4 \t\t\t Iteration 495 Loss Train: 0.34400010108947754\n",
      "Epoch: 4 \t\t\t Iteration 496 Loss Train: 0.48948997259140015\n",
      "Epoch: 4 \t\t\t Iteration 497 Loss Train: 0.2902909219264984\n",
      "Epoch: 4 \t\t\t Iteration 498 Loss Train: 0.5986281633377075\n",
      "Epoch: 4 \t\t\t Iteration 499 Loss Train: 0.4773126244544983\n",
      "Epoch: 4 \t\t\t Iteration 500 Loss Train: 0.3136132061481476\n",
      "Epoch: 4 \t\t\t Iteration 501 Loss Train: 0.5687100887298584\n",
      "Epoch: 4 \t\t\t Iteration 502 Loss Train: 0.4929637312889099\n",
      "Epoch: 4 \t\t\t Iteration 503 Loss Train: 0.554567277431488\n",
      "Epoch: 4 \t\t\t Iteration 504 Loss Train: 0.49827608466148376\n",
      "Epoch: 4 \t\t\t Iteration 505 Loss Train: 0.49714839458465576\n",
      "Epoch: 4 \t\t\t Iteration 506 Loss Train: 0.4743567705154419\n",
      "Epoch: 4 \t\t\t Iteration 507 Loss Train: 0.7168570756912231\n",
      "Epoch: 4 \t\t\t Iteration 508 Loss Train: 0.47368234395980835\n",
      "Epoch: 4 \t\t\t Iteration 509 Loss Train: 0.3651082515716553\n",
      "Epoch: 4 \t\t\t Iteration 510 Loss Train: 0.35385292768478394\n",
      "Epoch: 4 \t\t\t Iteration 511 Loss Train: 0.3826509416103363\n",
      "Epoch: 4 \t\t\t Iteration 512 Loss Train: 0.3645855784416199\n",
      "Epoch: 4 \t\t\t Iteration 513 Loss Train: 0.3762376010417938\n",
      "Epoch: 4 \t\t\t Iteration 514 Loss Train: 0.5700960755348206\n",
      "Epoch: 4 \t\t\t Iteration 515 Loss Train: 0.3355371356010437\n",
      "Epoch: 4 \t\t\t Iteration 516 Loss Train: 0.40463703870773315\n",
      "Epoch: 4 \t\t\t Iteration 517 Loss Train: 0.4381066858768463\n",
      "Epoch: 4 \t\t\t Iteration 518 Loss Train: 0.4753795862197876\n",
      "Epoch: 4 \t\t\t Iteration 519 Loss Train: 0.3999888300895691\n",
      "Epoch: 4 \t\t\t Iteration 520 Loss Train: 0.38000625371932983\n",
      "Epoch: 4 \t\t\t Iteration 521 Loss Train: 0.3501104712486267\n",
      "Epoch: 4 \t\t\t Iteration 522 Loss Train: 0.3839358687400818\n",
      "Epoch: 4 \t\t\t Iteration 523 Loss Train: 0.3804055452346802\n",
      "Epoch: 4 \t\t\t Iteration 524 Loss Train: 0.35297632217407227\n",
      "Epoch: 4 \t\t\t Iteration 525 Loss Train: 0.4590303301811218\n",
      "Epoch: 4 \t\t\t Iteration 526 Loss Train: 0.46878883242607117\n",
      "Epoch: 4 \t\t\t Iteration 527 Loss Train: 0.38718563318252563\n",
      "Epoch: 4 \t\t\t Iteration 528 Loss Train: 0.5028994083404541\n",
      "Epoch: 4 \t\t\t Iteration 529 Loss Train: 0.42636924982070923\n",
      "Epoch: 4 \t\t\t Iteration 530 Loss Train: 0.42735010385513306\n",
      "Epoch: 4 \t\t\t Iteration 531 Loss Train: 0.3249368667602539\n",
      "Epoch: 4 \t\t\t Iteration 532 Loss Train: 0.3295939564704895\n",
      "Epoch: 4 \t\t\t Iteration 533 Loss Train: 0.17514269053936005\n",
      "Epoch: 4 \t\t\t Iteration 534 Loss Train: 0.3051720857620239\n",
      "Epoch: 4 \t\t\t Iteration 535 Loss Train: 0.4134523570537567\n",
      "Epoch: 4 \t\t\t Iteration 536 Loss Train: 0.3536332845687866\n",
      "Epoch: 4 \t\t\t Iteration 537 Loss Train: 0.5281227827072144\n",
      "Epoch: 4 \t\t\t Iteration 538 Loss Train: 0.5018978118896484\n",
      "Epoch: 4 \t\t\t Iteration 539 Loss Train: 0.3971169590950012\n",
      "Epoch: 4 \t\t\t Iteration 540 Loss Train: 0.33796438574790955\n",
      "Epoch: 4 \t\t\t Iteration 541 Loss Train: 0.25746822357177734\n",
      "Epoch: 4 \t\t\t Iteration 542 Loss Train: 0.5679944753646851\n",
      "Epoch: 4 \t\t\t Iteration 543 Loss Train: 0.39148199558258057\n",
      "Epoch: 4 \t\t\t Iteration 544 Loss Train: 0.472811758518219\n",
      "Epoch: 4 \t\t\t Iteration 545 Loss Train: 0.3710154891014099\n",
      "Epoch: 4 \t\t\t Iteration 546 Loss Train: 0.4460545480251312\n",
      "Epoch: 4 \t\t\t Iteration 547 Loss Train: 0.477190762758255\n",
      "Epoch: 4 \t\t\t Iteration 548 Loss Train: 0.43854713439941406\n",
      "Epoch: 4 \t\t\t Iteration 549 Loss Train: 0.3104420304298401\n",
      "Epoch: 4 \t\t\t Iteration 550 Loss Train: 0.3595733642578125\n",
      "Epoch: 4 \t\t\t Iteration 551 Loss Train: 0.3363671898841858\n",
      "Epoch: 4 \t\t\t Iteration 552 Loss Train: 0.32967409491539\n",
      "Epoch: 4 \t\t\t Iteration 553 Loss Train: 0.46986591815948486\n",
      "Epoch: 4 \t\t\t Iteration 554 Loss Train: 0.41717255115509033\n",
      "Epoch: 4 \t\t\t Iteration 555 Loss Train: 0.5525195598602295\n",
      "Epoch: 4 \t\t\t Iteration 556 Loss Train: 0.3510187268257141\n",
      "Epoch: 4 \t\t\t Iteration 557 Loss Train: 0.35334697365760803\n",
      "Epoch: 4 \t\t\t Iteration 558 Loss Train: 0.42044001817703247\n",
      "Epoch: 4 \t\t\t Iteration 559 Loss Train: 0.33277618885040283\n",
      "Epoch: 4 \t\t\t Iteration 560 Loss Train: 0.4518392086029053\n",
      "Epoch: 4 \t\t\t Iteration 561 Loss Train: 0.38781553506851196\n",
      "Epoch: 4 \t\t\t Iteration 562 Loss Train: 0.3684030771255493\n",
      "Epoch: 4 \t\t\t Iteration 563 Loss Train: 0.3543704152107239\n",
      "Epoch: 4 \t\t\t Iteration 564 Loss Train: 0.49941858649253845\n",
      "Epoch: 4 \t\t\t Iteration 565 Loss Train: 0.2844218611717224\n",
      "Epoch: 4 \t\t\t Iteration 566 Loss Train: 0.40345141291618347\n",
      "Epoch: 4 \t\t\t Iteration 567 Loss Train: 0.43035778403282166\n",
      "Epoch: 4 \t\t\t Iteration 568 Loss Train: 0.31658944487571716\n",
      "Epoch: 4 \t\t\t Iteration 569 Loss Train: 0.4180179834365845\n",
      "Epoch: 4 \t\t\t Iteration 570 Loss Train: 0.48618537187576294\n",
      "Epoch: 4 \t\t\t Iteration 571 Loss Train: 0.5603606700897217\n",
      "Epoch: 4 \t\t\t Iteration 572 Loss Train: 0.3621090054512024\n",
      "Epoch: 4 \t\t\t Iteration 573 Loss Train: 0.3803040385246277\n",
      "Epoch: 4 \t\t\t Iteration 574 Loss Train: 0.5995397567749023\n",
      "Epoch: 4 \t\t\t Iteration 575 Loss Train: 0.4318316876888275\n",
      "Epoch: 4 \t\t\t Iteration 576 Loss Train: 0.4452582597732544\n",
      "Epoch: 4 \t\t\t Iteration 577 Loss Train: 0.21531006693840027\n",
      "Epoch: 4 \t\t\t Iteration 578 Loss Train: 0.2963441014289856\n",
      "Epoch: 4 \t\t\t Iteration 579 Loss Train: 0.53842693567276\n",
      "Epoch: 4 \t\t\t Iteration 580 Loss Train: 0.7485886216163635\n",
      "Epoch: 4 \t\t\t Iteration 581 Loss Train: 0.46529263257980347\n",
      "Epoch: 4 \t\t\t Iteration 582 Loss Train: 0.3719836175441742\n",
      "Epoch: 4 \t\t\t Iteration 583 Loss Train: 0.41215747594833374\n",
      "Epoch: 4 \t\t\t Iteration 584 Loss Train: 0.39554646611213684\n",
      "Epoch: 4 \t\t\t Iteration 585 Loss Train: 0.3200140595436096\n",
      "Epoch: 4 \t\t\t Iteration 586 Loss Train: 0.40184926986694336\n",
      "Epoch: 4 \t\t\t Iteration 587 Loss Train: 0.3842102289199829\n",
      "Epoch: 4 \t\t\t Iteration 588 Loss Train: 0.5336153507232666\n",
      "Epoch: 4 \t\t\t Iteration 589 Loss Train: 0.4061203598976135\n",
      "Epoch: 4 \t\t\t Iteration 590 Loss Train: 0.40054982900619507\n",
      "Epoch: 4 \t\t\t Iteration 591 Loss Train: 0.48137274384498596\n",
      "Epoch: 4 \t\t\t Iteration 592 Loss Train: 0.44882139563560486\n",
      "Epoch: 4 \t\t\t Iteration 593 Loss Train: 0.4403409957885742\n",
      "Epoch: 4 \t\t\t Iteration 594 Loss Train: 0.33939820528030396\n",
      "Epoch: 4 \t\t\t Iteration 595 Loss Train: 0.5823450684547424\n",
      "Epoch: 4 \t\t\t Iteration 596 Loss Train: 0.3769238591194153\n",
      "Epoch: 4 \t\t\t Iteration 597 Loss Train: 0.3881853222846985\n",
      "Epoch: 4 \t\t\t Iteration 598 Loss Train: 0.3268292546272278\n",
      "Epoch: 4 \t\t\t Iteration 599 Loss Train: 0.39943796396255493\n",
      "Epoch: 4 \t\t\t Iteration 600 Loss Train: 0.4689026176929474\n",
      "Epoch: 4 \t\t\t Iteration 601 Loss Train: 0.5895391702651978\n",
      "Epoch: 4 \t\t\t Iteration 602 Loss Train: 0.4467039108276367\n",
      "Epoch: 4 \t\t\t Iteration 603 Loss Train: 0.34809598326683044\n",
      "Epoch: 4 \t\t\t Iteration 604 Loss Train: 0.2975505590438843\n",
      "Epoch: 4 \t\t\t Iteration 605 Loss Train: 0.42236241698265076\n",
      "Epoch: 4 \t\t\t Iteration 606 Loss Train: 0.3649718761444092\n",
      "Epoch: 4 \t\t\t Iteration 607 Loss Train: 0.23378878831863403\n",
      "Epoch: 4 \t\t\t Iteration 608 Loss Train: 0.39294809103012085\n",
      "Epoch: 4 \t\t\t Iteration 609 Loss Train: 0.3974609375\n",
      "Epoch: 4 \t\t\t Iteration 610 Loss Train: 0.2722243070602417\n",
      "Epoch: 4 \t\t\t Iteration 611 Loss Train: 0.46010732650756836\n",
      "Epoch: 4 \t\t\t Iteration 612 Loss Train: 0.3697543144226074\n",
      "Epoch: 4 \t\t\t Iteration 613 Loss Train: 0.38843613862991333\n",
      "Epoch: 4 \t\t\t Iteration 614 Loss Train: 0.24101006984710693\n",
      "Epoch: 4 \t\t\t Iteration 615 Loss Train: 0.772750198841095\n",
      "Epoch: 4 \t\t\t Iteration 616 Loss Train: 0.5344113111495972\n",
      "Epoch: 4 \t\t\t Iteration 617 Loss Train: 0.4337049126625061\n",
      "Epoch: 4 \t\t\t Iteration 618 Loss Train: 0.35890650749206543\n",
      "Epoch: 4 \t\t\t Iteration 619 Loss Train: 0.4249110221862793\n",
      "Epoch: 4 \t\t\t Iteration 620 Loss Train: 0.4599304795265198\n",
      "Epoch: 4 \t\t\t Iteration 621 Loss Train: 0.7020556926727295\n",
      "Epoch: 4 \t\t\t Iteration 622 Loss Train: 0.5372748970985413\n",
      "Epoch: 4 \t\t\t Iteration 623 Loss Train: 0.46432822942733765\n",
      "Epoch: 4 \t\t\t Iteration 624 Loss Train: 0.40027809143066406\n",
      "Epoch: 4 \t\t\t Iteration 625 Loss Train: 0.3814288377761841\n",
      "Epoch: 4 \t\t\t Iteration 626 Loss Train: 0.33729425072669983\n",
      "Epoch: 4 \t\t\t Iteration 627 Loss Train: 0.4902980327606201\n",
      "Epoch: 4 \t\t\t Iteration 628 Loss Train: 0.4586098790168762\n",
      "Epoch: 4 \t\t\t Iteration 629 Loss Train: 0.5156962871551514\n",
      "Epoch: 4 \t\t\t Iteration 630 Loss Train: 0.2734200358390808\n",
      "Epoch: 4 \t\t\t Iteration 631 Loss Train: 0.3781702518463135\n",
      "Epoch: 4 \t\t\t Iteration 632 Loss Train: 0.3915576934814453\n",
      "Epoch: 4 \t\t\t Iteration 633 Loss Train: 0.5256400108337402\n",
      "Epoch: 4 \t\t\t Iteration 634 Loss Train: 0.47177356481552124\n",
      "Epoch: 4 \t\t\t Iteration 635 Loss Train: 0.3412480354309082\n",
      "Epoch: 4 \t\t\t Iteration 636 Loss Train: 0.4186449646949768\n",
      "Epoch: 4 \t\t\t Iteration 637 Loss Train: 0.42374664545059204\n",
      "Epoch: 4 \t\t\t Iteration 638 Loss Train: 0.3229232132434845\n",
      "Epoch: 4 \t\t\t Iteration 639 Loss Train: 0.4618644118309021\n",
      "Epoch: 4 \t\t\t Iteration 640 Loss Train: 0.4965325593948364\n",
      "Epoch: 4 \t\t\t Iteration 641 Loss Train: 0.42486777901649475\n",
      "Epoch: 4 \t\t\t Iteration 642 Loss Train: 0.5546549558639526\n",
      "Epoch: 4 \t\t\t Iteration 643 Loss Train: 0.2482990026473999\n",
      "Epoch: 4 \t\t\t Iteration 644 Loss Train: 0.34899014234542847\n",
      "Epoch: 4 \t\t\t Iteration 645 Loss Train: 0.4372643530368805\n",
      "Epoch: 4 \t\t\t Iteration 646 Loss Train: 0.30728140473365784\n",
      "Epoch: 4 \t\t\t Iteration 647 Loss Train: 0.4060100317001343\n",
      "Epoch: 4 \t\t\t Iteration 648 Loss Train: 0.31243637204170227\n",
      "Epoch: 4 \t\t\t Iteration 649 Loss Train: 0.4583963453769684\n",
      "Epoch: 4 \t\t\t Iteration 650 Loss Train: 0.37342557311058044\n",
      "Epoch: 4 \t\t\t Iteration 651 Loss Train: 0.4159677028656006\n",
      "Epoch: 4 \t\t\t Iteration 652 Loss Train: 0.4155360162258148\n",
      "Epoch: 4 \t\t\t Iteration 653 Loss Train: 0.4633200764656067\n",
      "Epoch: 4 \t\t\t Iteration 654 Loss Train: 0.3305620849132538\n",
      "Epoch: 4 \t\t\t Iteration 655 Loss Train: 0.30379337072372437\n",
      "Epoch: 4 \t\t\t Iteration 656 Loss Train: 0.1968563050031662\n",
      "Epoch: 4 \t\t\t Iteration 657 Loss Train: 0.4357195496559143\n",
      "Epoch: 4 \t\t\t Iteration 658 Loss Train: 0.82756108045578\n",
      "Epoch: 4 \t\t\t Iteration 659 Loss Train: 0.40592414140701294\n",
      "Epoch: 4 \t\t\t Iteration 660 Loss Train: 0.546466052532196\n",
      "Epoch: 4 \t\t\t Iteration 661 Loss Train: 0.43203508853912354\n",
      "Epoch: 4 \t\t\t Iteration 662 Loss Train: 0.5143771171569824\n",
      "Epoch: 4 \t\t\t Iteration 663 Loss Train: 0.4813971519470215\n",
      "Epoch: 4 \t\t\t Iteration 664 Loss Train: 0.48205551505088806\n",
      "Epoch: 4 \t\t\t Iteration 665 Loss Train: 0.35127580165863037\n",
      "Epoch: 4 \t\t\t Iteration 666 Loss Train: 0.41466766595840454\n",
      "Epoch: 4 \t\t\t Iteration 667 Loss Train: 0.2480980008840561\n",
      "Epoch: 4 \t\t\t Iteration 668 Loss Train: 0.3914867639541626\n",
      "Epoch: 4 \t\t\t Iteration 669 Loss Train: 0.22493049502372742\n",
      "Epoch: 4 \t\t\t Iteration 670 Loss Train: 0.37014567852020264\n",
      "Epoch: 4 \t\t\t Iteration 671 Loss Train: 0.7547791004180908\n",
      "Epoch: 4 \t\t\t Iteration 672 Loss Train: 0.3751634955406189\n",
      "Epoch: 4 \t\t\t Iteration 673 Loss Train: 0.5269066095352173\n",
      "Epoch: 4 \t\t\t Iteration 674 Loss Train: 0.4759378433227539\n",
      "Epoch: 4 \t\t\t Iteration 675 Loss Train: 0.4490612745285034\n",
      "Epoch: 4 \t\t\t Iteration 676 Loss Train: 0.5740070343017578\n",
      "Epoch: 4 \t\t\t Iteration 677 Loss Train: 0.4812605679035187\n",
      "Epoch: 4 \t\t\t Iteration 678 Loss Train: 0.26852601766586304\n",
      "Epoch: 4 \t\t\t Iteration 679 Loss Train: 0.23868714272975922\n",
      "Epoch: 4 \t\t\t Iteration 680 Loss Train: 0.40024393796920776\n",
      "Epoch: 4 \t\t\t Iteration 681 Loss Train: 0.4538106620311737\n",
      "Epoch: 4 \t\t\t Iteration 682 Loss Train: 0.23153555393218994\n",
      "Epoch: 4 \t\t\t Iteration 683 Loss Train: 0.21895116567611694\n",
      "Epoch: 4 \t\t\t Iteration 684 Loss Train: 0.46089693903923035\n",
      "Epoch: 4 \t\t\t Iteration 685 Loss Train: 0.5167354345321655\n",
      "Epoch: 4 \t\t\t Iteration 686 Loss Train: 0.46720215678215027\n",
      "Epoch: 4 \t\t\t Iteration 687 Loss Train: 0.30914434790611267\n",
      "Epoch: 4 \t\t\t Iteration 688 Loss Train: 0.8502276539802551\n",
      "Epoch: 4 \t\t\t Iteration 689 Loss Train: 0.4475293755531311\n",
      "Epoch: 4 \t\t\t Iteration 690 Loss Train: 0.4034096598625183\n",
      "Epoch: 4 \t\t\t Iteration 691 Loss Train: 0.4756777584552765\n",
      "Epoch: 4 \t\t\t Iteration 692 Loss Train: 0.49075278639793396\n",
      "Epoch: 4 \t\t\t Iteration 693 Loss Train: 0.3856115937232971\n",
      "Epoch: 4 \t\t\t Iteration 694 Loss Train: 0.39449453353881836\n",
      "Epoch: 4 \t\t\t Iteration 695 Loss Train: 0.4022069275379181\n",
      "Epoch: 4 \t\t\t Iteration 696 Loss Train: 0.4657103419303894\n",
      "Epoch: 4 \t\t\t Iteration 697 Loss Train: 0.34236013889312744\n",
      "Epoch: 4 \t\t\t Iteration 698 Loss Train: 0.2714695334434509\n",
      "Epoch: 4 \t\t\t Iteration 699 Loss Train: 0.23140856623649597\n",
      "Epoch: 4 \t\t\t Iteration 700 Loss Train: 0.46934574842453003\n",
      "Epoch: 4 \t\t\t Iteration 701 Loss Train: 0.43932127952575684\n",
      "Epoch: 4 \t\t\t Iteration 702 Loss Train: 0.4768698215484619\n",
      "Epoch: 4 \t\t\t Iteration 703 Loss Train: 0.5805556774139404\n",
      "Epoch: 4 \t\t\t Iteration 704 Loss Train: 0.5641452074050903\n",
      "Epoch: 4 \t\t\t Iteration 705 Loss Train: 0.5684915781021118\n",
      "Epoch: 4 \t\t\t Iteration 706 Loss Train: 0.5000994205474854\n",
      "Epoch: 4 \t\t\t Iteration 707 Loss Train: 0.40891844034194946\n",
      "Epoch: 4 \t\t\t Iteration 708 Loss Train: 0.4105226695537567\n",
      "Epoch: 4 \t\t\t Iteration 709 Loss Train: 0.3658672273159027\n",
      "Epoch: 4 \t\t\t Iteration 710 Loss Train: 0.3585653603076935\n",
      "Epoch: 4 \t\t\t Iteration 711 Loss Train: 0.2891288995742798\n",
      "Epoch: 4 \t\t\t Iteration 712 Loss Train: 0.3332682251930237\n",
      "Epoch: 4 \t\t\t Iteration 713 Loss Train: 0.4254540503025055\n",
      "Epoch: 4 \t\t\t Iteration 714 Loss Train: 0.47160816192626953\n",
      "Epoch: 4 \t\t\t Iteration 715 Loss Train: 0.2793768644332886\n",
      "Epoch: 4 \t\t\t Iteration 716 Loss Train: 0.39302951097488403\n",
      "Epoch: 4 \t\t\t Iteration 717 Loss Train: 0.43578070402145386\n",
      "Epoch: 4 \t\t\t Iteration 718 Loss Train: 0.4027893543243408\n",
      "Epoch: 4 \t\t\t Iteration 719 Loss Train: 0.5019670128822327\n",
      "Epoch: 4 \t\t\t Iteration 720 Loss Train: 0.46806100010871887\n",
      "Epoch: 4 \t\t\t Iteration 721 Loss Train: 0.5289837121963501\n",
      "Epoch: 4 \t\t\t Iteration 722 Loss Train: 0.35525673627853394\n",
      "Epoch: 4 \t\t\t Iteration 723 Loss Train: 0.3524148762226105\n",
      "Epoch: 4 \t\t\t Iteration 724 Loss Train: 0.3987633287906647\n",
      "Epoch: 4 \t\t\t Iteration 725 Loss Train: 0.28380781412124634\n",
      "Epoch: 4 \t\t\t Iteration 726 Loss Train: 0.49497324228286743\n",
      "Epoch: 4 \t\t\t Iteration 727 Loss Train: 0.4589254558086395\n",
      "Epoch: 4 \t\t\t Iteration 728 Loss Train: 0.5282391309738159\n",
      "Epoch: 4 \t\t\t Iteration 729 Loss Train: 0.36340683698654175\n",
      "Epoch: 4 \t\t\t Iteration 730 Loss Train: 0.32761067152023315\n",
      "Epoch: 4 \t\t\t Iteration 731 Loss Train: 0.4032089114189148\n",
      "Epoch: 4 \t\t\t Iteration 732 Loss Train: 0.24758772552013397\n",
      "Epoch: 4 \t\t\t Iteration 733 Loss Train: 0.35535383224487305\n",
      "Epoch: 4 \t\t\t Iteration 734 Loss Train: 0.7371764183044434\n",
      "Epoch: 4 \t\t\t Iteration 735 Loss Train: 0.47872453927993774\n",
      "Epoch: 4 \t\t\t Iteration 736 Loss Train: 0.5305548906326294\n",
      "Epoch: 4 \t\t\t Iteration 737 Loss Train: 0.4019452929496765\n",
      "Epoch: 4 \t\t\t Iteration 738 Loss Train: 0.36505287885665894\n",
      "Epoch: 4 \t\t\t Iteration 739 Loss Train: 1.02126145362854\n",
      "Epoch: 4 \t\t\t Iteration 740 Loss Train: 0.5464789271354675\n",
      "Epoch: 4 \t\t\t Iteration 741 Loss Train: 0.45156845450401306\n",
      "Epoch: 4 \t\t\t Iteration 742 Loss Train: 0.5314133763313293\n",
      "Epoch: 4 \t\t\t Iteration 743 Loss Train: 0.4509907364845276\n",
      "Epoch: 4 \t\t\t Iteration 744 Loss Train: 0.380410373210907\n",
      "Epoch: 4 \t\t\t Iteration 745 Loss Train: 0.4544430375099182\n",
      "Epoch: 4 \t\t\t Iteration 746 Loss Train: 0.4307810366153717\n",
      "Epoch: 4 \t\t\t Iteration 747 Loss Train: 0.4073391556739807\n",
      "Epoch: 4 \t\t\t Iteration 748 Loss Train: 0.3126789927482605\n",
      "Epoch: 4 \t\t\t Iteration 749 Loss Train: 0.364767849445343\n",
      "Epoch: 4 \t\t\t Iteration 750 Loss Train: 0.40747755765914917\n",
      "Epoch: 4 \t\t\t Iteration 751 Loss Train: 0.4285815954208374\n",
      "Epoch: 4 \t\t\t Iteration 752 Loss Train: 0.20689785480499268\n",
      "Epoch: 4 \t\t\t Iteration 753 Loss Train: 0.3998574912548065\n",
      "Epoch: 4 \t\t\t Iteration 754 Loss Train: 0.395594984292984\n",
      "Epoch: 4 \t\t\t Iteration 755 Loss Train: 0.6138710379600525\n",
      "Epoch: 4 \t\t\t Iteration 756 Loss Train: 0.4219537377357483\n",
      "Epoch: 4 \t\t\t Iteration 757 Loss Train: 0.4357471764087677\n",
      "Epoch: 4 \t\t\t Iteration 758 Loss Train: 0.34073418378829956\n",
      "Epoch: 4 \t\t\t Iteration 759 Loss Train: 0.41760769486427307\n",
      "Epoch: 4 \t\t\t Iteration 760 Loss Train: 0.4993412494659424\n",
      "Epoch: 4 \t\t\t Iteration 761 Loss Train: 0.5308116674423218\n",
      "Epoch: 4 \t\t\t Iteration 762 Loss Train: 0.29405850172042847\n",
      "Epoch: 4 \t\t\t Iteration 763 Loss Train: 0.385742723941803\n",
      "Epoch: 4 \t\t\t Iteration 764 Loss Train: 0.4555622339248657\n",
      "Epoch: 4 \t\t\t Iteration 765 Loss Train: 0.2485569566488266\n",
      "Epoch: 4 \t\t\t Iteration 766 Loss Train: 0.4377326965332031\n",
      "Epoch: 4 \t\t\t Iteration 767 Loss Train: 0.34737446904182434\n",
      "Epoch: 4 \t\t\t Iteration 768 Loss Train: 0.7894406318664551\n",
      "Epoch: 4 \t\t\t Iteration 769 Loss Train: 0.6058032512664795\n",
      "Epoch: 4 \t\t\t Iteration 770 Loss Train: 0.46380865573883057\n",
      "Epoch: 4 \t\t\t Iteration 771 Loss Train: 0.38618046045303345\n",
      "Epoch: 4 \t\t\t Iteration 772 Loss Train: 0.6062691807746887\n",
      "Epoch: 4 \t\t\t Iteration 773 Loss Train: 0.3876273036003113\n",
      "Epoch: 4 \t\t\t Iteration 774 Loss Train: 0.3125893473625183\n",
      "Epoch: 4 \t\t\t Iteration 775 Loss Train: 0.3797490894794464\n",
      "Epoch: 4 \t\t\t Iteration 776 Loss Train: 0.5993829965591431\n",
      "Epoch: 4 \t\t\t Iteration 777 Loss Train: 0.5336891412734985\n",
      "Epoch: 4 \t\t\t Iteration 778 Loss Train: 0.48462042212486267\n",
      "Epoch: 4 \t\t\t Iteration 779 Loss Train: 0.39133208990097046\n",
      "Epoch: 4 \t\t\t Iteration 780 Loss Train: 0.4208006262779236\n",
      "Epoch: 4 \t\t\t Iteration 781 Loss Train: 0.5301454067230225\n",
      "Epoch: 4 \t\t\t Iteration 782 Loss Train: 0.48176637291908264\n",
      "Epoch: 4 \t\t\t Iteration 783 Loss Train: 0.4175305962562561\n",
      "Epoch: 4 \t\t\t Iteration 784 Loss Train: 0.3775978982448578\n",
      "Epoch: 4 \t\t\t Iteration 785 Loss Train: 0.4760560989379883\n",
      "Epoch: 4 \t\t\t Iteration 786 Loss Train: 0.43237775564193726\n",
      "Epoch: 4 \t\t\t Iteration 787 Loss Train: 0.3538472652435303\n",
      "Epoch: 4 \t\t\t Iteration 788 Loss Train: 0.2327803671360016\n",
      "Epoch: 4 \t\t\t Iteration 789 Loss Train: 0.36816468834877014\n",
      "Epoch: 4 \t\t\t Iteration 790 Loss Train: 0.602912425994873\n",
      "Epoch: 4 \t\t\t Iteration 791 Loss Train: 0.41879600286483765\n",
      "Epoch: 4 \t\t\t Iteration 792 Loss Train: 0.4106166362762451\n",
      "Epoch: 4 \t\t\t Iteration 793 Loss Train: 0.40982532501220703\n",
      "Epoch: 4 \t\t\t Iteration 794 Loss Train: 0.5130792856216431\n",
      "Epoch: 4 \t\t\t Iteration 795 Loss Train: 0.4452192783355713\n",
      "Epoch: 4 \t\t\t Iteration 796 Loss Train: 0.3579583168029785\n",
      "Epoch: 4 \t\t\t Iteration 797 Loss Train: 0.2679080069065094\n",
      "Epoch: 4 \t\t\t Iteration 798 Loss Train: 0.38819950819015503\n",
      "Epoch: 4 \t\t\t Iteration 799 Loss Train: 0.31952425837516785\n",
      "Epoch: 4 \t\t\t Iteration 800 Loss Train: 0.3980754017829895\n",
      "Epoch: 4 \t\t\t Iteration 801 Loss Train: 0.2799673080444336\n",
      "Epoch: 4 \t\t\t Iteration 802 Loss Train: 0.2253040373325348\n",
      "Epoch: 4 \t\t\t Iteration 803 Loss Train: 0.6146063804626465\n",
      "Epoch: 4 \t\t\t Iteration 804 Loss Train: 0.3987821936607361\n",
      "Epoch: 4 \t\t\t Iteration 805 Loss Train: 0.45389196276664734\n",
      "Epoch: 4 \t\t\t Iteration 806 Loss Train: 0.30024439096450806\n",
      "Epoch: 4 \t\t\t Iteration 807 Loss Train: 0.46298572421073914\n",
      "Epoch: 4 \t\t\t Iteration 808 Loss Train: 0.4831341803073883\n",
      "Epoch: 4 \t\t\t Iteration 809 Loss Train: 0.6180177927017212\n",
      "Epoch: 4 \t\t\t Iteration 810 Loss Train: 0.47227028012275696\n",
      "Epoch: 4 \t\t\t Iteration 811 Loss Train: 0.3374549150466919\n",
      "Epoch: 4 \t\t\t Iteration 812 Loss Train: 0.26333874464035034\n",
      "Epoch: 4 \t\t\t Iteration 813 Loss Train: 0.2493848204612732\n",
      "Epoch: 4 \t\t\t Iteration 814 Loss Train: 0.47792527079582214\n",
      "Epoch: 4 \t\t\t Iteration 815 Loss Train: 0.5201650857925415\n",
      "Epoch: 4 \t\t\t Iteration 816 Loss Train: 0.31845954060554504\n",
      "Epoch: 4 \t\t\t Iteration 817 Loss Train: 0.37220409512519836\n",
      "Epoch: 4 \t\t\t Iteration 818 Loss Train: 0.43994081020355225\n",
      "Epoch: 4 \t\t\t Iteration 819 Loss Train: 0.37355977296829224\n",
      "Epoch: 4 \t\t\t Iteration 820 Loss Train: 0.46705663204193115\n",
      "Epoch: 4 \t\t\t Iteration 821 Loss Train: 0.46767646074295044\n",
      "Epoch: 4 \t\t\t Iteration 822 Loss Train: 0.490792840719223\n",
      "Epoch: 4 \t\t\t Iteration 823 Loss Train: 0.5032339096069336\n",
      "Epoch: 4 \t\t\t Iteration 824 Loss Train: 0.470383882522583\n",
      "Epoch: 4 \t\t\t Iteration 825 Loss Train: 0.3562697470188141\n",
      "Epoch: 4 \t\t\t Iteration 826 Loss Train: 0.32838642597198486\n",
      "Epoch: 4 \t\t\t Iteration 827 Loss Train: 0.38626596331596375\n",
      "Epoch: 4 \t\t\t Iteration 828 Loss Train: 0.5928155779838562\n",
      "Epoch: 4 \t\t\t Iteration 829 Loss Train: 0.4943677484989166\n",
      "Epoch: 4 \t\t\t Iteration 830 Loss Train: 0.5614777207374573\n",
      "Epoch: 4 \t\t\t Iteration 831 Loss Train: 0.40576958656311035\n",
      "Epoch: 4 \t\t\t Iteration 832 Loss Train: 0.2362065613269806\n",
      "Epoch: 4 \t\t\t Iteration 833 Loss Train: 0.47317010164260864\n",
      "Epoch: 4 \t\t\t Iteration 834 Loss Train: 0.2587778568267822\n",
      "Epoch: 4 \t\t\t Iteration 835 Loss Train: 0.35236918926239014\n",
      "Epoch: 4 \t\t\t Iteration 836 Loss Train: 0.32208341360092163\n",
      "Epoch: 4 \t\t\t Iteration 837 Loss Train: 0.5971490144729614\n",
      "Epoch: 4 \t\t\t Iteration 838 Loss Train: 0.4564000070095062\n",
      "Epoch: 4 \t\t\t Iteration 839 Loss Train: 0.4728105068206787\n",
      "Epoch: 4 \t\t\t Iteration 840 Loss Train: 0.4082382917404175\n",
      "Epoch: 4 \t\t\t Iteration 841 Loss Train: 0.5173721313476562\n",
      "Epoch: 4 \t\t\t Iteration 842 Loss Train: 0.2464098334312439\n",
      "Epoch: 4 \t\t\t Iteration 843 Loss Train: 0.29582327604293823\n",
      "Epoch: 4 \t\t\t Iteration 844 Loss Train: 0.5026164650917053\n",
      "Epoch: 4 \t\t\t Iteration 845 Loss Train: 0.5073310136795044\n",
      "Epoch: 4 \t\t\t Iteration 846 Loss Train: 0.3427692651748657\n",
      "Epoch: 4 \t\t\t Iteration 847 Loss Train: 0.34088653326034546\n",
      "Epoch: 4 \t\t\t Iteration 848 Loss Train: 0.4204455614089966\n",
      "Epoch: 4 \t\t\t Iteration 849 Loss Train: 0.31659168004989624\n",
      "Epoch: 4 \t\t\t Iteration 850 Loss Train: 0.2954883277416229\n",
      "Epoch: 4 \t\t\t Iteration 851 Loss Train: 0.4501379132270813\n",
      "Epoch: 4 \t\t\t Iteration 852 Loss Train: 0.3985831141471863\n",
      "Epoch: 4 \t\t\t Iteration 853 Loss Train: 0.39251795411109924\n",
      "Epoch: 4 \t\t\t Iteration 854 Loss Train: 0.44027429819107056\n",
      "Epoch: 4 \t\t\t Iteration 855 Loss Train: 0.3203946352005005\n",
      "Epoch: 4 \t\t\t Iteration 856 Loss Train: 0.47912612557411194\n",
      "Epoch: 4 \t\t\t Iteration 857 Loss Train: 0.34135201573371887\n",
      "Epoch: 4 \t\t\t Iteration 858 Loss Train: 0.29745280742645264\n",
      "Epoch: 4 \t\t\t Iteration 859 Loss Train: 0.30569401383399963\n",
      "Epoch: 4 \t\t\t Iteration 860 Loss Train: 0.4037451148033142\n",
      "Epoch: 4 \t\t\t Iteration 861 Loss Train: 0.28465256094932556\n",
      "Epoch: 4 \t\t\t Iteration 862 Loss Train: 0.2948959469795227\n",
      "Epoch: 4 \t\t\t Iteration 863 Loss Train: 0.42692840099334717\n",
      "Epoch: 4 \t\t\t Iteration 864 Loss Train: 0.7154459357261658\n",
      "Epoch: 4 \t\t\t Iteration 865 Loss Train: 0.40242040157318115\n",
      "Epoch: 4 \t\t\t Iteration 866 Loss Train: 0.25053083896636963\n",
      "Epoch: 4 \t\t\t Iteration 867 Loss Train: 0.39012420177459717\n",
      "Epoch: 4 \t\t\t Iteration 868 Loss Train: 0.635304868221283\n",
      "Epoch: 4 \t\t\t Iteration 869 Loss Train: 0.43900734186172485\n",
      "Epoch: 4 \t\t\t Iteration 870 Loss Train: 0.4782196879386902\n",
      "Epoch: 4 \t\t\t Iteration 871 Loss Train: 0.43984556198120117\n",
      "Epoch: 4 \t\t\t Iteration 872 Loss Train: 0.38569897413253784\n",
      "Epoch: 4 \t\t\t Iteration 873 Loss Train: 0.5666428804397583\n",
      "Epoch: 4 \t\t\t Iteration 874 Loss Train: 0.5593006610870361\n",
      "Epoch: 4 \t\t\t Iteration 875 Loss Train: 0.35921722650527954\n",
      "Epoch: 4 \t\t\t Iteration 876 Loss Train: 0.29826903343200684\n",
      "Epoch: 4 \t\t\t Iteration 877 Loss Train: 0.5011053085327148\n",
      "Epoch: 4 \t\t\t Iteration 878 Loss Train: 0.3932676911354065\n",
      "Epoch: 4 \t\t\t Iteration 879 Loss Train: 0.5207395553588867\n",
      "Epoch: 4 \t\t\t Iteration 880 Loss Train: 0.605424702167511\n",
      "Epoch: 4 \t\t\t Iteration 881 Loss Train: 0.4540907144546509\n",
      "Epoch: 4 \t\t\t Iteration 882 Loss Train: 0.4691940248012543\n",
      "Epoch: 4 \t\t\t Iteration 883 Loss Train: 0.3737819194793701\n",
      "Epoch: 4 \t\t\t Iteration 884 Loss Train: 0.6737052202224731\n",
      "Epoch: 4 \t\t\t Iteration 885 Loss Train: 0.4716273248195648\n",
      "Epoch: 4 \t\t\t Iteration 886 Loss Train: 0.30799829959869385\n",
      "Epoch: 4 \t\t\t Iteration 887 Loss Train: 0.33901387453079224\n",
      "Epoch: 4 \t\t\t Iteration 888 Loss Train: 0.3761352300643921\n",
      "Epoch: 4 \t\t\t Iteration 889 Loss Train: 0.3495180010795593\n",
      "Epoch: 4 \t\t\t Iteration 890 Loss Train: 0.7750306129455566\n",
      "Epoch: 4 \t\t\t Iteration 891 Loss Train: 0.4409646987915039\n",
      "Epoch: 4 \t\t\t Iteration 892 Loss Train: 0.45135965943336487\n",
      "Epoch: 4 \t\t\t Iteration 893 Loss Train: 0.3016320466995239\n",
      "Epoch: 4 \t\t\t Iteration 894 Loss Train: 0.40426740050315857\n",
      "Epoch: 4 \t\t\t Iteration 895 Loss Train: 0.5608136653900146\n",
      "Epoch: 4 \t\t\t Iteration 896 Loss Train: 0.30250218510627747\n",
      "Epoch: 4 \t\t\t Iteration 897 Loss Train: 0.46103301644325256\n",
      "Epoch: 4 \t\t\t Iteration 898 Loss Train: 0.4068353772163391\n",
      "Epoch: 4 \t\t\t Iteration 899 Loss Train: 0.5179271697998047\n",
      "Epoch: 4 \t\t\t Iteration 900 Loss Train: 0.2514413595199585\n",
      "Epoch: 4 \t\t\t Iteration 901 Loss Train: 0.5732725858688354\n",
      "Epoch: 4 \t\t\t Iteration 902 Loss Train: 0.4292318820953369\n",
      "Epoch: 4 \t\t\t Iteration 903 Loss Train: 0.44862014055252075\n",
      "Epoch: 4 \t\t\t Iteration 904 Loss Train: 0.3893742561340332\n",
      "Epoch: 4 \t\t\t Iteration 905 Loss Train: 0.40708357095718384\n",
      "Epoch: 4 \t\t\t Iteration 906 Loss Train: 0.2686096429824829\n",
      "Epoch: 4 \t\t\t Iteration 907 Loss Train: 0.5496095418930054\n",
      "Epoch: 4 \t\t\t Iteration 908 Loss Train: 0.43698716163635254\n",
      "Epoch: 4 \t\t\t Iteration 909 Loss Train: 0.3601160943508148\n",
      "Epoch: 4 \t\t\t Iteration 910 Loss Train: 0.5871712565422058\n",
      "Epoch: 4 \t\t\t Iteration 911 Loss Train: 0.5226667523384094\n",
      "Epoch: 4 \t\t\t Iteration 912 Loss Train: 0.4759046137332916\n",
      "Epoch: 4 \t\t\t Iteration 913 Loss Train: 0.3263779580593109\n",
      "Epoch: 4 \t\t\t Iteration 914 Loss Train: 0.3100847601890564\n",
      "Epoch: 4 \t\t\t Iteration 915 Loss Train: 0.4697157144546509\n",
      "Epoch: 4 \t\t\t Iteration 916 Loss Train: 0.41310280561447144\n",
      "Epoch: 4 \t\t\t Iteration 917 Loss Train: 0.4910850524902344\n",
      "Epoch: 4 \t\t\t Iteration 918 Loss Train: 0.40161943435668945\n",
      "Epoch: 4 \t\t\t Iteration 919 Loss Train: 0.3842219114303589\n",
      "Epoch: 4 \t\t\t Iteration 920 Loss Train: 0.4542715549468994\n",
      "Epoch: 4 \t\t\t Iteration 921 Loss Train: 0.4072479009628296\n",
      "Epoch: 4 \t\t\t Iteration 922 Loss Train: 0.28150442242622375\n",
      "Epoch: 4 \t\t\t Iteration 923 Loss Train: 0.5400179028511047\n",
      "Epoch: 4 \t\t\t Iteration 924 Loss Train: 0.46904563903808594\n",
      "Epoch: 4 \t\t\t Iteration 925 Loss Train: 0.4537796676158905\n",
      "Epoch: 4 \t\t\t Iteration 926 Loss Train: 0.3554726839065552\n",
      "Epoch: 4 \t\t\t Iteration 927 Loss Train: 0.4801914691925049\n",
      "Epoch: 4 \t\t\t Iteration 928 Loss Train: 0.5340747237205505\n",
      "Epoch: 4 \t\t\t Iteration 929 Loss Train: 0.46753522753715515\n",
      "Epoch: 4 \t\t\t Iteration 930 Loss Train: 0.37856629490852356\n",
      "Epoch: 4 \t\t\t Iteration 931 Loss Train: 0.41470763087272644\n",
      "Epoch: 4 \t\t\t Iteration 932 Loss Train: 0.3257545828819275\n",
      "Epoch: 4 \t\t\t Iteration 933 Loss Train: 0.3448386490345001\n",
      "Epoch: 4 \t\t\t Iteration 934 Loss Train: 0.5166033506393433\n",
      "Epoch: 4 \t\t\t Iteration 935 Loss Train: 0.4281464219093323\n",
      "Epoch: 4 \t\t\t Iteration 936 Loss Train: 0.5669737458229065\n",
      "Epoch: 4 \t\t\t Iteration 937 Loss Train: 0.3795197010040283\n",
      "Epoch: 4 \t\t\t Iteration 938 Loss Train: 0.5327805280685425\n",
      "Epoch: 4 \t\t\t Iteration 939 Loss Train: 0.41040194034576416\n",
      "Epoch: 4 \t\t\t Iteration 940 Loss Train: 0.42965686321258545\n",
      "Epoch: 4 \t\t\t Iteration 941 Loss Train: 0.36620041728019714\n",
      "Epoch: 4 \t\t\t Iteration 942 Loss Train: 0.46063232421875\n",
      "Epoch: 4 \t\t\t Iteration 943 Loss Train: 0.3911474943161011\n",
      "Epoch: 4 \t\t\t Iteration 944 Loss Train: 0.34352731704711914\n",
      "Epoch: 4 \t\t\t Iteration 945 Loss Train: 0.4111498296260834\n",
      "Epoch: 4 \t\t\t Iteration 946 Loss Train: 0.324520468711853\n",
      "Epoch: 4 \t\t\t Iteration 947 Loss Train: 0.20463263988494873\n",
      "Epoch: 4 \t\t\t Iteration 948 Loss Train: 0.12022818624973297\n",
      "Epoch: 4 \t\t\t Iteration 949 Loss Train: 0.3120321035385132\n",
      "Epoch: 4 \t\t\t Iteration 950 Loss Train: 0.4379168748855591\n",
      "Epoch: 4 \t\t\t Iteration 951 Loss Train: 0.4824044108390808\n",
      "Epoch: 4 \t\t\t Iteration 952 Loss Train: 0.39504945278167725\n",
      "Epoch: 4 \t\t\t Iteration 953 Loss Train: 0.2400752604007721\n",
      "Epoch: 4 \t\t\t Iteration 954 Loss Train: 0.5659109950065613\n",
      "Epoch: 4 \t\t\t Iteration 955 Loss Train: 0.4936754107475281\n",
      "Epoch: 4 \t\t\t Iteration 956 Loss Train: 0.36806559562683105\n",
      "Epoch: 4 \t\t\t Iteration 957 Loss Train: 0.49845078587532043\n",
      "Epoch: 4 \t\t\t Iteration 958 Loss Train: 0.5064233541488647\n",
      "Epoch: 4 \t\t\t Iteration 959 Loss Train: 0.47832849621772766\n",
      "Epoch: 4 \t\t\t Iteration 960 Loss Train: 0.3575459420681\n",
      "Epoch: 4 \t\t\t Iteration 961 Loss Train: 0.7641105055809021\n",
      "Epoch: 4 \t\t\t Iteration 962 Loss Train: 0.38765814900398254\n",
      "Epoch: 4 \t\t\t Iteration 963 Loss Train: 0.4152299165725708\n",
      "Epoch: 4 \t\t\t Iteration 964 Loss Train: 0.386072039604187\n",
      "Epoch: 4 \t\t\t Iteration 965 Loss Train: 0.4466913342475891\n",
      "Epoch: 4 \t\t\t Iteration 966 Loss Train: 0.3807836174964905\n",
      "Epoch: 4 \t\t\t Iteration 967 Loss Train: 0.23024111986160278\n",
      "Epoch: 4 \t\t\t Iteration 968 Loss Train: 0.3616243600845337\n",
      "Epoch: 4 \t\t\t Iteration 969 Loss Train: 0.555014967918396\n",
      "Epoch: 4 \t\t\t Iteration 970 Loss Train: 0.28808608651161194\n",
      "Epoch: 4 \t\t\t Iteration 971 Loss Train: 0.429534375667572\n",
      "Epoch: 4 \t\t\t Iteration 972 Loss Train: 0.5822970867156982\n",
      "Epoch: 4 \t\t\t Iteration 973 Loss Train: 0.5700633525848389\n",
      "Epoch: 4 \t\t\t Iteration 974 Loss Train: 0.399856299161911\n",
      "Epoch: 4 \t\t\t Iteration 975 Loss Train: 0.4906540811061859\n",
      "Epoch: 4 \t\t\t Iteration 976 Loss Train: 0.42032238841056824\n",
      "Epoch: 4 \t\t\t Iteration 977 Loss Train: 0.42323440313339233\n",
      "Epoch: 4 \t\t\t Iteration 978 Loss Train: 0.5154462456703186\n",
      "Epoch: 4 \t\t\t Iteration 979 Loss Train: 0.4361022114753723\n",
      "Epoch: 4 \t\t\t Iteration 980 Loss Train: 0.3246661424636841\n",
      "Epoch: 4 \t\t\t Iteration 981 Loss Train: 0.20451697707176208\n",
      "Epoch: 4 \t\t\t Iteration 982 Loss Train: 0.4217481017112732\n",
      "Epoch: 4 \t\t\t Iteration 983 Loss Train: 0.38780441880226135\n",
      "Epoch: 4 \t\t\t Iteration 984 Loss Train: 0.5591614246368408\n",
      "Epoch: 4 \t\t\t Iteration 985 Loss Train: 0.4387603998184204\n",
      "Epoch: 4 \t\t\t Iteration 986 Loss Train: 0.5233728885650635\n",
      "Epoch: 4 \t\t\t Iteration 987 Loss Train: 0.480297327041626\n",
      "Epoch: 4 \t\t\t Iteration 988 Loss Train: 0.24012158811092377\n",
      "Epoch: 4 \t\t\t Iteration 989 Loss Train: 0.3290126323699951\n",
      "Epoch: 4 \t\t\t Iteration 990 Loss Train: 0.32407963275909424\n",
      "Epoch: 4 \t\t\t Iteration 991 Loss Train: 0.30880019068717957\n",
      "Epoch: 4 \t\t\t Iteration 992 Loss Train: 0.48197004199028015\n",
      "Epoch: 4 \t\t\t Iteration 993 Loss Train: 0.4237057566642761\n",
      "Epoch: 4 \t\t\t Iteration 994 Loss Train: 0.3424484133720398\n",
      "Epoch: 4 \t\t\t Iteration 995 Loss Train: 0.3287939727306366\n",
      "Epoch: 4 \t\t\t Iteration 996 Loss Train: 0.6584651470184326\n",
      "Epoch: 4 \t\t\t Iteration 997 Loss Train: 0.43960970640182495\n",
      "Epoch: 4 \t\t\t Iteration 998 Loss Train: 0.4508246183395386\n",
      "Epoch: 4 \t\t\t Iteration 999 Loss Train: 0.6580391526222229\n",
      "Epoch: 4 \t\t\t Iteration 1000 Loss Train: 0.46770358085632324\n",
      "Epoch: 4 \t\t\t Iteration 1001 Loss Train: 0.4691348075866699\n",
      "Epoch: 4 \t\t\t Iteration 1002 Loss Train: 0.5052266120910645\n",
      "Epoch: 4 \t\t\t Iteration 1003 Loss Train: 0.21263539791107178\n",
      "Epoch: 4 \t\t\t Iteration 1004 Loss Train: 0.3136318325996399\n",
      "Epoch: 4 \t\t\t Iteration 1005 Loss Train: 0.5321530103683472\n",
      "Epoch: 4 \t\t\t Iteration 1006 Loss Train: 0.30028069019317627\n",
      "Epoch: 4 \t\t\t Iteration 1007 Loss Train: 0.45423024892807007\n",
      "Epoch: 4 \t\t\t Iteration 1008 Loss Train: 0.39146992564201355\n",
      "Epoch: 4 \t\t\t Iteration 1009 Loss Train: 0.3796761631965637\n",
      "Epoch: 4 \t\t\t Iteration 1010 Loss Train: 0.49323520064353943\n",
      "Epoch: 4 \t\t\t Iteration 1011 Loss Train: 0.47845152020454407\n",
      "Epoch: 4 \t\t\t Iteration 1012 Loss Train: 0.47232481837272644\n",
      "Epoch: 4 \t\t\t Iteration 1013 Loss Train: 0.24883989989757538\n",
      "Epoch: 4 \t\t\t Iteration 1014 Loss Train: 0.4939333200454712\n",
      "Epoch: 4 \t\t\t Iteration 1015 Loss Train: 0.34438735246658325\n",
      "Epoch: 4 \t\t\t Iteration 1016 Loss Train: 0.387453556060791\n",
      "Epoch: 4 \t\t\t Iteration 1017 Loss Train: 0.5116498470306396\n",
      "Epoch: 4 \t\t\t Iteration 1018 Loss Train: 0.5479907989501953\n",
      "Epoch: 4 \t\t\t Iteration 1019 Loss Train: 0.30566391348838806\n",
      "Epoch: 4 \t\t\t Iteration 1020 Loss Train: 0.4490128755569458\n",
      "Epoch: 4 \t\t\t Iteration 1021 Loss Train: 0.4331132471561432\n",
      "Epoch: 4 \t\t\t Iteration 1022 Loss Train: 0.5621660351753235\n",
      "Epoch: 4 \t\t\t Iteration 1023 Loss Train: 0.4121570587158203\n",
      "Epoch: 4 \t\t\t Iteration 1024 Loss Train: 0.22681143879890442\n",
      "Epoch: 4 \t\t\t Iteration 1025 Loss Train: 0.35931384563446045\n",
      "Epoch: 4 \t\t\t Iteration 1026 Loss Train: 0.41560572385787964\n",
      "Epoch: 4 \t\t\t Iteration 1027 Loss Train: 0.5350955724716187\n",
      "Epoch: 4 \t\t\t Iteration 1028 Loss Train: 0.2978888750076294\n",
      "Epoch: 4 \t\t\t Iteration 1029 Loss Train: 0.12378814816474915\n",
      "Epoch: 4 \t\t\t Iteration 1030 Loss Train: 0.4758304953575134\n",
      "Epoch: 4 \t\t\t Iteration 1031 Loss Train: 0.3541417419910431\n",
      "Epoch: 4 \t\t\t Iteration 1032 Loss Train: 0.4890080690383911\n",
      "Epoch: 4 \t\t\t Iteration 1033 Loss Train: 0.42794564366340637\n",
      "Epoch: 4 \t\t\t Iteration 1034 Loss Train: 0.45157480239868164\n",
      "Epoch: 4 \t\t\t Iteration 1035 Loss Train: 0.3685276210308075\n",
      "Epoch: 4 \t\t\t Iteration 1036 Loss Train: 0.2466607689857483\n",
      "Epoch: 4 \t\t\t Iteration 1037 Loss Train: 0.22298359870910645\n",
      "Epoch: 4 \t\t\t Iteration 1038 Loss Train: 0.45558595657348633\n",
      "Epoch: 4 \t\t\t Iteration 1039 Loss Train: 0.3658967912197113\n",
      "Epoch: 4 \t\t\t Iteration 1040 Loss Train: 0.3898756504058838\n",
      "Epoch: 4 \t\t\t Iteration 1041 Loss Train: 0.589500904083252\n",
      "Epoch: 4 \t\t\t Iteration 1042 Loss Train: 0.49413132667541504\n",
      "Epoch: 4 \t\t\t Iteration 1043 Loss Train: 0.657757580280304\n",
      "Epoch: 4 \t\t\t Iteration 1044 Loss Train: 0.37901613116264343\n",
      "Epoch: 4 \t\t\t Iteration 1045 Loss Train: 0.3819162845611572\n",
      "Epoch: 4 \t\t\t Iteration 1046 Loss Train: 0.3460603952407837\n",
      "Epoch: 4 \t\t\t Iteration 1047 Loss Train: 0.29505932331085205\n",
      "Epoch: 4 \t\t\t Iteration 1048 Loss Train: 0.5651617050170898\n",
      "Epoch: 4 \t\t\t Iteration 1049 Loss Train: 0.36437132954597473\n",
      "Epoch: 4 \t\t\t Iteration 1050 Loss Train: 0.5023751258850098\n",
      "Epoch: 4 \t\t\t Iteration 1051 Loss Train: 0.3546586036682129\n",
      "Epoch: 4 \t\t\t Iteration 1052 Loss Train: 0.5412931442260742\n",
      "Epoch: 4 \t\t\t Iteration 1053 Loss Train: 0.4430769681930542\n",
      "Epoch: 4 \t\t\t Iteration 1054 Loss Train: 0.3418887257575989\n",
      "Epoch: 4 \t\t\t Iteration 1055 Loss Train: 0.5252114534378052\n",
      "Epoch: 4 \t\t\t Iteration 1056 Loss Train: 0.31784787774086\n",
      "Epoch: 4 \t\t\t Iteration 1057 Loss Train: 0.41139477491378784\n",
      "Epoch: 4 \t\t\t Iteration 1058 Loss Train: 0.5193663239479065\n",
      "Epoch: 4 \t\t\t Iteration 1059 Loss Train: 0.4118196368217468\n",
      "Epoch: 4 \t\t\t Iteration 1060 Loss Train: 0.3687245547771454\n",
      "Epoch: 4 \t\t\t Iteration 1061 Loss Train: 0.2207408845424652\n",
      "Epoch: 4 \t\t\t Iteration 1062 Loss Train: 0.49096378684043884\n",
      "Epoch: 4 \t\t\t Iteration 1063 Loss Train: 0.47275978326797485\n",
      "Epoch: 4 \t\t\t Iteration 1064 Loss Train: 0.3017340302467346\n",
      "Epoch: 4 \t\t\t Iteration 1065 Loss Train: 0.42776399850845337\n",
      "Epoch: 4 \t\t\t Iteration 1066 Loss Train: 0.5652525424957275\n",
      "Epoch: 4 \t\t\t Iteration 1067 Loss Train: 0.42240405082702637\n",
      "Epoch: 4 \t\t\t Iteration 1068 Loss Train: 0.42066463828086853\n",
      "Epoch: 4 \t\t\t Iteration 1069 Loss Train: 0.2836840748786926\n",
      "Epoch: 4 \t\t\t Iteration 1070 Loss Train: 0.33077389001846313\n",
      "Epoch: 4 \t\t\t Iteration 1071 Loss Train: 0.20045018196105957\n",
      "Epoch: 4 \t\t\t Iteration 1072 Loss Train: 0.41747206449508667\n",
      "Epoch: 4 \t\t\t Iteration 1073 Loss Train: 0.44547954201698303\n",
      "Epoch: 4 \t\t\t Iteration 1074 Loss Train: 0.6766602993011475\n",
      "Epoch: 4 \t\t\t Iteration 1075 Loss Train: 0.29175424575805664\n",
      "Epoch: 4 \t\t\t Iteration 1076 Loss Train: 0.3532847464084625\n",
      "Epoch: 4 \t\t\t Iteration 1077 Loss Train: 0.33975380659103394\n",
      "Epoch: 4 \t\t\t Iteration 1078 Loss Train: 0.40302416682243347\n",
      "Epoch: 4 \t\t\t Iteration 1079 Loss Train: 0.3575917184352875\n",
      "Epoch: 4 \t\t\t Iteration 1080 Loss Train: 0.2634713351726532\n",
      "Epoch: 4 \t\t\t Iteration 1081 Loss Train: 0.3164214491844177\n",
      "Epoch: 4 \t\t\t Iteration 1082 Loss Train: 0.2620680332183838\n",
      "Epoch: 4 \t\t\t Iteration 1083 Loss Train: 0.478779137134552\n",
      "Epoch: 4 \t\t\t Iteration 1084 Loss Train: 0.3815958499908447\n",
      "Epoch: 4 \t\t\t Iteration 1085 Loss Train: 0.5270012617111206\n",
      "Epoch: 4 \t\t\t Iteration 1086 Loss Train: 0.47820794582366943\n",
      "Epoch: 4 \t\t\t Iteration 1087 Loss Train: 0.464191734790802\n",
      "Epoch: 4 \t\t\t Iteration 1088 Loss Train: 0.5832397937774658\n",
      "Epoch: 4 \t\t\t Iteration 1089 Loss Train: 0.5768948793411255\n",
      "Epoch: 4 \t\t\t Iteration 1090 Loss Train: 0.4333018660545349\n",
      "Epoch: 4 \t\t\t Iteration 1091 Loss Train: 0.42677611112594604\n",
      "Epoch: 4 \t\t\t Iteration 1092 Loss Train: 0.4179248809814453\n",
      "Epoch: 4 \t\t\t Iteration 1093 Loss Train: 0.44037556648254395\n",
      "Epoch: 4 \t\t\t Iteration 1094 Loss Train: 0.47796350717544556\n",
      "Epoch: 4 \t\t\t Iteration 1095 Loss Train: 0.30339640378952026\n",
      "Epoch: 4 \t\t\t Iteration 1096 Loss Train: 0.3740517497062683\n",
      "Epoch: 4 \t\t\t Iteration 1097 Loss Train: 0.7898727655410767\n",
      "Epoch: 4 \t\t\t Iteration 1098 Loss Train: 0.48857101798057556\n",
      "Epoch: 4 \t\t\t Iteration 1099 Loss Train: 0.3956855237483978\n",
      "Epoch: 4 \t\t\t Iteration 1100 Loss Train: 0.41494908928871155\n",
      "Epoch: 4 \t\t\t Iteration 1101 Loss Train: 0.28745710849761963\n",
      "Epoch: 4 \t\t\t Iteration 1102 Loss Train: 0.32508373260498047\n",
      "Epoch: 4 \t\t\t Iteration 1103 Loss Train: 0.45061004161834717\n",
      "Epoch: 4 \t\t\t Iteration 1104 Loss Train: 0.42017287015914917\n",
      "Epoch: 4 \t\t\t Iteration 1105 Loss Train: 0.3934279978275299\n",
      "Epoch: 4 \t\t\t Iteration 1106 Loss Train: 0.3902547061443329\n",
      "Epoch: 4 \t\t\t Iteration 1107 Loss Train: 0.40410178899765015\n",
      "Epoch: 4 \t\t\t Iteration 1108 Loss Train: 0.2268374115228653\n",
      "Epoch: 4 \t\t\t Iteration 1109 Loss Train: 0.4905638098716736\n",
      "Epoch: 4 \t\t\t Iteration 1110 Loss Train: 0.404945433139801\n",
      "Epoch: 4 \t\t\t Iteration 1111 Loss Train: 0.5304661989212036\n",
      "Epoch: 4 \t\t\t Iteration 1112 Loss Train: 0.3080873489379883\n",
      "Epoch: 4 \t\t\t Iteration 1113 Loss Train: 0.4646281599998474\n",
      "Epoch: 4 \t\t\t Iteration 1114 Loss Train: 0.321749210357666\n",
      "Epoch: 4 \t\t\t Iteration 1115 Loss Train: 0.3413792550563812\n",
      "Epoch: 4 \t\t\t Iteration 1116 Loss Train: 0.4744870662689209\n",
      "Epoch: 4 \t\t\t Iteration 1117 Loss Train: 0.4524339437484741\n",
      "Epoch: 4 \t\t\t Iteration 1118 Loss Train: 0.33730995655059814\n",
      "Epoch: 4 \t\t\t Iteration 1119 Loss Train: 0.3587665259838104\n",
      "Epoch: 4 \t\t\t Iteration 1120 Loss Train: 0.3969706594944\n",
      "Epoch: 4 \t\t\t Iteration 1121 Loss Train: 0.373649001121521\n",
      "Epoch: 4 \t\t\t Iteration 1122 Loss Train: 0.4594777226448059\n",
      "Epoch: 4 \t\t\t Iteration 1123 Loss Train: 0.4559195041656494\n",
      "Epoch: 4 \t\t\t Iteration 1124 Loss Train: 0.4943419098854065\n",
      "Epoch: 4 \t\t\t Iteration 1125 Loss Train: 0.4347565770149231\n",
      "Epoch: 4 \t\t\t Iteration 1126 Loss Train: 0.4385829269886017\n",
      "Epoch: 4 \t\t\t Iteration 1127 Loss Train: 0.41058966517448425\n",
      "Epoch: 4 \t\t\t Iteration 1128 Loss Train: 0.26346415281295776\n",
      "Epoch: 4 \t\t\t Iteration 1129 Loss Train: 0.601884126663208\n",
      "Epoch: 4 \t\t\t Iteration 1130 Loss Train: 0.2385362982749939\n",
      "Epoch: 4 \t\t\t Iteration 1131 Loss Train: 0.5028148889541626\n",
      "Epoch: 4 \t\t\t Iteration 1132 Loss Train: 0.3728374242782593\n",
      "Epoch: 4 \t\t\t Iteration 1133 Loss Train: 0.5076807737350464\n",
      "Epoch: 4 \t\t\t Iteration 1134 Loss Train: 0.3107840120792389\n",
      "Epoch: 4 \t\t\t Iteration 1135 Loss Train: 0.2582944929599762\n",
      "Epoch: 4 \t\t\t Iteration 1136 Loss Train: 0.5730178952217102\n",
      "Epoch: 4 \t\t\t Iteration 1137 Loss Train: 0.410298228263855\n",
      "Epoch: 4 \t\t\t Iteration 1138 Loss Train: 0.3296002447605133\n",
      "Epoch: 4 \t\t\t Iteration 1139 Loss Train: 0.3343610167503357\n",
      "Epoch: 4 \t\t\t Iteration 1140 Loss Train: 0.45344871282577515\n",
      "Epoch: 4 \t\t\t Iteration 1141 Loss Train: 0.549136757850647\n",
      "Epoch: 4 \t\t\t Iteration 1142 Loss Train: 0.36828333139419556\n",
      "Epoch: 4 \t\t\t Iteration 1143 Loss Train: 0.3752233386039734\n",
      "Epoch: 4 \t\t\t Iteration 1144 Loss Train: 0.5009826421737671\n",
      "Epoch: 4 \t\t\t Iteration 1145 Loss Train: 0.48836174607276917\n",
      "Epoch: 4 \t\t\t Iteration 1146 Loss Train: 0.45418745279312134\n",
      "Epoch: 4 \t\t\t Iteration 1147 Loss Train: 0.40187060832977295\n",
      "Epoch: 4 \t\t\t Iteration 1148 Loss Train: 0.38236352801322937\n",
      "Epoch: 4 \t\t\t Iteration 1149 Loss Train: 0.37059396505355835\n",
      "Epoch: 4 \t\t\t Iteration 1150 Loss Train: 0.35877472162246704\n",
      "Epoch: 4 \t\t\t Iteration 1151 Loss Train: 0.545521080493927\n",
      "Epoch: 4 \t\t\t Iteration 1152 Loss Train: 0.5127750635147095\n",
      "Epoch: 4 \t\t\t Iteration 1153 Loss Train: 0.2901599705219269\n",
      "Epoch: 4 \t\t\t Iteration 1154 Loss Train: 0.5593956708908081\n",
      "Epoch: 4 \t\t\t Iteration 1155 Loss Train: 0.3584963381290436\n",
      "Epoch: 4 \t\t\t Iteration 1156 Loss Train: 0.41247281432151794\n",
      "Epoch: 4 \t\t\t Iteration 1157 Loss Train: 0.4066159725189209\n",
      "Epoch: 4 \t\t\t Iteration 1158 Loss Train: 0.36266136169433594\n",
      "Epoch: 4 \t\t\t Iteration 1159 Loss Train: 0.31779128313064575\n",
      "Epoch: 4 \t\t\t Iteration 1160 Loss Train: 0.31992676854133606\n",
      "Epoch: 4 \t\t\t Iteration 1161 Loss Train: 0.23229490220546722\n",
      "Epoch: 4 \t\t\t Iteration 1162 Loss Train: 0.25274717807769775\n",
      "Epoch: 4 \t\t\t Iteration 1163 Loss Train: 0.171275794506073\n",
      "Epoch: 4 \t\t\t Iteration 1164 Loss Train: 0.6109915971755981\n",
      "Epoch: 4 \t\t\t Iteration 1165 Loss Train: 0.3690088987350464\n",
      "Epoch: 4 \t\t\t Iteration 1166 Loss Train: 0.32837021350860596\n",
      "Epoch: 4 \t\t\t Iteration 1167 Loss Train: 0.30606192350387573\n",
      "Epoch: 4 \t\t\t Iteration 1168 Loss Train: 0.42822569608688354\n",
      "Epoch: 4 \t\t\t Iteration 1169 Loss Train: 0.36311811208724976\n",
      "Epoch: 4 \t\t\t Iteration 1170 Loss Train: 0.5460944175720215\n",
      "Epoch: 4 \t\t\t Iteration 1171 Loss Train: 0.433464378118515\n",
      "Epoch: 4 \t\t\t Iteration 1172 Loss Train: 0.42270809412002563\n",
      "Epoch: 4 \t\t\t Iteration 1173 Loss Train: 0.40713489055633545\n",
      "Epoch: 4 \t\t\t Iteration 1174 Loss Train: 0.3258417844772339\n",
      "Epoch: 4 \t\t\t Iteration 1175 Loss Train: 0.3939621150493622\n",
      "Epoch: 4 \t\t\t Iteration 1176 Loss Train: 0.46467268466949463\n",
      "Epoch: 4 \t\t\t Iteration 1177 Loss Train: 0.5923604369163513\n",
      "Epoch: 4 \t\t\t Iteration 1178 Loss Train: 0.4319998323917389\n",
      "Epoch: 4 \t\t\t Iteration 1179 Loss Train: 0.33972105383872986\n",
      "Epoch: 4 \t\t\t Iteration 1180 Loss Train: 0.32147207856178284\n",
      "Epoch: 4 \t\t\t Iteration 1181 Loss Train: 0.6862308382987976\n",
      "Epoch: 4 \t\t\t Iteration 1182 Loss Train: 0.39885082840919495\n",
      "Epoch: 4 \t\t\t Iteration 1183 Loss Train: 0.48822295665740967\n",
      "Epoch: 4 \t\t\t Iteration 1184 Loss Train: 0.4767928123474121\n",
      "Epoch: 4 \t\t\t Iteration 1185 Loss Train: 0.5201531052589417\n",
      "Epoch: 4 \t\t\t Iteration 1186 Loss Train: 0.244387686252594\n",
      "Epoch: 4 \t\t\t Iteration 1187 Loss Train: 0.32396984100341797\n",
      "Epoch: 4 \t\t\t Iteration 1188 Loss Train: 0.4584815502166748\n",
      "Epoch: 4 \t\t\t Iteration 1189 Loss Train: 0.4550613462924957\n",
      "Epoch: 4 \t\t\t Iteration 1190 Loss Train: 0.37524187564849854\n",
      "Epoch: 4 \t\t\t Iteration 1191 Loss Train: 0.34588944911956787\n",
      "Epoch: 4 \t\t\t Iteration 1192 Loss Train: 0.34849217534065247\n",
      "Epoch: 4 \t\t\t Iteration 1193 Loss Train: 0.4008852243423462\n",
      "Epoch: 4 \t\t\t Iteration 1194 Loss Train: 0.46577978134155273\n",
      "Epoch: 4 \t\t\t Iteration 1195 Loss Train: 0.4287036061286926\n",
      "Epoch: 4 \t\t\t Iteration 1196 Loss Train: 0.6836172342300415\n",
      "Epoch: 4 \t\t\t Iteration 1197 Loss Train: 0.36886048316955566\n",
      "Epoch: 4 \t\t\t Iteration 1198 Loss Train: 0.5743217468261719\n",
      "Epoch: 4 \t\t\t Iteration 1199 Loss Train: 0.33804798126220703\n",
      "Epoch: 4 \t\t\t Iteration 1200 Loss Train: 0.3899061381816864\n",
      "Epoch: 4 \t\t\t Iteration 1201 Loss Train: 0.5068234205245972\n",
      "Epoch: 4 \t\t\t Iteration 1202 Loss Train: 0.27032914757728577\n",
      "Epoch: 4 \t\t\t Iteration 1203 Loss Train: 0.4128616452217102\n",
      "Epoch: 4 \t\t\t Iteration 1204 Loss Train: 0.5064239501953125\n",
      "Epoch: 4 \t\t\t Iteration 1205 Loss Train: 0.4395388066768646\n",
      "Epoch: 4 \t\t\t Iteration 1206 Loss Train: 0.5038264989852905\n",
      "Epoch: 4 \t\t\t Iteration 1207 Loss Train: 0.4972972273826599\n",
      "Epoch: 4 \t\t\t Iteration 1208 Loss Train: 0.28904545307159424\n",
      "Epoch: 4 \t\t\t Iteration 1209 Loss Train: 0.3253883719444275\n",
      "Epoch: 4 \t\t\t Iteration 1210 Loss Train: 0.2846105694770813\n",
      "Epoch: 4 \t\t\t Iteration 1211 Loss Train: 0.6036906242370605\n",
      "Epoch: 4 \t\t\t Iteration 1212 Loss Train: 0.43241605162620544\n",
      "Epoch: 4 \t\t\t Iteration 1213 Loss Train: 0.3963790237903595\n",
      "Epoch: 4 \t\t\t Iteration 1214 Loss Train: 0.46960750222206116\n",
      "Epoch: 4 \t\t\t Iteration 1215 Loss Train: 0.3617778718471527\n",
      "Epoch: 4 \t\t\t Iteration 1216 Loss Train: 0.28676873445510864\n",
      "Epoch: 4 \t\t\t Iteration 1217 Loss Train: 0.5932910442352295\n",
      "Epoch: 4 \t\t\t Iteration 1218 Loss Train: 0.5285722613334656\n",
      "Epoch: 4 \t\t\t Iteration 1219 Loss Train: 0.30205193161964417\n",
      "Epoch: 4 \t\t\t Iteration 1220 Loss Train: 0.540280818939209\n",
      "Epoch: 4 \t\t\t Iteration 1221 Loss Train: 0.2744092345237732\n",
      "Epoch: 4 \t\t\t Iteration 1222 Loss Train: 0.26744526624679565\n",
      "Epoch: 4 \t\t\t Iteration 1223 Loss Train: 0.41883543133735657\n",
      "Epoch: 4 \t\t\t Iteration 1224 Loss Train: 0.35705918073654175\n",
      "Epoch: 4 \t\t\t Iteration 1225 Loss Train: 0.3243035078048706\n",
      "Epoch: 4 \t\t\t Iteration 1226 Loss Train: 0.469513475894928\n",
      "Epoch: 4 \t\t\t Iteration 1227 Loss Train: 0.42639270424842834\n",
      "Epoch: 4 \t\t\t Iteration 1228 Loss Train: 0.431214302778244\n",
      "Epoch: 4 \t\t\t Iteration 1229 Loss Train: 0.5291711091995239\n",
      "Epoch: 4 \t\t\t Iteration 1230 Loss Train: 0.3268328309059143\n",
      "Epoch: 4 \t\t\t Iteration 1231 Loss Train: 0.32828494906425476\n",
      "Epoch: 4 \t\t\t Iteration 1232 Loss Train: 0.44680920243263245\n",
      "Epoch: 4 \t\t\t Iteration 1233 Loss Train: 0.29512789845466614\n",
      "Epoch: 4 \t\t\t Iteration 1234 Loss Train: 0.5107601881027222\n",
      "Epoch: 4 \t\t\t Iteration 1235 Loss Train: 0.34969767928123474\n",
      "Epoch: 4 \t\t\t Iteration 1236 Loss Train: 0.36756956577301025\n",
      "Epoch: 4 \t\t\t Iteration 1237 Loss Train: 0.3721015453338623\n",
      "Epoch: 4 \t\t\t Iteration 1238 Loss Train: 0.7120724320411682\n",
      "Epoch: 4 \t\t\t Iteration 1239 Loss Train: 0.5264948606491089\n",
      "Epoch: 4 \t\t\t Iteration 1240 Loss Train: 0.45741090178489685\n",
      "Epoch: 4 \t\t\t Iteration 1241 Loss Train: 0.36287134885787964\n",
      "Epoch: 4 \t\t\t Iteration 1242 Loss Train: 0.4192962050437927\n",
      "Epoch: 4 \t\t\t Iteration 1243 Loss Train: 0.3889129161834717\n",
      "Epoch: 4 \t\t\t Iteration 1244 Loss Train: 0.5077382922172546\n",
      "Epoch: 4 \t\t\t Iteration 1245 Loss Train: 0.21445374190807343\n",
      "Epoch: 4 \t\t\t Iteration 1246 Loss Train: 0.44965142011642456\n",
      "Epoch: 4 \t\t\t Iteration 1247 Loss Train: 0.3548392355442047\n",
      "Epoch: 4 \t\t\t Iteration 1248 Loss Train: 0.2129482626914978\n",
      "Epoch: 4 \t\t\t Iteration 1249 Loss Train: 0.6678017973899841\n",
      "Epoch: 4 \t\t\t Iteration 1250 Loss Train: 0.4044463336467743\n",
      "Epoch: 4 \t\t\t Iteration 1251 Loss Train: 0.37504416704177856\n",
      "Epoch: 4 \t\t\t Iteration 1252 Loss Train: 0.4729047417640686\n",
      "Epoch: 4 \t\t\t Iteration 1253 Loss Train: 0.3847694396972656\n",
      "Epoch: 4 \t\t\t Iteration 1254 Loss Train: 0.4417565166950226\n",
      "Epoch: 4 \t\t\t Iteration 1255 Loss Train: 0.2659972012042999\n",
      "Epoch: 4 \t\t\t Iteration 1256 Loss Train: 0.37325868010520935\n",
      "Epoch: 4 \t\t\t Iteration 1257 Loss Train: 0.4017096757888794\n",
      "Epoch: 4 \t\t\t Iteration 1258 Loss Train: 0.35894647240638733\n",
      "Epoch: 4 \t\t\t Iteration 1259 Loss Train: 0.49191081523895264\n",
      "Epoch: 4 \t\t\t Iteration 1260 Loss Train: 0.3934115767478943\n",
      "Epoch: 4 \t\t\t Iteration 1261 Loss Train: 0.16661666333675385\n",
      "Epoch: 4 \t\t\t Iteration 1262 Loss Train: 0.2625487744808197\n",
      "Epoch: 4 \t\t\t Iteration 1263 Loss Train: 0.41010022163391113\n",
      "Epoch: 4 \t\t\t Iteration 1264 Loss Train: 0.4182720184326172\n",
      "Epoch: 4 \t\t\t Iteration 1265 Loss Train: 0.4623803496360779\n",
      "Epoch: 4 \t\t\t Iteration 1266 Loss Train: 0.5428330898284912\n",
      "Epoch: 4 \t\t\t Iteration 1267 Loss Train: 0.40464478731155396\n",
      "Epoch: 4 \t\t\t Iteration 1268 Loss Train: 0.33118024468421936\n",
      "Epoch: 4 \t\t\t Iteration 1269 Loss Train: 0.36966782808303833\n",
      "Epoch: 4 \t\t\t Iteration 1270 Loss Train: 0.4705492854118347\n",
      "Epoch: 4 \t\t\t Iteration 1271 Loss Train: 0.3589681386947632\n",
      "Epoch: 4 \t\t\t Iteration 1272 Loss Train: 0.2585660219192505\n",
      "Epoch: 4 \t\t\t Iteration 1273 Loss Train: 0.3751369118690491\n",
      "Epoch: 4 \t\t\t Iteration 1274 Loss Train: 0.33476459980010986\n",
      "Epoch: 4 \t\t\t Iteration 1275 Loss Train: 0.3277919292449951\n",
      "Epoch: 4 \t\t\t Iteration 1276 Loss Train: 0.43622249364852905\n",
      "Epoch: 4 \t\t\t Iteration 1277 Loss Train: 0.33859777450561523\n",
      "Epoch: 4 \t\t\t Iteration 1278 Loss Train: 0.3351139426231384\n",
      "Epoch: 4 \t\t\t Iteration 1279 Loss Train: 0.35563308000564575\n",
      "Epoch: 4 \t\t\t Iteration 1280 Loss Train: 0.35992759466171265\n",
      "Epoch: 4 \t\t\t Iteration 1281 Loss Train: 0.3957556486129761\n",
      "Epoch: 4 \t\t\t Iteration 1282 Loss Train: 0.22827184200286865\n",
      "Epoch: 4 \t\t\t Iteration 1283 Loss Train: 0.30094456672668457\n",
      "Epoch: 4 \t\t\t Iteration 1284 Loss Train: 0.6517921686172485\n",
      "Epoch: 4 \t\t\t Iteration 1285 Loss Train: 0.39127886295318604\n",
      "Epoch: 4 \t\t\t Iteration 1286 Loss Train: 0.5155119895935059\n",
      "Epoch: 4 \t\t\t Iteration 1287 Loss Train: 0.40715816617012024\n",
      "Epoch: 4 \t\t\t Iteration 1288 Loss Train: 0.36712878942489624\n",
      "Epoch: 4 \t\t\t Iteration 1289 Loss Train: 0.3120059669017792\n",
      "Epoch: 4 \t\t\t Iteration 1290 Loss Train: 0.3868499994277954\n",
      "Epoch: 4 \t\t\t Iteration 1291 Loss Train: 0.5840503573417664\n",
      "Epoch: 4 \t\t\t Iteration 1292 Loss Train: 0.3435971140861511\n",
      "Epoch: 4 \t\t\t Iteration 1293 Loss Train: 0.4218113124370575\n",
      "Epoch: 4 \t\t\t Iteration 1294 Loss Train: 0.3509030342102051\n",
      "Epoch: 4 \t\t\t Iteration 1295 Loss Train: 0.39332252740859985\n",
      "Epoch: 4 \t\t\t Iteration 1296 Loss Train: 0.32080262899398804\n",
      "Epoch: 4 \t\t\t Iteration 1297 Loss Train: 0.4345458745956421\n",
      "Epoch: 4 \t\t\t Iteration 1298 Loss Train: 0.4291793704032898\n",
      "Epoch: 4 \t\t\t Iteration 1299 Loss Train: 0.7085320949554443\n",
      "Epoch: 4 \t\t\t Iteration 1300 Loss Train: 0.47362059354782104\n",
      "Epoch: 4 \t\t\t Iteration 1301 Loss Train: 0.44923439621925354\n",
      "Epoch: 4 \t\t\t Iteration 1302 Loss Train: 0.5397575497627258\n",
      "Epoch: 4 \t\t\t Iteration 1303 Loss Train: 0.38848063349723816\n",
      "Epoch: 4 \t\t\t Iteration 1304 Loss Train: 0.2843015193939209\n",
      "Epoch: 4 \t\t\t Iteration 1305 Loss Train: 0.28488889336586\n",
      "Epoch: 4 \t\t\t Iteration 1306 Loss Train: 0.3197009861469269\n",
      "Epoch: 4 \t\t\t Iteration 1307 Loss Train: 0.37336796522140503\n",
      "Epoch: 4 \t\t\t Iteration 1308 Loss Train: 0.3542003631591797\n",
      "Epoch: 4 \t\t\t Iteration 1309 Loss Train: 0.5717829465866089\n",
      "Epoch: 4 \t\t\t Iteration 1310 Loss Train: 0.562193751335144\n",
      "Epoch: 4 \t\t\t Iteration 1311 Loss Train: 0.38389867544174194\n",
      "Epoch: 4 \t\t\t Iteration 1312 Loss Train: 0.3194900155067444\n",
      "Epoch: 4 \t\t\t Iteration 1313 Loss Train: 0.7028980255126953\n",
      "Epoch: 4 \t\t\t Iteration 1314 Loss Train: 0.41687458753585815\n",
      "Epoch: 4 \t\t\t Iteration 1315 Loss Train: 0.394060343503952\n",
      "Epoch: 4 \t\t\t Iteration 1316 Loss Train: 0.45954516530036926\n",
      "Epoch: 4 \t\t\t Iteration 1317 Loss Train: 0.3275545537471771\n",
      "Epoch: 4 \t\t\t Iteration 1318 Loss Train: 0.5371463894844055\n",
      "Epoch: 4 \t\t\t Iteration 1319 Loss Train: 0.5119417905807495\n",
      "Epoch: 4 \t\t\t Iteration 1320 Loss Train: 0.33901312947273254\n",
      "Epoch: 4 \t\t\t Iteration 1321 Loss Train: 0.39199185371398926\n",
      "Epoch: 4 \t\t\t Iteration 1322 Loss Train: 0.4277923107147217\n",
      "Epoch: 4 \t\t\t Iteration 1323 Loss Train: 0.27588778734207153\n",
      "Epoch: 4 \t\t\t Iteration 1324 Loss Train: 0.441692590713501\n",
      "Epoch: 4 \t\t\t Iteration 1325 Loss Train: 0.5680376291275024\n",
      "Epoch: 4 \t\t\t Iteration 1326 Loss Train: 0.4470651149749756\n",
      "Epoch: 4 \t\t\t Iteration 1327 Loss Train: 0.4471389651298523\n",
      "Epoch: 4 \t\t\t Iteration 1328 Loss Train: 0.47710418701171875\n",
      "Epoch: 4 \t\t\t Iteration 1329 Loss Train: 0.4375144839286804\n",
      "Epoch: 4 \t\t\t Iteration 1330 Loss Train: 0.37482625246047974\n",
      "Epoch: 4 \t\t\t Iteration 1331 Loss Train: 0.41433584690093994\n",
      "Epoch: 4 \t\t\t Iteration 1332 Loss Train: 0.4828934967517853\n",
      "Epoch: 4 \t\t\t Iteration 1333 Loss Train: 0.4607893228530884\n",
      "Epoch: 4 \t\t\t Iteration 1334 Loss Train: 0.42328310012817383\n",
      "Epoch: 4 \t\t\t Iteration 1335 Loss Train: 0.3271663188934326\n",
      "Epoch: 4 \t\t\t Iteration 1336 Loss Train: 0.1840045303106308\n",
      "Epoch: 4 \t\t\t Iteration 1337 Loss Train: 0.6828389763832092\n",
      "Epoch: 4 \t\t\t Iteration 1338 Loss Train: 0.45800045132637024\n",
      "Epoch: 4 \t\t\t Iteration 1339 Loss Train: 0.4304254651069641\n",
      "Epoch: 4 \t\t\t Iteration 1340 Loss Train: 0.5493899583816528\n",
      "Epoch: 4 \t\t\t Iteration 1341 Loss Train: 0.4056556224822998\n",
      "Epoch: 4 \t\t\t Iteration 1342 Loss Train: 0.35828304290771484\n",
      "Epoch: 4 \t\t\t Iteration 1343 Loss Train: 0.3286058008670807\n",
      "Epoch: 4 \t\t\t Iteration 1344 Loss Train: 0.18382719159126282\n",
      "Epoch: 4 \t\t\t Iteration 1345 Loss Train: 0.3848389983177185\n",
      "Epoch: 4 \t\t\t Iteration 1346 Loss Train: 0.4799414873123169\n",
      "Epoch: 4 \t\t\t Iteration 1347 Loss Train: 0.486276775598526\n",
      "Epoch: 4 \t\t\t Iteration 1348 Loss Train: 0.5490514039993286\n",
      "Epoch: 4 \t\t\t Iteration 1349 Loss Train: 0.4823167622089386\n",
      "Epoch: 4 \t\t\t Iteration 1350 Loss Train: 0.26653599739074707\n",
      "Epoch: 4 \t\t\t Iteration 1351 Loss Train: 0.46954357624053955\n",
      "Epoch: 4 \t\t\t Iteration 1352 Loss Train: 0.2892356216907501\n",
      "Epoch: 4 \t\t\t Iteration 1353 Loss Train: 0.440223753452301\n",
      "Epoch: 4 \t\t\t Iteration 1354 Loss Train: 0.503407895565033\n",
      "Epoch: 4 \t\t\t Iteration 1355 Loss Train: 0.4935247302055359\n",
      "Epoch: 4 \t\t\t Iteration 1356 Loss Train: 0.4718360900878906\n",
      "Epoch: 4 \t\t\t Iteration 1357 Loss Train: 0.37741217017173767\n",
      "Epoch: 4 \t\t\t Iteration 1358 Loss Train: 0.45695632696151733\n",
      "Epoch: 4 \t\t\t Iteration 1359 Loss Train: 0.44714754819869995\n",
      "Epoch: 4 \t\t\t Iteration 1360 Loss Train: 0.45728179812431335\n",
      "Epoch: 4 \t\t\t Iteration 1361 Loss Train: 0.3296636939048767\n",
      "Epoch: 4 \t\t\t Iteration 1362 Loss Train: 0.5362476110458374\n",
      "Epoch: 4 \t\t\t Iteration 1363 Loss Train: 0.2628694176673889\n",
      "Epoch: 4 \t\t\t Iteration 1364 Loss Train: 0.3018757998943329\n",
      "Epoch: 4 \t\t\t Iteration 1365 Loss Train: 0.24089208245277405\n",
      "Epoch: 4 \t\t\t Iteration 1366 Loss Train: 0.724577784538269\n",
      "Epoch: 4 \t\t\t Iteration 1367 Loss Train: 0.39971691370010376\n",
      "Epoch: 4 \t\t\t Iteration 1368 Loss Train: 0.3688405156135559\n",
      "Epoch: 4 \t\t\t Iteration 1369 Loss Train: 0.3962334394454956\n",
      "Epoch: 4 \t\t\t Iteration 1370 Loss Train: 0.44460535049438477\n",
      "Epoch: 4 \t\t\t Iteration 1371 Loss Train: 0.3703101873397827\n",
      "Epoch: 4 \t\t\t Iteration 1372 Loss Train: 0.4092262387275696\n",
      "Epoch: 4 \t\t\t Iteration 1373 Loss Train: 0.3807893991470337\n",
      "Epoch: 4 \t\t\t Iteration 1374 Loss Train: 0.29007482528686523\n",
      "Epoch: 4 \t\t\t Iteration 1375 Loss Train: 0.23923462629318237\n",
      "Epoch: 4 \t\t\t Iteration 1376 Loss Train: 0.7366364002227783\n",
      "Epoch: 4 \t\t\t Iteration 1377 Loss Train: 0.3827020525932312\n",
      "Epoch: 4 \t\t\t Iteration 1378 Loss Train: 0.37274667620658875\n",
      "Epoch: 4 \t\t\t Iteration 1379 Loss Train: 0.29878097772598267\n",
      "Epoch: 4 \t\t\t Iteration 1380 Loss Train: 0.33455559611320496\n",
      "Epoch: 4 \t\t\t Iteration 1381 Loss Train: 0.46255940198898315\n",
      "Epoch: 4 \t\t\t Iteration 1382 Loss Train: 0.256969153881073\n",
      "Epoch: 4 \t\t\t Iteration 1383 Loss Train: 0.6505798697471619\n",
      "Epoch: 4 \t\t\t Iteration 1384 Loss Train: 0.30106714367866516\n",
      "Epoch: 4 \t\t\t Iteration 1385 Loss Train: 0.41645926237106323\n",
      "Epoch: 4 \t\t\t Iteration 1386 Loss Train: 0.4278915524482727\n",
      "Epoch: 4 \t\t\t Iteration 1387 Loss Train: 0.6117083430290222\n",
      "Epoch: 4 \t\t\t Iteration 1388 Loss Train: 0.37791991233825684\n",
      "Epoch: 4 \t\t\t Iteration 1389 Loss Train: 0.3608631193637848\n",
      "Epoch: 4 \t\t\t Iteration 1390 Loss Train: 0.29169923067092896\n",
      "Epoch: 4 \t\t\t Iteration 1391 Loss Train: 0.40837550163269043\n",
      "Epoch: 4 \t\t\t Iteration 1392 Loss Train: 0.35260313749313354\n",
      "Epoch: 4 \t\t\t Iteration 1393 Loss Train: 0.33113303780555725\n",
      "Epoch: 4 \t\t\t Iteration 1394 Loss Train: 0.29552125930786133\n",
      "Epoch: 4 \t\t\t Iteration 1395 Loss Train: 0.7385764718055725\n",
      "Epoch: 4 / 5 \t\t\t Training Loss:0.41838696570165695\n",
      "Epoch: 5 \t\t\t Iteration 1 Loss Train: 0.7526532411575317\n",
      "Epoch: 5 \t\t\t Iteration 2 Loss Train: 0.5149787068367004\n",
      "Epoch: 5 \t\t\t Iteration 3 Loss Train: 0.9477066397666931\n",
      "Epoch: 5 \t\t\t Iteration 4 Loss Train: 0.39662331342697144\n",
      "Epoch: 5 \t\t\t Iteration 5 Loss Train: 0.5079201459884644\n",
      "Epoch: 5 \t\t\t Iteration 6 Loss Train: 0.45856425166130066\n",
      "Epoch: 5 \t\t\t Iteration 7 Loss Train: 0.330218106508255\n",
      "Epoch: 5 \t\t\t Iteration 8 Loss Train: 0.2275136113166809\n",
      "Epoch: 5 \t\t\t Iteration 9 Loss Train: 0.5627521276473999\n",
      "Epoch: 5 \t\t\t Iteration 10 Loss Train: 0.32026711106300354\n",
      "Epoch: 5 \t\t\t Iteration 11 Loss Train: 0.347428560256958\n",
      "Epoch: 5 \t\t\t Iteration 12 Loss Train: 0.17155246436595917\n",
      "Epoch: 5 \t\t\t Iteration 13 Loss Train: 0.5100712180137634\n",
      "Epoch: 5 \t\t\t Iteration 14 Loss Train: 0.3906494379043579\n",
      "Epoch: 5 \t\t\t Iteration 15 Loss Train: 0.35196223855018616\n",
      "Epoch: 5 \t\t\t Iteration 16 Loss Train: 0.22389663755893707\n",
      "Epoch: 5 \t\t\t Iteration 17 Loss Train: 0.23461373150348663\n",
      "Epoch: 5 \t\t\t Iteration 18 Loss Train: 0.4013107419013977\n",
      "Epoch: 5 \t\t\t Iteration 19 Loss Train: 0.3162558674812317\n",
      "Epoch: 5 \t\t\t Iteration 20 Loss Train: 0.370084673166275\n",
      "Epoch: 5 \t\t\t Iteration 21 Loss Train: 0.4126952290534973\n",
      "Epoch: 5 \t\t\t Iteration 22 Loss Train: 0.39549314975738525\n",
      "Epoch: 5 \t\t\t Iteration 23 Loss Train: 0.3090858459472656\n",
      "Epoch: 5 \t\t\t Iteration 24 Loss Train: 0.3965122103691101\n",
      "Epoch: 5 \t\t\t Iteration 25 Loss Train: 0.3872285485267639\n",
      "Epoch: 5 \t\t\t Iteration 26 Loss Train: 0.39830511808395386\n",
      "Epoch: 5 \t\t\t Iteration 27 Loss Train: 0.9078555703163147\n",
      "Epoch: 5 \t\t\t Iteration 28 Loss Train: 0.4917272627353668\n",
      "Epoch: 5 \t\t\t Iteration 29 Loss Train: 0.4007914960384369\n",
      "Epoch: 5 \t\t\t Iteration 30 Loss Train: 0.3339424133300781\n",
      "Epoch: 5 \t\t\t Iteration 31 Loss Train: 0.6535391807556152\n",
      "Epoch: 5 \t\t\t Iteration 32 Loss Train: 0.38342082500457764\n",
      "Epoch: 5 \t\t\t Iteration 33 Loss Train: 0.41113173961639404\n",
      "Epoch: 5 \t\t\t Iteration 34 Loss Train: 0.36653950810432434\n",
      "Epoch: 5 \t\t\t Iteration 35 Loss Train: 0.4656108319759369\n",
      "Epoch: 5 \t\t\t Iteration 36 Loss Train: 0.4413955807685852\n",
      "Epoch: 5 \t\t\t Iteration 37 Loss Train: 0.35417401790618896\n",
      "Epoch: 5 \t\t\t Iteration 38 Loss Train: 0.4522056579589844\n",
      "Epoch: 5 \t\t\t Iteration 39 Loss Train: 0.4369961619377136\n",
      "Epoch: 5 \t\t\t Iteration 40 Loss Train: 0.3864154517650604\n",
      "Epoch: 5 \t\t\t Iteration 41 Loss Train: 0.3719775974750519\n",
      "Epoch: 5 \t\t\t Iteration 42 Loss Train: 0.39324963092803955\n",
      "Epoch: 5 \t\t\t Iteration 43 Loss Train: 0.5081882476806641\n",
      "Epoch: 5 \t\t\t Iteration 44 Loss Train: 0.3868415057659149\n",
      "Epoch: 5 \t\t\t Iteration 45 Loss Train: 0.5036526918411255\n",
      "Epoch: 5 \t\t\t Iteration 46 Loss Train: 0.37512752413749695\n",
      "Epoch: 5 \t\t\t Iteration 47 Loss Train: 0.2687525451183319\n",
      "Epoch: 5 \t\t\t Iteration 48 Loss Train: 0.6300379633903503\n",
      "Epoch: 5 \t\t\t Iteration 49 Loss Train: 0.34986117482185364\n",
      "Epoch: 5 \t\t\t Iteration 50 Loss Train: 0.5184075832366943\n",
      "Epoch: 5 \t\t\t Iteration 51 Loss Train: 0.42424818873405457\n",
      "Epoch: 5 \t\t\t Iteration 52 Loss Train: 0.380600243806839\n",
      "Epoch: 5 \t\t\t Iteration 53 Loss Train: 0.39083346724510193\n",
      "Epoch: 5 \t\t\t Iteration 54 Loss Train: 0.5756618976593018\n",
      "Epoch: 5 \t\t\t Iteration 55 Loss Train: 0.334307461977005\n",
      "Epoch: 5 \t\t\t Iteration 56 Loss Train: 0.2783772945404053\n",
      "Epoch: 5 \t\t\t Iteration 57 Loss Train: 0.4823639988899231\n",
      "Epoch: 5 \t\t\t Iteration 58 Loss Train: 0.42398983240127563\n",
      "Epoch: 5 \t\t\t Iteration 59 Loss Train: 0.2681494355201721\n",
      "Epoch: 5 \t\t\t Iteration 60 Loss Train: 0.39515066146850586\n",
      "Epoch: 5 \t\t\t Iteration 61 Loss Train: 0.30914491415023804\n",
      "Epoch: 5 \t\t\t Iteration 62 Loss Train: 0.4158914387226105\n",
      "Epoch: 5 \t\t\t Iteration 63 Loss Train: 0.37151509523391724\n",
      "Epoch: 5 \t\t\t Iteration 64 Loss Train: 0.5839129686355591\n",
      "Epoch: 5 \t\t\t Iteration 65 Loss Train: 0.3638870418071747\n",
      "Epoch: 5 \t\t\t Iteration 66 Loss Train: 0.3719213306903839\n",
      "Epoch: 5 \t\t\t Iteration 67 Loss Train: 0.5939946174621582\n",
      "Epoch: 5 \t\t\t Iteration 68 Loss Train: 0.42784619331359863\n",
      "Epoch: 5 \t\t\t Iteration 69 Loss Train: 0.5498836040496826\n",
      "Epoch: 5 \t\t\t Iteration 70 Loss Train: 0.32224661111831665\n",
      "Epoch: 5 \t\t\t Iteration 71 Loss Train: 0.5642274618148804\n",
      "Epoch: 5 \t\t\t Iteration 72 Loss Train: 0.4296931326389313\n",
      "Epoch: 5 \t\t\t Iteration 73 Loss Train: 0.48836055397987366\n",
      "Epoch: 5 \t\t\t Iteration 74 Loss Train: 0.4015566408634186\n",
      "Epoch: 5 \t\t\t Iteration 75 Loss Train: 0.3731575012207031\n",
      "Epoch: 5 \t\t\t Iteration 76 Loss Train: 0.3776053786277771\n",
      "Epoch: 5 \t\t\t Iteration 77 Loss Train: 0.5542911291122437\n",
      "Epoch: 5 \t\t\t Iteration 78 Loss Train: 0.4063038229942322\n",
      "Epoch: 5 \t\t\t Iteration 79 Loss Train: 0.3587157428264618\n",
      "Epoch: 5 \t\t\t Iteration 80 Loss Train: 0.3232160806655884\n",
      "Epoch: 5 \t\t\t Iteration 81 Loss Train: 0.585686445236206\n",
      "Epoch: 5 \t\t\t Iteration 82 Loss Train: 0.49154770374298096\n",
      "Epoch: 5 \t\t\t Iteration 83 Loss Train: 0.35439956188201904\n",
      "Epoch: 5 \t\t\t Iteration 84 Loss Train: 0.3099340498447418\n",
      "Epoch: 5 \t\t\t Iteration 85 Loss Train: 0.1867063343524933\n",
      "Epoch: 5 \t\t\t Iteration 86 Loss Train: 0.34173157811164856\n",
      "Epoch: 5 \t\t\t Iteration 87 Loss Train: 0.47942134737968445\n",
      "Epoch: 5 \t\t\t Iteration 88 Loss Train: 0.6231793165206909\n",
      "Epoch: 5 \t\t\t Iteration 89 Loss Train: 0.4593603014945984\n",
      "Epoch: 5 \t\t\t Iteration 90 Loss Train: 0.49070581793785095\n",
      "Epoch: 5 \t\t\t Iteration 91 Loss Train: 0.40056997537612915\n",
      "Epoch: 5 \t\t\t Iteration 92 Loss Train: 0.429508239030838\n",
      "Epoch: 5 \t\t\t Iteration 93 Loss Train: 0.3454384207725525\n",
      "Epoch: 5 \t\t\t Iteration 94 Loss Train: 0.5282588005065918\n",
      "Epoch: 5 \t\t\t Iteration 95 Loss Train: 0.43232887983322144\n",
      "Epoch: 5 \t\t\t Iteration 96 Loss Train: 0.5493534207344055\n",
      "Epoch: 5 \t\t\t Iteration 97 Loss Train: 0.5139275193214417\n",
      "Epoch: 5 \t\t\t Iteration 98 Loss Train: 0.5989645719528198\n",
      "Epoch: 5 \t\t\t Iteration 99 Loss Train: 0.611398458480835\n",
      "Epoch: 5 \t\t\t Iteration 100 Loss Train: 0.38199445605278015\n",
      "Epoch: 5 \t\t\t Iteration 101 Loss Train: 0.43616360425949097\n",
      "Epoch: 5 \t\t\t Iteration 102 Loss Train: 0.39695489406585693\n",
      "Epoch: 5 \t\t\t Iteration 103 Loss Train: 0.3381725251674652\n",
      "Epoch: 5 \t\t\t Iteration 104 Loss Train: 0.3847011625766754\n",
      "Epoch: 5 \t\t\t Iteration 105 Loss Train: 0.480193167924881\n",
      "Epoch: 5 \t\t\t Iteration 106 Loss Train: 0.3945777416229248\n",
      "Epoch: 5 \t\t\t Iteration 107 Loss Train: 0.5383129119873047\n",
      "Epoch: 5 \t\t\t Iteration 108 Loss Train: 0.3848564624786377\n",
      "Epoch: 5 \t\t\t Iteration 109 Loss Train: 0.3000876009464264\n",
      "Epoch: 5 \t\t\t Iteration 110 Loss Train: 0.22863918542861938\n",
      "Epoch: 5 \t\t\t Iteration 111 Loss Train: 0.4640182852745056\n",
      "Epoch: 5 \t\t\t Iteration 112 Loss Train: 0.4134065508842468\n",
      "Epoch: 5 \t\t\t Iteration 113 Loss Train: 0.4782281816005707\n",
      "Epoch: 5 \t\t\t Iteration 114 Loss Train: 0.3900645971298218\n",
      "Epoch: 5 \t\t\t Iteration 115 Loss Train: 0.528931200504303\n",
      "Epoch: 5 \t\t\t Iteration 116 Loss Train: 0.44898149371147156\n",
      "Epoch: 5 \t\t\t Iteration 117 Loss Train: 0.33183392882347107\n",
      "Epoch: 5 \t\t\t Iteration 118 Loss Train: 0.2517035901546478\n",
      "Epoch: 5 \t\t\t Iteration 119 Loss Train: 0.29062312841415405\n",
      "Epoch: 5 \t\t\t Iteration 120 Loss Train: 0.23935404419898987\n",
      "Epoch: 5 \t\t\t Iteration 121 Loss Train: 0.4362625479698181\n",
      "Epoch: 5 \t\t\t Iteration 122 Loss Train: 0.4246039390563965\n",
      "Epoch: 5 \t\t\t Iteration 123 Loss Train: 0.524939775466919\n",
      "Epoch: 5 \t\t\t Iteration 124 Loss Train: 0.32906579971313477\n",
      "Epoch: 5 \t\t\t Iteration 125 Loss Train: 0.3505025804042816\n",
      "Epoch: 5 \t\t\t Iteration 126 Loss Train: 0.22086012363433838\n",
      "Epoch: 5 \t\t\t Iteration 127 Loss Train: 0.496494323015213\n",
      "Epoch: 5 \t\t\t Iteration 128 Loss Train: 0.5241736769676208\n",
      "Epoch: 5 \t\t\t Iteration 129 Loss Train: 0.44046083092689514\n",
      "Epoch: 5 \t\t\t Iteration 130 Loss Train: 0.27611303329467773\n",
      "Epoch: 5 \t\t\t Iteration 131 Loss Train: 0.4641422629356384\n",
      "Epoch: 5 \t\t\t Iteration 132 Loss Train: 0.3509484827518463\n",
      "Epoch: 5 \t\t\t Iteration 133 Loss Train: 0.2629615068435669\n",
      "Epoch: 5 \t\t\t Iteration 134 Loss Train: 0.5242751836776733\n",
      "Epoch: 5 \t\t\t Iteration 135 Loss Train: 0.39346253871917725\n",
      "Epoch: 5 \t\t\t Iteration 136 Loss Train: 0.582325279712677\n",
      "Epoch: 5 \t\t\t Iteration 137 Loss Train: 0.3828924298286438\n",
      "Epoch: 5 \t\t\t Iteration 138 Loss Train: 0.46225956082344055\n",
      "Epoch: 5 \t\t\t Iteration 139 Loss Train: 0.36723557114601135\n",
      "Epoch: 5 \t\t\t Iteration 140 Loss Train: 0.2853659391403198\n",
      "Epoch: 5 \t\t\t Iteration 141 Loss Train: 0.5718443393707275\n",
      "Epoch: 5 \t\t\t Iteration 142 Loss Train: 0.4216480255126953\n",
      "Epoch: 5 \t\t\t Iteration 143 Loss Train: 0.40664172172546387\n",
      "Epoch: 5 \t\t\t Iteration 144 Loss Train: 0.41208046674728394\n",
      "Epoch: 5 \t\t\t Iteration 145 Loss Train: 0.4539873003959656\n",
      "Epoch: 5 \t\t\t Iteration 146 Loss Train: 0.36538165807724\n",
      "Epoch: 5 \t\t\t Iteration 147 Loss Train: 0.32839539647102356\n",
      "Epoch: 5 \t\t\t Iteration 148 Loss Train: 0.4036737084388733\n",
      "Epoch: 5 \t\t\t Iteration 149 Loss Train: 0.5027493238449097\n",
      "Epoch: 5 \t\t\t Iteration 150 Loss Train: 0.3820904493331909\n",
      "Epoch: 5 \t\t\t Iteration 151 Loss Train: 0.3946177363395691\n",
      "Epoch: 5 \t\t\t Iteration 152 Loss Train: 0.4332428276538849\n",
      "Epoch: 5 \t\t\t Iteration 153 Loss Train: 0.4244355261325836\n",
      "Epoch: 5 \t\t\t Iteration 154 Loss Train: 0.35165083408355713\n",
      "Epoch: 5 \t\t\t Iteration 155 Loss Train: 0.3785199820995331\n",
      "Epoch: 5 \t\t\t Iteration 156 Loss Train: 0.4056681990623474\n",
      "Epoch: 5 \t\t\t Iteration 157 Loss Train: 0.3635691702365875\n",
      "Epoch: 5 \t\t\t Iteration 158 Loss Train: 0.5020052790641785\n",
      "Epoch: 5 \t\t\t Iteration 159 Loss Train: 0.37052255868911743\n",
      "Epoch: 5 \t\t\t Iteration 160 Loss Train: 0.3179484009742737\n",
      "Epoch: 5 \t\t\t Iteration 161 Loss Train: 0.6403830647468567\n",
      "Epoch: 5 \t\t\t Iteration 162 Loss Train: 0.40296438336372375\n",
      "Epoch: 5 \t\t\t Iteration 163 Loss Train: 0.35803213715553284\n",
      "Epoch: 5 \t\t\t Iteration 164 Loss Train: 0.3814201354980469\n",
      "Epoch: 5 \t\t\t Iteration 165 Loss Train: 0.5219215154647827\n",
      "Epoch: 5 \t\t\t Iteration 166 Loss Train: 0.412009060382843\n",
      "Epoch: 5 \t\t\t Iteration 167 Loss Train: 0.46943390369415283\n",
      "Epoch: 5 \t\t\t Iteration 168 Loss Train: 0.2994210720062256\n",
      "Epoch: 5 \t\t\t Iteration 169 Loss Train: 0.4064326584339142\n",
      "Epoch: 5 \t\t\t Iteration 170 Loss Train: 0.5823099613189697\n",
      "Epoch: 5 \t\t\t Iteration 171 Loss Train: 0.3978661298751831\n",
      "Epoch: 5 \t\t\t Iteration 172 Loss Train: 0.41676628589630127\n",
      "Epoch: 5 \t\t\t Iteration 173 Loss Train: 0.4474201798439026\n",
      "Epoch: 5 \t\t\t Iteration 174 Loss Train: 0.4189907908439636\n",
      "Epoch: 5 \t\t\t Iteration 175 Loss Train: 0.4280743896961212\n",
      "Epoch: 5 \t\t\t Iteration 176 Loss Train: 0.5566362738609314\n",
      "Epoch: 5 \t\t\t Iteration 177 Loss Train: 0.44896501302719116\n",
      "Epoch: 5 \t\t\t Iteration 178 Loss Train: 0.48179060220718384\n",
      "Epoch: 5 \t\t\t Iteration 179 Loss Train: 0.35814720392227173\n",
      "Epoch: 5 \t\t\t Iteration 180 Loss Train: 0.32013171911239624\n",
      "Epoch: 5 \t\t\t Iteration 181 Loss Train: 0.7329010963439941\n",
      "Epoch: 5 \t\t\t Iteration 182 Loss Train: 0.39301005005836487\n",
      "Epoch: 5 \t\t\t Iteration 183 Loss Train: 0.30017009377479553\n",
      "Epoch: 5 \t\t\t Iteration 184 Loss Train: 0.4319832921028137\n",
      "Epoch: 5 \t\t\t Iteration 185 Loss Train: 0.6271529197692871\n",
      "Epoch: 5 \t\t\t Iteration 186 Loss Train: 0.43069297075271606\n",
      "Epoch: 5 \t\t\t Iteration 187 Loss Train: 0.440990686416626\n",
      "Epoch: 5 \t\t\t Iteration 188 Loss Train: 0.3481074571609497\n",
      "Epoch: 5 \t\t\t Iteration 189 Loss Train: 0.33907654881477356\n",
      "Epoch: 5 \t\t\t Iteration 190 Loss Train: 0.552876889705658\n",
      "Epoch: 5 \t\t\t Iteration 191 Loss Train: 0.469440758228302\n",
      "Epoch: 5 \t\t\t Iteration 192 Loss Train: 0.35485005378723145\n",
      "Epoch: 5 \t\t\t Iteration 193 Loss Train: 0.4865300953388214\n",
      "Epoch: 5 \t\t\t Iteration 194 Loss Train: 0.3171215057373047\n",
      "Epoch: 5 \t\t\t Iteration 195 Loss Train: 0.45826083421707153\n",
      "Epoch: 5 \t\t\t Iteration 196 Loss Train: 0.3577827215194702\n",
      "Epoch: 5 \t\t\t Iteration 197 Loss Train: 0.2722201347351074\n",
      "Epoch: 5 \t\t\t Iteration 198 Loss Train: 0.4790152907371521\n",
      "Epoch: 5 \t\t\t Iteration 199 Loss Train: 0.3590761423110962\n",
      "Epoch: 5 \t\t\t Iteration 200 Loss Train: 0.5747968554496765\n",
      "Epoch: 5 \t\t\t Iteration 201 Loss Train: 0.5918523669242859\n",
      "Epoch: 5 \t\t\t Iteration 202 Loss Train: 0.4798680543899536\n",
      "Epoch: 5 \t\t\t Iteration 203 Loss Train: 0.5462501645088196\n",
      "Epoch: 5 \t\t\t Iteration 204 Loss Train: 0.5230529308319092\n",
      "Epoch: 5 \t\t\t Iteration 205 Loss Train: 0.40829166769981384\n",
      "Epoch: 5 \t\t\t Iteration 206 Loss Train: 0.3192256689071655\n",
      "Epoch: 5 \t\t\t Iteration 207 Loss Train: 0.5191336870193481\n",
      "Epoch: 5 \t\t\t Iteration 208 Loss Train: 0.5546531677246094\n",
      "Epoch: 5 \t\t\t Iteration 209 Loss Train: 0.2887694239616394\n",
      "Epoch: 5 \t\t\t Iteration 210 Loss Train: 0.2708338797092438\n",
      "Epoch: 5 \t\t\t Iteration 211 Loss Train: 0.4267668128013611\n",
      "Epoch: 5 \t\t\t Iteration 212 Loss Train: 0.42204898595809937\n",
      "Epoch: 5 \t\t\t Iteration 213 Loss Train: 0.21503648161888123\n",
      "Epoch: 5 \t\t\t Iteration 214 Loss Train: 0.31489744782447815\n",
      "Epoch: 5 \t\t\t Iteration 215 Loss Train: 0.3571537137031555\n",
      "Epoch: 5 \t\t\t Iteration 216 Loss Train: 0.37090960144996643\n",
      "Epoch: 5 \t\t\t Iteration 217 Loss Train: 0.5890234708786011\n",
      "Epoch: 5 \t\t\t Iteration 218 Loss Train: 0.46326392889022827\n",
      "Epoch: 5 \t\t\t Iteration 219 Loss Train: 0.4167715907096863\n",
      "Epoch: 5 \t\t\t Iteration 220 Loss Train: 0.41411107778549194\n",
      "Epoch: 5 \t\t\t Iteration 221 Loss Train: 0.259192556142807\n",
      "Epoch: 5 \t\t\t Iteration 222 Loss Train: 0.6351385116577148\n",
      "Epoch: 5 \t\t\t Iteration 223 Loss Train: 0.42738351225852966\n",
      "Epoch: 5 \t\t\t Iteration 224 Loss Train: 0.35237979888916016\n",
      "Epoch: 5 \t\t\t Iteration 225 Loss Train: 0.3665531277656555\n",
      "Epoch: 5 \t\t\t Iteration 226 Loss Train: 0.2623540163040161\n",
      "Epoch: 5 \t\t\t Iteration 227 Loss Train: 0.2693297564983368\n",
      "Epoch: 5 \t\t\t Iteration 228 Loss Train: 0.4929265081882477\n",
      "Epoch: 5 \t\t\t Iteration 229 Loss Train: 0.5301769971847534\n",
      "Epoch: 5 \t\t\t Iteration 230 Loss Train: 0.37703245878219604\n",
      "Epoch: 5 \t\t\t Iteration 231 Loss Train: 0.42953598499298096\n",
      "Epoch: 5 \t\t\t Iteration 232 Loss Train: 0.5349920392036438\n",
      "Epoch: 5 \t\t\t Iteration 233 Loss Train: 0.4677199125289917\n",
      "Epoch: 5 \t\t\t Iteration 234 Loss Train: 0.49637866020202637\n",
      "Epoch: 5 \t\t\t Iteration 235 Loss Train: 0.5049753189086914\n",
      "Epoch: 5 \t\t\t Iteration 236 Loss Train: 0.5134954452514648\n",
      "Epoch: 5 \t\t\t Iteration 237 Loss Train: 0.4736472964286804\n",
      "Epoch: 5 \t\t\t Iteration 238 Loss Train: 0.3503629267215729\n",
      "Epoch: 5 \t\t\t Iteration 239 Loss Train: 0.16129083931446075\n",
      "Epoch: 5 \t\t\t Iteration 240 Loss Train: 0.37417149543762207\n",
      "Epoch: 5 \t\t\t Iteration 241 Loss Train: 0.1859082579612732\n",
      "Epoch: 5 \t\t\t Iteration 242 Loss Train: 0.6054711937904358\n",
      "Epoch: 5 \t\t\t Iteration 243 Loss Train: 0.4153427183628082\n",
      "Epoch: 5 \t\t\t Iteration 244 Loss Train: 0.5272834897041321\n",
      "Epoch: 5 \t\t\t Iteration 245 Loss Train: 0.41084954142570496\n",
      "Epoch: 5 \t\t\t Iteration 246 Loss Train: 0.4895186424255371\n",
      "Epoch: 5 \t\t\t Iteration 247 Loss Train: 0.4426478147506714\n",
      "Epoch: 5 \t\t\t Iteration 248 Loss Train: 0.40381357073783875\n",
      "Epoch: 5 \t\t\t Iteration 249 Loss Train: 0.2968801259994507\n",
      "Epoch: 5 \t\t\t Iteration 250 Loss Train: 0.38595348596572876\n",
      "Epoch: 5 \t\t\t Iteration 251 Loss Train: 0.491976797580719\n",
      "Epoch: 5 \t\t\t Iteration 252 Loss Train: 0.42989861965179443\n",
      "Epoch: 5 \t\t\t Iteration 253 Loss Train: 0.35125648975372314\n",
      "Epoch: 5 \t\t\t Iteration 254 Loss Train: 0.4255957007408142\n",
      "Epoch: 5 \t\t\t Iteration 255 Loss Train: 0.3050352931022644\n",
      "Epoch: 5 \t\t\t Iteration 256 Loss Train: 0.4050082564353943\n",
      "Epoch: 5 \t\t\t Iteration 257 Loss Train: 0.3216429054737091\n",
      "Epoch: 5 \t\t\t Iteration 258 Loss Train: 0.5630198121070862\n",
      "Epoch: 5 \t\t\t Iteration 259 Loss Train: 0.28967341780662537\n",
      "Epoch: 5 \t\t\t Iteration 260 Loss Train: 0.38124942779541016\n",
      "Epoch: 5 \t\t\t Iteration 261 Loss Train: 0.45285218954086304\n",
      "Epoch: 5 \t\t\t Iteration 262 Loss Train: 0.43762826919555664\n",
      "Epoch: 5 \t\t\t Iteration 263 Loss Train: 0.2906113862991333\n",
      "Epoch: 5 \t\t\t Iteration 264 Loss Train: 0.5534117221832275\n",
      "Epoch: 5 \t\t\t Iteration 265 Loss Train: 0.44458937644958496\n",
      "Epoch: 5 \t\t\t Iteration 266 Loss Train: 0.28553706407546997\n",
      "Epoch: 5 \t\t\t Iteration 267 Loss Train: 0.5988248586654663\n",
      "Epoch: 5 \t\t\t Iteration 268 Loss Train: 0.43963009119033813\n",
      "Epoch: 5 \t\t\t Iteration 269 Loss Train: 0.3671797811985016\n",
      "Epoch: 5 \t\t\t Iteration 270 Loss Train: 0.45648616552352905\n",
      "Epoch: 5 \t\t\t Iteration 271 Loss Train: 0.5070903301239014\n",
      "Epoch: 5 \t\t\t Iteration 272 Loss Train: 0.3595849871635437\n",
      "Epoch: 5 \t\t\t Iteration 273 Loss Train: 0.3868173360824585\n",
      "Epoch: 5 \t\t\t Iteration 274 Loss Train: 0.4857761561870575\n",
      "Epoch: 5 \t\t\t Iteration 275 Loss Train: 0.37564316391944885\n",
      "Epoch: 5 \t\t\t Iteration 276 Loss Train: 0.4436967372894287\n",
      "Epoch: 5 \t\t\t Iteration 277 Loss Train: 0.4472387433052063\n",
      "Epoch: 5 \t\t\t Iteration 278 Loss Train: 0.5464020371437073\n",
      "Epoch: 5 \t\t\t Iteration 279 Loss Train: 0.5033106803894043\n",
      "Epoch: 5 \t\t\t Iteration 280 Loss Train: 0.4501318037509918\n",
      "Epoch: 5 \t\t\t Iteration 281 Loss Train: 0.43595191836357117\n",
      "Epoch: 5 \t\t\t Iteration 282 Loss Train: 0.42144811153411865\n",
      "Epoch: 5 \t\t\t Iteration 283 Loss Train: 0.5737943649291992\n",
      "Epoch: 5 \t\t\t Iteration 284 Loss Train: 0.46732446551322937\n",
      "Epoch: 5 \t\t\t Iteration 285 Loss Train: 0.42112767696380615\n",
      "Epoch: 5 \t\t\t Iteration 286 Loss Train: 0.4358953833580017\n",
      "Epoch: 5 \t\t\t Iteration 287 Loss Train: 0.2841377258300781\n",
      "Epoch: 5 \t\t\t Iteration 288 Loss Train: 0.3819034695625305\n",
      "Epoch: 5 \t\t\t Iteration 289 Loss Train: 0.3069312572479248\n",
      "Epoch: 5 \t\t\t Iteration 290 Loss Train: 0.27402544021606445\n",
      "Epoch: 5 \t\t\t Iteration 291 Loss Train: 0.42840591073036194\n",
      "Epoch: 5 \t\t\t Iteration 292 Loss Train: 0.4513443112373352\n",
      "Epoch: 5 \t\t\t Iteration 293 Loss Train: 0.38647204637527466\n",
      "Epoch: 5 \t\t\t Iteration 294 Loss Train: 0.4858444929122925\n",
      "Epoch: 5 \t\t\t Iteration 295 Loss Train: 0.473749041557312\n",
      "Epoch: 5 \t\t\t Iteration 296 Loss Train: 0.39533382654190063\n",
      "Epoch: 5 \t\t\t Iteration 297 Loss Train: 0.4968886375427246\n",
      "Epoch: 5 \t\t\t Iteration 298 Loss Train: 0.41711413860321045\n",
      "Epoch: 5 \t\t\t Iteration 299 Loss Train: 0.5151412487030029\n",
      "Epoch: 5 \t\t\t Iteration 300 Loss Train: 0.37016022205352783\n",
      "Epoch: 5 \t\t\t Iteration 301 Loss Train: 0.297863245010376\n",
      "Epoch: 5 \t\t\t Iteration 302 Loss Train: 0.3634464740753174\n",
      "Epoch: 5 \t\t\t Iteration 303 Loss Train: 0.32472389936447144\n",
      "Epoch: 5 \t\t\t Iteration 304 Loss Train: 0.3842439353466034\n",
      "Epoch: 5 \t\t\t Iteration 305 Loss Train: 0.308417946100235\n",
      "Epoch: 5 \t\t\t Iteration 306 Loss Train: 0.5621147751808167\n",
      "Epoch: 5 \t\t\t Iteration 307 Loss Train: 0.45153120160102844\n",
      "Epoch: 5 \t\t\t Iteration 308 Loss Train: 0.39156216382980347\n",
      "Epoch: 5 \t\t\t Iteration 309 Loss Train: 0.5099167823791504\n",
      "Epoch: 5 \t\t\t Iteration 310 Loss Train: 0.40465810894966125\n",
      "Epoch: 5 \t\t\t Iteration 311 Loss Train: 0.31324586272239685\n",
      "Epoch: 5 \t\t\t Iteration 312 Loss Train: 0.6790430545806885\n",
      "Epoch: 5 \t\t\t Iteration 313 Loss Train: 0.41701123118400574\n",
      "Epoch: 5 \t\t\t Iteration 314 Loss Train: 0.5052570700645447\n",
      "Epoch: 5 \t\t\t Iteration 315 Loss Train: 0.2701224088668823\n",
      "Epoch: 5 \t\t\t Iteration 316 Loss Train: 0.4112747609615326\n",
      "Epoch: 5 \t\t\t Iteration 317 Loss Train: 0.2634643018245697\n",
      "Epoch: 5 \t\t\t Iteration 318 Loss Train: 0.3641951084136963\n",
      "Epoch: 5 \t\t\t Iteration 319 Loss Train: 0.4799792766571045\n",
      "Epoch: 5 \t\t\t Iteration 320 Loss Train: 0.3373199999332428\n",
      "Epoch: 5 \t\t\t Iteration 321 Loss Train: 0.3385523557662964\n",
      "Epoch: 5 \t\t\t Iteration 322 Loss Train: 0.3072056770324707\n",
      "Epoch: 5 \t\t\t Iteration 323 Loss Train: 0.6893119812011719\n",
      "Epoch: 5 \t\t\t Iteration 324 Loss Train: 0.4736126661300659\n",
      "Epoch: 5 \t\t\t Iteration 325 Loss Train: 0.4236331284046173\n",
      "Epoch: 5 \t\t\t Iteration 326 Loss Train: 0.4128863513469696\n",
      "Epoch: 5 \t\t\t Iteration 327 Loss Train: 0.4153861403465271\n",
      "Epoch: 5 \t\t\t Iteration 328 Loss Train: 0.3980810046195984\n",
      "Epoch: 5 \t\t\t Iteration 329 Loss Train: 0.3482872247695923\n",
      "Epoch: 5 \t\t\t Iteration 330 Loss Train: 0.2782157063484192\n",
      "Epoch: 5 \t\t\t Iteration 331 Loss Train: 0.318340539932251\n",
      "Epoch: 5 \t\t\t Iteration 332 Loss Train: 0.5503600835800171\n",
      "Epoch: 5 \t\t\t Iteration 333 Loss Train: 0.40676820278167725\n",
      "Epoch: 5 \t\t\t Iteration 334 Loss Train: 0.4357384443283081\n",
      "Epoch: 5 \t\t\t Iteration 335 Loss Train: 0.3942036032676697\n",
      "Epoch: 5 \t\t\t Iteration 336 Loss Train: 0.39546188712120056\n",
      "Epoch: 5 \t\t\t Iteration 337 Loss Train: 0.5822845101356506\n",
      "Epoch: 5 \t\t\t Iteration 338 Loss Train: 0.43613797426223755\n",
      "Epoch: 5 \t\t\t Iteration 339 Loss Train: 0.6147531270980835\n",
      "Epoch: 5 \t\t\t Iteration 340 Loss Train: 0.49692729115486145\n",
      "Epoch: 5 \t\t\t Iteration 341 Loss Train: 0.5669684410095215\n",
      "Epoch: 5 \t\t\t Iteration 342 Loss Train: 0.3976902961730957\n",
      "Epoch: 5 \t\t\t Iteration 343 Loss Train: 0.3465266227722168\n",
      "Epoch: 5 \t\t\t Iteration 344 Loss Train: 0.5000772476196289\n",
      "Epoch: 5 \t\t\t Iteration 345 Loss Train: 0.6097779273986816\n",
      "Epoch: 5 \t\t\t Iteration 346 Loss Train: 0.5775790214538574\n",
      "Epoch: 5 \t\t\t Iteration 347 Loss Train: 0.34013813734054565\n",
      "Epoch: 5 \t\t\t Iteration 348 Loss Train: 0.3858109414577484\n",
      "Epoch: 5 \t\t\t Iteration 349 Loss Train: 0.3868081569671631\n",
      "Epoch: 5 \t\t\t Iteration 350 Loss Train: 0.3992050290107727\n",
      "Epoch: 5 \t\t\t Iteration 351 Loss Train: 0.38568875193595886\n",
      "Epoch: 5 \t\t\t Iteration 352 Loss Train: 0.3420550227165222\n",
      "Epoch: 5 \t\t\t Iteration 353 Loss Train: 0.30533385276794434\n",
      "Epoch: 5 \t\t\t Iteration 354 Loss Train: 0.4377821683883667\n",
      "Epoch: 5 \t\t\t Iteration 355 Loss Train: 0.3851696252822876\n",
      "Epoch: 5 \t\t\t Iteration 356 Loss Train: 0.37795543670654297\n",
      "Epoch: 5 \t\t\t Iteration 357 Loss Train: 0.42882901430130005\n",
      "Epoch: 5 \t\t\t Iteration 358 Loss Train: 0.4432970881462097\n",
      "Epoch: 5 \t\t\t Iteration 359 Loss Train: 0.3804381191730499\n",
      "Epoch: 5 \t\t\t Iteration 360 Loss Train: 0.4121108055114746\n",
      "Epoch: 5 \t\t\t Iteration 361 Loss Train: 0.4974058270454407\n",
      "Epoch: 5 \t\t\t Iteration 362 Loss Train: 0.3625144064426422\n",
      "Epoch: 5 \t\t\t Iteration 363 Loss Train: 0.528079628944397\n",
      "Epoch: 5 \t\t\t Iteration 364 Loss Train: 0.32381880283355713\n",
      "Epoch: 5 \t\t\t Iteration 365 Loss Train: 0.24885255098342896\n",
      "Epoch: 5 \t\t\t Iteration 366 Loss Train: 0.3110535442829132\n",
      "Epoch: 5 \t\t\t Iteration 367 Loss Train: 0.34975987672805786\n",
      "Epoch: 5 \t\t\t Iteration 368 Loss Train: 0.3572113513946533\n",
      "Epoch: 5 \t\t\t Iteration 369 Loss Train: 0.3739837110042572\n",
      "Epoch: 5 \t\t\t Iteration 370 Loss Train: 0.5351672172546387\n",
      "Epoch: 5 \t\t\t Iteration 371 Loss Train: 0.42157071828842163\n",
      "Epoch: 5 \t\t\t Iteration 372 Loss Train: 0.36879226565361023\n",
      "Epoch: 5 \t\t\t Iteration 373 Loss Train: 0.31949302554130554\n",
      "Epoch: 5 \t\t\t Iteration 374 Loss Train: 0.3968020975589752\n",
      "Epoch: 5 \t\t\t Iteration 375 Loss Train: 0.34415069222450256\n",
      "Epoch: 5 \t\t\t Iteration 376 Loss Train: 0.3085869252681732\n",
      "Epoch: 5 \t\t\t Iteration 377 Loss Train: 0.4631307125091553\n",
      "Epoch: 5 \t\t\t Iteration 378 Loss Train: 0.34266793727874756\n",
      "Epoch: 5 \t\t\t Iteration 379 Loss Train: 0.48732417821884155\n",
      "Epoch: 5 \t\t\t Iteration 380 Loss Train: 0.34048140048980713\n",
      "Epoch: 5 \t\t\t Iteration 381 Loss Train: 0.3886827528476715\n",
      "Epoch: 5 \t\t\t Iteration 382 Loss Train: 0.5534765720367432\n",
      "Epoch: 5 \t\t\t Iteration 383 Loss Train: 0.4083617329597473\n",
      "Epoch: 5 \t\t\t Iteration 384 Loss Train: 0.4318234622478485\n",
      "Epoch: 5 \t\t\t Iteration 385 Loss Train: 0.5532048940658569\n",
      "Epoch: 5 \t\t\t Iteration 386 Loss Train: 0.2989591956138611\n",
      "Epoch: 5 \t\t\t Iteration 387 Loss Train: 0.46287569403648376\n",
      "Epoch: 5 \t\t\t Iteration 388 Loss Train: 0.4134250581264496\n",
      "Epoch: 5 \t\t\t Iteration 389 Loss Train: 0.3837727904319763\n",
      "Epoch: 5 \t\t\t Iteration 390 Loss Train: 0.5190310478210449\n",
      "Epoch: 5 \t\t\t Iteration 391 Loss Train: 0.33733493089675903\n",
      "Epoch: 5 \t\t\t Iteration 392 Loss Train: 0.3300189673900604\n",
      "Epoch: 5 \t\t\t Iteration 393 Loss Train: 0.3797087073326111\n",
      "Epoch: 5 \t\t\t Iteration 394 Loss Train: 0.41850030422210693\n",
      "Epoch: 5 \t\t\t Iteration 395 Loss Train: 0.42552080750465393\n",
      "Epoch: 5 \t\t\t Iteration 396 Loss Train: 0.40063053369522095\n",
      "Epoch: 5 \t\t\t Iteration 397 Loss Train: 0.6292673349380493\n",
      "Epoch: 5 \t\t\t Iteration 398 Loss Train: 0.34394270181655884\n",
      "Epoch: 5 \t\t\t Iteration 399 Loss Train: 0.35898321866989136\n",
      "Epoch: 5 \t\t\t Iteration 400 Loss Train: 0.2718779146671295\n",
      "Epoch: 5 \t\t\t Iteration 401 Loss Train: 0.449237197637558\n",
      "Epoch: 5 \t\t\t Iteration 402 Loss Train: 0.2920287847518921\n",
      "Epoch: 5 \t\t\t Iteration 403 Loss Train: 0.4208807945251465\n",
      "Epoch: 5 \t\t\t Iteration 404 Loss Train: 0.4252206087112427\n",
      "Epoch: 5 \t\t\t Iteration 405 Loss Train: 0.4436632990837097\n",
      "Epoch: 5 \t\t\t Iteration 406 Loss Train: 0.4639129638671875\n",
      "Epoch: 5 \t\t\t Iteration 407 Loss Train: 0.5888115167617798\n",
      "Epoch: 5 \t\t\t Iteration 408 Loss Train: 0.46606630086898804\n",
      "Epoch: 5 \t\t\t Iteration 409 Loss Train: 0.4407007694244385\n",
      "Epoch: 5 \t\t\t Iteration 410 Loss Train: 0.35169559717178345\n",
      "Epoch: 5 \t\t\t Iteration 411 Loss Train: 0.5142176151275635\n",
      "Epoch: 5 \t\t\t Iteration 412 Loss Train: 0.5384906530380249\n",
      "Epoch: 5 \t\t\t Iteration 413 Loss Train: 0.4635167717933655\n",
      "Epoch: 5 \t\t\t Iteration 414 Loss Train: 0.4398897588253021\n",
      "Epoch: 5 \t\t\t Iteration 415 Loss Train: 0.2937253713607788\n",
      "Epoch: 5 \t\t\t Iteration 416 Loss Train: 0.4273166060447693\n",
      "Epoch: 5 \t\t\t Iteration 417 Loss Train: 0.5454441905021667\n",
      "Epoch: 5 \t\t\t Iteration 418 Loss Train: 0.5751392841339111\n",
      "Epoch: 5 \t\t\t Iteration 419 Loss Train: 0.47628727555274963\n",
      "Epoch: 5 \t\t\t Iteration 420 Loss Train: 0.4699258804321289\n",
      "Epoch: 5 \t\t\t Iteration 421 Loss Train: 0.3384888470172882\n",
      "Epoch: 5 \t\t\t Iteration 422 Loss Train: 0.4485781192779541\n",
      "Epoch: 5 \t\t\t Iteration 423 Loss Train: 0.380969375371933\n",
      "Epoch: 5 \t\t\t Iteration 424 Loss Train: 0.3281979560852051\n",
      "Epoch: 5 \t\t\t Iteration 425 Loss Train: 0.34098684787750244\n",
      "Epoch: 5 \t\t\t Iteration 426 Loss Train: 0.6270821690559387\n",
      "Epoch: 5 \t\t\t Iteration 427 Loss Train: 0.3360128402709961\n",
      "Epoch: 5 \t\t\t Iteration 428 Loss Train: 0.3584141731262207\n",
      "Epoch: 5 \t\t\t Iteration 429 Loss Train: 0.3129383325576782\n",
      "Epoch: 5 \t\t\t Iteration 430 Loss Train: 0.4081122577190399\n",
      "Epoch: 5 \t\t\t Iteration 431 Loss Train: 0.21601158380508423\n",
      "Epoch: 5 \t\t\t Iteration 432 Loss Train: 0.31841498613357544\n",
      "Epoch: 5 \t\t\t Iteration 433 Loss Train: 0.41705289483070374\n",
      "Epoch: 5 \t\t\t Iteration 434 Loss Train: 0.16432757675647736\n",
      "Epoch: 5 \t\t\t Iteration 435 Loss Train: 0.3704093098640442\n",
      "Epoch: 5 \t\t\t Iteration 436 Loss Train: 0.4392024576663971\n",
      "Epoch: 5 \t\t\t Iteration 437 Loss Train: 0.3244372308254242\n",
      "Epoch: 5 \t\t\t Iteration 438 Loss Train: 0.5318589210510254\n",
      "Epoch: 5 \t\t\t Iteration 439 Loss Train: 0.36494842171669006\n",
      "Epoch: 5 \t\t\t Iteration 440 Loss Train: 0.20273801684379578\n",
      "Epoch: 5 \t\t\t Iteration 441 Loss Train: 0.2931617200374603\n",
      "Epoch: 5 \t\t\t Iteration 442 Loss Train: 0.6732289791107178\n",
      "Epoch: 5 \t\t\t Iteration 443 Loss Train: 0.5190102458000183\n",
      "Epoch: 5 \t\t\t Iteration 444 Loss Train: 0.4136609435081482\n",
      "Epoch: 5 \t\t\t Iteration 445 Loss Train: 0.4643231928348541\n",
      "Epoch: 5 \t\t\t Iteration 446 Loss Train: 0.41446319222450256\n",
      "Epoch: 5 \t\t\t Iteration 447 Loss Train: 0.2856629490852356\n",
      "Epoch: 5 \t\t\t Iteration 448 Loss Train: 0.39373672008514404\n",
      "Epoch: 5 \t\t\t Iteration 449 Loss Train: 0.3718639016151428\n",
      "Epoch: 5 \t\t\t Iteration 450 Loss Train: 0.3982085585594177\n",
      "Epoch: 5 \t\t\t Iteration 451 Loss Train: 0.5242433547973633\n",
      "Epoch: 5 \t\t\t Iteration 452 Loss Train: 0.3387587070465088\n",
      "Epoch: 5 \t\t\t Iteration 453 Loss Train: 0.2836962640285492\n",
      "Epoch: 5 \t\t\t Iteration 454 Loss Train: 0.36569175124168396\n",
      "Epoch: 5 \t\t\t Iteration 455 Loss Train: 0.2696473300457001\n",
      "Epoch: 5 \t\t\t Iteration 456 Loss Train: 0.488788366317749\n",
      "Epoch: 5 \t\t\t Iteration 457 Loss Train: 0.3978344798088074\n",
      "Epoch: 5 \t\t\t Iteration 458 Loss Train: 0.38305267691612244\n",
      "Epoch: 5 \t\t\t Iteration 459 Loss Train: 0.4762927293777466\n",
      "Epoch: 5 \t\t\t Iteration 460 Loss Train: 0.4672384262084961\n",
      "Epoch: 5 \t\t\t Iteration 461 Loss Train: 0.4067156910896301\n",
      "Epoch: 5 \t\t\t Iteration 462 Loss Train: 0.3955925703048706\n",
      "Epoch: 5 \t\t\t Iteration 463 Loss Train: 0.5632995963096619\n",
      "Epoch: 5 \t\t\t Iteration 464 Loss Train: 0.3338620960712433\n",
      "Epoch: 5 \t\t\t Iteration 465 Loss Train: 0.3527834713459015\n",
      "Epoch: 5 \t\t\t Iteration 466 Loss Train: 0.27019184827804565\n",
      "Epoch: 5 \t\t\t Iteration 467 Loss Train: 0.26735708117485046\n",
      "Epoch: 5 \t\t\t Iteration 468 Loss Train: 0.2459753304719925\n",
      "Epoch: 5 \t\t\t Iteration 469 Loss Train: 0.12562648952007294\n",
      "Epoch: 5 \t\t\t Iteration 470 Loss Train: 0.7047045230865479\n",
      "Epoch: 5 \t\t\t Iteration 471 Loss Train: 0.3910193145275116\n",
      "Epoch: 5 \t\t\t Iteration 472 Loss Train: 0.34491166472435\n",
      "Epoch: 5 \t\t\t Iteration 473 Loss Train: 0.303062379360199\n",
      "Epoch: 5 \t\t\t Iteration 474 Loss Train: 0.3116709291934967\n",
      "Epoch: 5 \t\t\t Iteration 475 Loss Train: 0.41655516624450684\n",
      "Epoch: 5 \t\t\t Iteration 476 Loss Train: 0.41464266180992126\n",
      "Epoch: 5 \t\t\t Iteration 477 Loss Train: 0.3447624444961548\n",
      "Epoch: 5 \t\t\t Iteration 478 Loss Train: 0.3241271674633026\n",
      "Epoch: 5 \t\t\t Iteration 479 Loss Train: 0.7511357665061951\n",
      "Epoch: 5 \t\t\t Iteration 480 Loss Train: 0.39500078558921814\n",
      "Epoch: 5 \t\t\t Iteration 481 Loss Train: 0.31617575883865356\n",
      "Epoch: 5 \t\t\t Iteration 482 Loss Train: 0.3570190668106079\n",
      "Epoch: 5 \t\t\t Iteration 483 Loss Train: 0.1966182440519333\n",
      "Epoch: 5 \t\t\t Iteration 484 Loss Train: 0.40720075368881226\n",
      "Epoch: 5 \t\t\t Iteration 485 Loss Train: 0.40289658308029175\n",
      "Epoch: 5 \t\t\t Iteration 486 Loss Train: 0.4753029942512512\n",
      "Epoch: 5 \t\t\t Iteration 487 Loss Train: 0.5197621583938599\n",
      "Epoch: 5 \t\t\t Iteration 488 Loss Train: 0.37671536207199097\n",
      "Epoch: 5 \t\t\t Iteration 489 Loss Train: 0.2121165692806244\n",
      "Epoch: 5 \t\t\t Iteration 490 Loss Train: 0.4232710599899292\n",
      "Epoch: 5 \t\t\t Iteration 491 Loss Train: 0.34256666898727417\n",
      "Epoch: 5 \t\t\t Iteration 492 Loss Train: 0.5615822076797485\n",
      "Epoch: 5 \t\t\t Iteration 493 Loss Train: 0.43386077880859375\n",
      "Epoch: 5 \t\t\t Iteration 494 Loss Train: 0.4239577651023865\n",
      "Epoch: 5 \t\t\t Iteration 495 Loss Train: 0.34571409225463867\n",
      "Epoch: 5 \t\t\t Iteration 496 Loss Train: 0.48383182287216187\n",
      "Epoch: 5 \t\t\t Iteration 497 Loss Train: 0.265360951423645\n",
      "Epoch: 5 \t\t\t Iteration 498 Loss Train: 0.6226912140846252\n",
      "Epoch: 5 \t\t\t Iteration 499 Loss Train: 0.4845094382762909\n",
      "Epoch: 5 \t\t\t Iteration 500 Loss Train: 0.2938166856765747\n",
      "Epoch: 5 \t\t\t Iteration 501 Loss Train: 0.5788160562515259\n",
      "Epoch: 5 \t\t\t Iteration 502 Loss Train: 0.4921775162220001\n",
      "Epoch: 5 \t\t\t Iteration 503 Loss Train: 0.5631353855133057\n",
      "Epoch: 5 \t\t\t Iteration 504 Loss Train: 0.4726533889770508\n",
      "Epoch: 5 \t\t\t Iteration 505 Loss Train: 0.4989595413208008\n",
      "Epoch: 5 \t\t\t Iteration 506 Loss Train: 0.469369113445282\n",
      "Epoch: 5 \t\t\t Iteration 507 Loss Train: 0.7303147315979004\n",
      "Epoch: 5 \t\t\t Iteration 508 Loss Train: 0.4740811288356781\n",
      "Epoch: 5 \t\t\t Iteration 509 Loss Train: 0.3618469834327698\n",
      "Epoch: 5 \t\t\t Iteration 510 Loss Train: 0.35389944911003113\n",
      "Epoch: 5 \t\t\t Iteration 511 Loss Train: 0.3849906027317047\n",
      "Epoch: 5 \t\t\t Iteration 512 Loss Train: 0.3672451376914978\n",
      "Epoch: 5 \t\t\t Iteration 513 Loss Train: 0.3784347474575043\n",
      "Epoch: 5 \t\t\t Iteration 514 Loss Train: 0.5511719584465027\n",
      "Epoch: 5 \t\t\t Iteration 515 Loss Train: 0.3287044167518616\n",
      "Epoch: 5 \t\t\t Iteration 516 Loss Train: 0.40650174021720886\n",
      "Epoch: 5 \t\t\t Iteration 517 Loss Train: 0.437163770198822\n",
      "Epoch: 5 \t\t\t Iteration 518 Loss Train: 0.47875267267227173\n",
      "Epoch: 5 \t\t\t Iteration 519 Loss Train: 0.3968941271305084\n",
      "Epoch: 5 \t\t\t Iteration 520 Loss Train: 0.3840476870536804\n",
      "Epoch: 5 \t\t\t Iteration 521 Loss Train: 0.34564414620399475\n",
      "Epoch: 5 \t\t\t Iteration 522 Loss Train: 0.38833004236221313\n",
      "Epoch: 5 \t\t\t Iteration 523 Loss Train: 0.38093680143356323\n",
      "Epoch: 5 \t\t\t Iteration 524 Loss Train: 0.3474562168121338\n",
      "Epoch: 5 \t\t\t Iteration 525 Loss Train: 0.45661038160324097\n",
      "Epoch: 5 \t\t\t Iteration 526 Loss Train: 0.4611956477165222\n",
      "Epoch: 5 \t\t\t Iteration 527 Loss Train: 0.36258167028427124\n",
      "Epoch: 5 \t\t\t Iteration 528 Loss Train: 0.4998058080673218\n",
      "Epoch: 5 \t\t\t Iteration 529 Loss Train: 0.4410563111305237\n",
      "Epoch: 5 \t\t\t Iteration 530 Loss Train: 0.4342944025993347\n",
      "Epoch: 5 \t\t\t Iteration 531 Loss Train: 0.32973361015319824\n",
      "Epoch: 5 \t\t\t Iteration 532 Loss Train: 0.31975477933883667\n",
      "Epoch: 5 \t\t\t Iteration 533 Loss Train: 0.186538428068161\n",
      "Epoch: 5 \t\t\t Iteration 534 Loss Train: 0.3040187358856201\n",
      "Epoch: 5 \t\t\t Iteration 535 Loss Train: 0.4396969676017761\n",
      "Epoch: 5 \t\t\t Iteration 536 Loss Train: 0.33840644359588623\n",
      "Epoch: 5 \t\t\t Iteration 537 Loss Train: 0.48209434747695923\n",
      "Epoch: 5 \t\t\t Iteration 538 Loss Train: 0.4769238531589508\n",
      "Epoch: 5 \t\t\t Iteration 539 Loss Train: 0.3863486051559448\n",
      "Epoch: 5 \t\t\t Iteration 540 Loss Train: 0.3399544060230255\n",
      "Epoch: 5 \t\t\t Iteration 541 Loss Train: 0.25552424788475037\n",
      "Epoch: 5 \t\t\t Iteration 542 Loss Train: 0.5840073227882385\n",
      "Epoch: 5 \t\t\t Iteration 543 Loss Train: 0.3889227509498596\n",
      "Epoch: 5 \t\t\t Iteration 544 Loss Train: 0.47116607427597046\n",
      "Epoch: 5 \t\t\t Iteration 545 Loss Train: 0.36409714818000793\n",
      "Epoch: 5 \t\t\t Iteration 546 Loss Train: 0.43782925605773926\n",
      "Epoch: 5 \t\t\t Iteration 547 Loss Train: 0.4820863604545593\n",
      "Epoch: 5 \t\t\t Iteration 548 Loss Train: 0.4307032525539398\n",
      "Epoch: 5 \t\t\t Iteration 549 Loss Train: 0.29316192865371704\n",
      "Epoch: 5 \t\t\t Iteration 550 Loss Train: 0.3648158311843872\n",
      "Epoch: 5 \t\t\t Iteration 551 Loss Train: 0.3391406536102295\n",
      "Epoch: 5 \t\t\t Iteration 552 Loss Train: 0.3230239748954773\n",
      "Epoch: 5 \t\t\t Iteration 553 Loss Train: 0.45455068349838257\n",
      "Epoch: 5 \t\t\t Iteration 554 Loss Train: 0.41132140159606934\n",
      "Epoch: 5 \t\t\t Iteration 555 Loss Train: 0.5578252673149109\n",
      "Epoch: 5 \t\t\t Iteration 556 Loss Train: 0.32494398951530457\n",
      "Epoch: 5 \t\t\t Iteration 557 Loss Train: 0.3631538450717926\n",
      "Epoch: 5 \t\t\t Iteration 558 Loss Train: 0.4230765700340271\n",
      "Epoch: 5 \t\t\t Iteration 559 Loss Train: 0.3194892406463623\n",
      "Epoch: 5 \t\t\t Iteration 560 Loss Train: 0.4742978513240814\n",
      "Epoch: 5 \t\t\t Iteration 561 Loss Train: 0.38601353764533997\n",
      "Epoch: 5 \t\t\t Iteration 562 Loss Train: 0.36096179485321045\n",
      "Epoch: 5 \t\t\t Iteration 563 Loss Train: 0.3588957190513611\n",
      "Epoch: 5 \t\t\t Iteration 564 Loss Train: 0.4764833450317383\n",
      "Epoch: 5 \t\t\t Iteration 565 Loss Train: 0.2737613320350647\n",
      "Epoch: 5 \t\t\t Iteration 566 Loss Train: 0.4033393859863281\n",
      "Epoch: 5 \t\t\t Iteration 567 Loss Train: 0.41911762952804565\n",
      "Epoch: 5 \t\t\t Iteration 568 Loss Train: 0.3224242627620697\n",
      "Epoch: 5 \t\t\t Iteration 569 Loss Train: 0.42081308364868164\n",
      "Epoch: 5 \t\t\t Iteration 570 Loss Train: 0.4786619544029236\n",
      "Epoch: 5 \t\t\t Iteration 571 Loss Train: 0.5477049946784973\n",
      "Epoch: 5 \t\t\t Iteration 572 Loss Train: 0.36090320348739624\n",
      "Epoch: 5 \t\t\t Iteration 573 Loss Train: 0.378228098154068\n",
      "Epoch: 5 \t\t\t Iteration 574 Loss Train: 0.6007843017578125\n",
      "Epoch: 5 \t\t\t Iteration 575 Loss Train: 0.43162083625793457\n",
      "Epoch: 5 \t\t\t Iteration 576 Loss Train: 0.44276249408721924\n",
      "Epoch: 5 \t\t\t Iteration 577 Loss Train: 0.2190479338169098\n",
      "Epoch: 5 \t\t\t Iteration 578 Loss Train: 0.3002757132053375\n",
      "Epoch: 5 \t\t\t Iteration 579 Loss Train: 0.5549434423446655\n",
      "Epoch: 5 \t\t\t Iteration 580 Loss Train: 0.770755410194397\n",
      "Epoch: 5 \t\t\t Iteration 581 Loss Train: 0.4576628506183624\n",
      "Epoch: 5 \t\t\t Iteration 582 Loss Train: 0.37466558814048767\n",
      "Epoch: 5 \t\t\t Iteration 583 Loss Train: 0.4138872027397156\n",
      "Epoch: 5 \t\t\t Iteration 584 Loss Train: 0.39316973090171814\n",
      "Epoch: 5 \t\t\t Iteration 585 Loss Train: 0.31956231594085693\n",
      "Epoch: 5 \t\t\t Iteration 586 Loss Train: 0.4054916203022003\n",
      "Epoch: 5 \t\t\t Iteration 587 Loss Train: 0.382448673248291\n",
      "Epoch: 5 \t\t\t Iteration 588 Loss Train: 0.5355013608932495\n",
      "Epoch: 5 \t\t\t Iteration 589 Loss Train: 0.4379779100418091\n",
      "Epoch: 5 \t\t\t Iteration 590 Loss Train: 0.39365673065185547\n",
      "Epoch: 5 \t\t\t Iteration 591 Loss Train: 0.46965399384498596\n",
      "Epoch: 5 \t\t\t Iteration 592 Loss Train: 0.44299763441085815\n",
      "Epoch: 5 \t\t\t Iteration 593 Loss Train: 0.43744122982025146\n",
      "Epoch: 5 \t\t\t Iteration 594 Loss Train: 0.33494341373443604\n",
      "Epoch: 5 \t\t\t Iteration 595 Loss Train: 0.5676856637001038\n",
      "Epoch: 5 \t\t\t Iteration 596 Loss Train: 0.37403056025505066\n",
      "Epoch: 5 \t\t\t Iteration 597 Loss Train: 0.38780736923217773\n",
      "Epoch: 5 \t\t\t Iteration 598 Loss Train: 0.3264698386192322\n",
      "Epoch: 5 \t\t\t Iteration 599 Loss Train: 0.41026225686073303\n",
      "Epoch: 5 \t\t\t Iteration 600 Loss Train: 0.4639456272125244\n",
      "Epoch: 5 \t\t\t Iteration 601 Loss Train: 0.567442774772644\n",
      "Epoch: 5 \t\t\t Iteration 602 Loss Train: 0.42627912759780884\n",
      "Epoch: 5 \t\t\t Iteration 603 Loss Train: 0.33187973499298096\n",
      "Epoch: 5 \t\t\t Iteration 604 Loss Train: 0.2928544878959656\n",
      "Epoch: 5 \t\t\t Iteration 605 Loss Train: 0.4379424452781677\n",
      "Epoch: 5 \t\t\t Iteration 606 Loss Train: 0.3678516745567322\n",
      "Epoch: 5 \t\t\t Iteration 607 Loss Train: 0.22533977031707764\n",
      "Epoch: 5 \t\t\t Iteration 608 Loss Train: 0.39251625537872314\n",
      "Epoch: 5 \t\t\t Iteration 609 Loss Train: 0.3941032886505127\n",
      "Epoch: 5 \t\t\t Iteration 610 Loss Train: 0.2944076955318451\n",
      "Epoch: 5 \t\t\t Iteration 611 Loss Train: 0.44029855728149414\n",
      "Epoch: 5 \t\t\t Iteration 612 Loss Train: 0.36698371171951294\n",
      "Epoch: 5 \t\t\t Iteration 613 Loss Train: 0.3984435200691223\n",
      "Epoch: 5 \t\t\t Iteration 614 Loss Train: 0.23670849204063416\n",
      "Epoch: 5 \t\t\t Iteration 615 Loss Train: 0.7141629457473755\n",
      "Epoch: 5 \t\t\t Iteration 616 Loss Train: 0.5366311073303223\n",
      "Epoch: 5 \t\t\t Iteration 617 Loss Train: 0.44850194454193115\n",
      "Epoch: 5 \t\t\t Iteration 618 Loss Train: 0.3553648591041565\n",
      "Epoch: 5 \t\t\t Iteration 619 Loss Train: 0.40963584184646606\n",
      "Epoch: 5 \t\t\t Iteration 620 Loss Train: 0.46092984080314636\n",
      "Epoch: 5 \t\t\t Iteration 621 Loss Train: 0.7201998233795166\n",
      "Epoch: 5 \t\t\t Iteration 622 Loss Train: 0.5378620624542236\n",
      "Epoch: 5 \t\t\t Iteration 623 Loss Train: 0.45682770013809204\n",
      "Epoch: 5 \t\t\t Iteration 624 Loss Train: 0.3955864906311035\n",
      "Epoch: 5 \t\t\t Iteration 625 Loss Train: 0.3796010911464691\n",
      "Epoch: 5 \t\t\t Iteration 626 Loss Train: 0.33971789479255676\n",
      "Epoch: 5 \t\t\t Iteration 627 Loss Train: 0.477195680141449\n",
      "Epoch: 5 \t\t\t Iteration 628 Loss Train: 0.45943713188171387\n",
      "Epoch: 5 \t\t\t Iteration 629 Loss Train: 0.5177041292190552\n",
      "Epoch: 5 \t\t\t Iteration 630 Loss Train: 0.26304537057876587\n",
      "Epoch: 5 \t\t\t Iteration 631 Loss Train: 0.37922966480255127\n",
      "Epoch: 5 \t\t\t Iteration 632 Loss Train: 0.38581016659736633\n",
      "Epoch: 5 \t\t\t Iteration 633 Loss Train: 0.5162868499755859\n",
      "Epoch: 5 \t\t\t Iteration 634 Loss Train: 0.4745118021965027\n",
      "Epoch: 5 \t\t\t Iteration 635 Loss Train: 0.33758312463760376\n",
      "Epoch: 5 \t\t\t Iteration 636 Loss Train: 0.4165682792663574\n",
      "Epoch: 5 \t\t\t Iteration 637 Loss Train: 0.4302516281604767\n",
      "Epoch: 5 \t\t\t Iteration 638 Loss Train: 0.3174230456352234\n",
      "Epoch: 5 \t\t\t Iteration 639 Loss Train: 0.4747869074344635\n",
      "Epoch: 5 \t\t\t Iteration 640 Loss Train: 0.5040287971496582\n",
      "Epoch: 5 \t\t\t Iteration 641 Loss Train: 0.4201614558696747\n",
      "Epoch: 5 \t\t\t Iteration 642 Loss Train: 0.5667579174041748\n",
      "Epoch: 5 \t\t\t Iteration 643 Loss Train: 0.2674579620361328\n",
      "Epoch: 5 \t\t\t Iteration 644 Loss Train: 0.350297212600708\n",
      "Epoch: 5 \t\t\t Iteration 645 Loss Train: 0.4228869676589966\n",
      "Epoch: 5 \t\t\t Iteration 646 Loss Train: 0.29536840319633484\n",
      "Epoch: 5 \t\t\t Iteration 647 Loss Train: 0.39881443977355957\n",
      "Epoch: 5 \t\t\t Iteration 648 Loss Train: 0.3253781795501709\n",
      "Epoch: 5 \t\t\t Iteration 649 Loss Train: 0.4553114175796509\n",
      "Epoch: 5 \t\t\t Iteration 650 Loss Train: 0.3772602677345276\n",
      "Epoch: 5 \t\t\t Iteration 651 Loss Train: 0.42670777440071106\n",
      "Epoch: 5 \t\t\t Iteration 652 Loss Train: 0.4184039533138275\n",
      "Epoch: 5 \t\t\t Iteration 653 Loss Train: 0.4536827802658081\n",
      "Epoch: 5 \t\t\t Iteration 654 Loss Train: 0.3292284607887268\n",
      "Epoch: 5 \t\t\t Iteration 655 Loss Train: 0.3070937991142273\n",
      "Epoch: 5 \t\t\t Iteration 656 Loss Train: 0.23193466663360596\n",
      "Epoch: 5 \t\t\t Iteration 657 Loss Train: 0.4121263027191162\n",
      "Epoch: 5 \t\t\t Iteration 658 Loss Train: 0.8904403448104858\n",
      "Epoch: 5 \t\t\t Iteration 659 Loss Train: 0.3796212077140808\n",
      "Epoch: 5 \t\t\t Iteration 660 Loss Train: 0.5524653196334839\n",
      "Epoch: 5 \t\t\t Iteration 661 Loss Train: 0.4141320586204529\n",
      "Epoch: 5 \t\t\t Iteration 662 Loss Train: 0.5132884383201599\n",
      "Epoch: 5 \t\t\t Iteration 663 Loss Train: 0.4820557236671448\n",
      "Epoch: 5 \t\t\t Iteration 664 Loss Train: 0.47296595573425293\n",
      "Epoch: 5 \t\t\t Iteration 665 Loss Train: 0.3413742184638977\n",
      "Epoch: 5 \t\t\t Iteration 666 Loss Train: 0.4120439887046814\n",
      "Epoch: 5 \t\t\t Iteration 667 Loss Train: 0.2525714933872223\n",
      "Epoch: 5 \t\t\t Iteration 668 Loss Train: 0.3630771338939667\n",
      "Epoch: 5 \t\t\t Iteration 669 Loss Train: 0.21498167514801025\n",
      "Epoch: 5 \t\t\t Iteration 670 Loss Train: 0.371124804019928\n",
      "Epoch: 5 \t\t\t Iteration 671 Loss Train: 0.7699404954910278\n",
      "Epoch: 5 \t\t\t Iteration 672 Loss Train: 0.33594852685928345\n",
      "Epoch: 5 \t\t\t Iteration 673 Loss Train: 0.5235468149185181\n",
      "Epoch: 5 \t\t\t Iteration 674 Loss Train: 0.4572855532169342\n",
      "Epoch: 5 \t\t\t Iteration 675 Loss Train: 0.41460567712783813\n",
      "Epoch: 5 \t\t\t Iteration 676 Loss Train: 0.560215950012207\n",
      "Epoch: 5 \t\t\t Iteration 677 Loss Train: 0.4835910499095917\n",
      "Epoch: 5 \t\t\t Iteration 678 Loss Train: 0.2606235146522522\n",
      "Epoch: 5 \t\t\t Iteration 679 Loss Train: 0.2435692548751831\n",
      "Epoch: 5 \t\t\t Iteration 680 Loss Train: 0.38629114627838135\n",
      "Epoch: 5 \t\t\t Iteration 681 Loss Train: 0.43461093306541443\n",
      "Epoch: 5 \t\t\t Iteration 682 Loss Train: 0.29209089279174805\n",
      "Epoch: 5 \t\t\t Iteration 683 Loss Train: 0.2537824511528015\n",
      "Epoch: 5 \t\t\t Iteration 684 Loss Train: 0.42901772260665894\n",
      "Epoch: 5 \t\t\t Iteration 685 Loss Train: 0.5247366428375244\n",
      "Epoch: 5 \t\t\t Iteration 686 Loss Train: 0.46011218428611755\n",
      "Epoch: 5 \t\t\t Iteration 687 Loss Train: 0.2838097810745239\n",
      "Epoch: 5 \t\t\t Iteration 688 Loss Train: 0.8343201875686646\n",
      "Epoch: 5 \t\t\t Iteration 689 Loss Train: 0.41782206296920776\n",
      "Epoch: 5 \t\t\t Iteration 690 Loss Train: 0.38083145022392273\n",
      "Epoch: 5 \t\t\t Iteration 691 Loss Train: 0.46815258264541626\n",
      "Epoch: 5 \t\t\t Iteration 692 Loss Train: 0.49886879324913025\n",
      "Epoch: 5 \t\t\t Iteration 693 Loss Train: 0.38313159346580505\n",
      "Epoch: 5 \t\t\t Iteration 694 Loss Train: 0.3873555064201355\n",
      "Epoch: 5 \t\t\t Iteration 695 Loss Train: 0.39037394523620605\n",
      "Epoch: 5 \t\t\t Iteration 696 Loss Train: 0.46385371685028076\n",
      "Epoch: 5 \t\t\t Iteration 697 Loss Train: 0.33802589774131775\n",
      "Epoch: 5 \t\t\t Iteration 698 Loss Train: 0.2604379951953888\n",
      "Epoch: 5 \t\t\t Iteration 699 Loss Train: 0.2268180549144745\n",
      "Epoch: 5 \t\t\t Iteration 700 Loss Train: 0.4768913984298706\n",
      "Epoch: 5 \t\t\t Iteration 701 Loss Train: 0.4328995943069458\n",
      "Epoch: 5 \t\t\t Iteration 702 Loss Train: 0.4522739052772522\n",
      "Epoch: 5 \t\t\t Iteration 703 Loss Train: 0.5992752909660339\n",
      "Epoch: 5 \t\t\t Iteration 704 Loss Train: 0.5796754360198975\n",
      "Epoch: 5 \t\t\t Iteration 705 Loss Train: 0.573198139667511\n",
      "Epoch: 5 \t\t\t Iteration 706 Loss Train: 0.49720796942710876\n",
      "Epoch: 5 \t\t\t Iteration 707 Loss Train: 0.40559616684913635\n",
      "Epoch: 5 \t\t\t Iteration 708 Loss Train: 0.4116358160972595\n",
      "Epoch: 5 \t\t\t Iteration 709 Loss Train: 0.38086754083633423\n",
      "Epoch: 5 \t\t\t Iteration 710 Loss Train: 0.3534318804740906\n",
      "Epoch: 5 \t\t\t Iteration 711 Loss Train: 0.2893928289413452\n",
      "Epoch: 5 \t\t\t Iteration 712 Loss Train: 0.3473995327949524\n",
      "Epoch: 5 \t\t\t Iteration 713 Loss Train: 0.4293406903743744\n",
      "Epoch: 5 \t\t\t Iteration 714 Loss Train: 0.4914511442184448\n",
      "Epoch: 5 \t\t\t Iteration 715 Loss Train: 0.2911333441734314\n",
      "Epoch: 5 \t\t\t Iteration 716 Loss Train: 0.36591947078704834\n",
      "Epoch: 5 \t\t\t Iteration 717 Loss Train: 0.4425656795501709\n",
      "Epoch: 5 \t\t\t Iteration 718 Loss Train: 0.4096986651420593\n",
      "Epoch: 5 \t\t\t Iteration 719 Loss Train: 0.4907953441143036\n",
      "Epoch: 5 \t\t\t Iteration 720 Loss Train: 0.4878239035606384\n",
      "Epoch: 5 \t\t\t Iteration 721 Loss Train: 0.4996914565563202\n",
      "Epoch: 5 \t\t\t Iteration 722 Loss Train: 0.3528258204460144\n",
      "Epoch: 5 \t\t\t Iteration 723 Loss Train: 0.3576242923736572\n",
      "Epoch: 5 \t\t\t Iteration 724 Loss Train: 0.39381253719329834\n",
      "Epoch: 5 \t\t\t Iteration 725 Loss Train: 0.28590714931488037\n",
      "Epoch: 5 \t\t\t Iteration 726 Loss Train: 0.48502784967422485\n",
      "Epoch: 5 \t\t\t Iteration 727 Loss Train: 0.44296979904174805\n",
      "Epoch: 5 \t\t\t Iteration 728 Loss Train: 0.5332809686660767\n",
      "Epoch: 5 \t\t\t Iteration 729 Loss Train: 0.36317160725593567\n",
      "Epoch: 5 \t\t\t Iteration 730 Loss Train: 0.3252130150794983\n",
      "Epoch: 5 \t\t\t Iteration 731 Loss Train: 0.40147000551223755\n",
      "Epoch: 5 \t\t\t Iteration 732 Loss Train: 0.2409249246120453\n",
      "Epoch: 5 \t\t\t Iteration 733 Loss Train: 0.3508845567703247\n",
      "Epoch: 5 \t\t\t Iteration 734 Loss Train: 0.6971616744995117\n",
      "Epoch: 5 \t\t\t Iteration 735 Loss Train: 0.4889124631881714\n",
      "Epoch: 5 \t\t\t Iteration 736 Loss Train: 0.5365576148033142\n",
      "Epoch: 5 \t\t\t Iteration 737 Loss Train: 0.3916255235671997\n",
      "Epoch: 5 \t\t\t Iteration 738 Loss Train: 0.36235612630844116\n",
      "Epoch: 5 \t\t\t Iteration 739 Loss Train: 1.010866403579712\n",
      "Epoch: 5 \t\t\t Iteration 740 Loss Train: 0.5203562378883362\n",
      "Epoch: 5 \t\t\t Iteration 741 Loss Train: 0.4109961986541748\n",
      "Epoch: 5 \t\t\t Iteration 742 Loss Train: 0.5429274439811707\n",
      "Epoch: 5 \t\t\t Iteration 743 Loss Train: 0.439294695854187\n",
      "Epoch: 5 \t\t\t Iteration 744 Loss Train: 0.3670646846294403\n",
      "Epoch: 5 \t\t\t Iteration 745 Loss Train: 0.4445101022720337\n",
      "Epoch: 5 \t\t\t Iteration 746 Loss Train: 0.42237600684165955\n",
      "Epoch: 5 \t\t\t Iteration 747 Loss Train: 0.4081934988498688\n",
      "Epoch: 5 \t\t\t Iteration 748 Loss Train: 0.3063187003135681\n",
      "Epoch: 5 \t\t\t Iteration 749 Loss Train: 0.3522608280181885\n",
      "Epoch: 5 \t\t\t Iteration 750 Loss Train: 0.4032090902328491\n",
      "Epoch: 5 \t\t\t Iteration 751 Loss Train: 0.4244847595691681\n",
      "Epoch: 5 \t\t\t Iteration 752 Loss Train: 0.18951553106307983\n",
      "Epoch: 5 \t\t\t Iteration 753 Loss Train: 0.4120844006538391\n",
      "Epoch: 5 \t\t\t Iteration 754 Loss Train: 0.39121830463409424\n",
      "Epoch: 5 \t\t\t Iteration 755 Loss Train: 0.6088593602180481\n",
      "Epoch: 5 \t\t\t Iteration 756 Loss Train: 0.40563517808914185\n",
      "Epoch: 5 \t\t\t Iteration 757 Loss Train: 0.4403301477432251\n",
      "Epoch: 5 \t\t\t Iteration 758 Loss Train: 0.33745014667510986\n",
      "Epoch: 5 \t\t\t Iteration 759 Loss Train: 0.4101085066795349\n",
      "Epoch: 5 \t\t\t Iteration 760 Loss Train: 0.4770185351371765\n",
      "Epoch: 5 \t\t\t Iteration 761 Loss Train: 0.5862812399864197\n",
      "Epoch: 5 \t\t\t Iteration 762 Loss Train: 0.2939717173576355\n",
      "Epoch: 5 \t\t\t Iteration 763 Loss Train: 0.39529579877853394\n",
      "Epoch: 5 \t\t\t Iteration 764 Loss Train: 0.4568803310394287\n",
      "Epoch: 5 \t\t\t Iteration 765 Loss Train: 0.2463907152414322\n",
      "Epoch: 5 \t\t\t Iteration 766 Loss Train: 0.4082975685596466\n",
      "Epoch: 5 \t\t\t Iteration 767 Loss Train: 0.3170134425163269\n",
      "Epoch: 5 \t\t\t Iteration 768 Loss Train: 0.7481265068054199\n",
      "Epoch: 5 \t\t\t Iteration 769 Loss Train: 0.6363195776939392\n",
      "Epoch: 5 \t\t\t Iteration 770 Loss Train: 0.462462842464447\n",
      "Epoch: 5 \t\t\t Iteration 771 Loss Train: 0.3882840871810913\n",
      "Epoch: 5 \t\t\t Iteration 772 Loss Train: 0.6143450736999512\n",
      "Epoch: 5 \t\t\t Iteration 773 Loss Train: 0.3856111764907837\n",
      "Epoch: 5 \t\t\t Iteration 774 Loss Train: 0.3117295801639557\n",
      "Epoch: 5 \t\t\t Iteration 775 Loss Train: 0.38147151470184326\n",
      "Epoch: 5 \t\t\t Iteration 776 Loss Train: 0.6063159108161926\n",
      "Epoch: 5 \t\t\t Iteration 777 Loss Train: 0.5302059650421143\n",
      "Epoch: 5 \t\t\t Iteration 778 Loss Train: 0.47559067606925964\n",
      "Epoch: 5 \t\t\t Iteration 779 Loss Train: 0.3828272521495819\n",
      "Epoch: 5 \t\t\t Iteration 780 Loss Train: 0.424150675535202\n",
      "Epoch: 5 \t\t\t Iteration 781 Loss Train: 0.5431501865386963\n",
      "Epoch: 5 \t\t\t Iteration 782 Loss Train: 0.47156962752342224\n",
      "Epoch: 5 \t\t\t Iteration 783 Loss Train: 0.38744115829467773\n",
      "Epoch: 5 \t\t\t Iteration 784 Loss Train: 0.3864058256149292\n",
      "Epoch: 5 \t\t\t Iteration 785 Loss Train: 0.5339763164520264\n",
      "Epoch: 5 \t\t\t Iteration 786 Loss Train: 0.42480233311653137\n",
      "Epoch: 5 \t\t\t Iteration 787 Loss Train: 0.3569796085357666\n",
      "Epoch: 5 \t\t\t Iteration 788 Loss Train: 0.2413940131664276\n",
      "Epoch: 5 \t\t\t Iteration 789 Loss Train: 0.3699023723602295\n",
      "Epoch: 5 \t\t\t Iteration 790 Loss Train: 0.6344367265701294\n",
      "Epoch: 5 \t\t\t Iteration 791 Loss Train: 0.3535573482513428\n",
      "Epoch: 5 \t\t\t Iteration 792 Loss Train: 0.40979963541030884\n",
      "Epoch: 5 \t\t\t Iteration 793 Loss Train: 0.41320061683654785\n",
      "Epoch: 5 \t\t\t Iteration 794 Loss Train: 0.4942306578159332\n",
      "Epoch: 5 \t\t\t Iteration 795 Loss Train: 0.444581538438797\n",
      "Epoch: 5 \t\t\t Iteration 796 Loss Train: 0.35880452394485474\n",
      "Epoch: 5 \t\t\t Iteration 797 Loss Train: 0.26675093173980713\n",
      "Epoch: 5 \t\t\t Iteration 798 Loss Train: 0.373380571603775\n",
      "Epoch: 5 \t\t\t Iteration 799 Loss Train: 0.31362345814704895\n",
      "Epoch: 5 \t\t\t Iteration 800 Loss Train: 0.3866778016090393\n",
      "Epoch: 5 \t\t\t Iteration 801 Loss Train: 0.2462121695280075\n",
      "Epoch: 5 \t\t\t Iteration 802 Loss Train: 0.2299915850162506\n",
      "Epoch: 5 \t\t\t Iteration 803 Loss Train: 0.6808853149414062\n",
      "Epoch: 5 \t\t\t Iteration 804 Loss Train: 0.3801887035369873\n",
      "Epoch: 5 \t\t\t Iteration 805 Loss Train: 0.45037513971328735\n",
      "Epoch: 5 \t\t\t Iteration 806 Loss Train: 0.3447612226009369\n",
      "Epoch: 5 \t\t\t Iteration 807 Loss Train: 0.4530228078365326\n",
      "Epoch: 5 \t\t\t Iteration 808 Loss Train: 0.4768715500831604\n",
      "Epoch: 5 \t\t\t Iteration 809 Loss Train: 0.6193652153015137\n",
      "Epoch: 5 \t\t\t Iteration 810 Loss Train: 0.48213478922843933\n",
      "Epoch: 5 \t\t\t Iteration 811 Loss Train: 0.34335020184516907\n",
      "Epoch: 5 \t\t\t Iteration 812 Loss Train: 0.2883237600326538\n",
      "Epoch: 5 \t\t\t Iteration 813 Loss Train: 0.2461388111114502\n",
      "Epoch: 5 \t\t\t Iteration 814 Loss Train: 0.45867082476615906\n",
      "Epoch: 5 \t\t\t Iteration 815 Loss Train: 0.5221949815750122\n",
      "Epoch: 5 \t\t\t Iteration 816 Loss Train: 0.32777974009513855\n",
      "Epoch: 5 \t\t\t Iteration 817 Loss Train: 0.3599025011062622\n",
      "Epoch: 5 \t\t\t Iteration 818 Loss Train: 0.4407125413417816\n",
      "Epoch: 5 \t\t\t Iteration 819 Loss Train: 0.3742574453353882\n",
      "Epoch: 5 \t\t\t Iteration 820 Loss Train: 0.46471089124679565\n",
      "Epoch: 5 \t\t\t Iteration 821 Loss Train: 0.45629146695137024\n",
      "Epoch: 5 \t\t\t Iteration 822 Loss Train: 0.48608967661857605\n",
      "Epoch: 5 \t\t\t Iteration 823 Loss Train: 0.4996595084667206\n",
      "Epoch: 5 \t\t\t Iteration 824 Loss Train: 0.4541538655757904\n",
      "Epoch: 5 \t\t\t Iteration 825 Loss Train: 0.33936482667922974\n",
      "Epoch: 5 \t\t\t Iteration 826 Loss Train: 0.32296130061149597\n",
      "Epoch: 5 \t\t\t Iteration 827 Loss Train: 0.3867030143737793\n",
      "Epoch: 5 \t\t\t Iteration 828 Loss Train: 0.552770733833313\n",
      "Epoch: 5 \t\t\t Iteration 829 Loss Train: 0.5162832736968994\n",
      "Epoch: 5 \t\t\t Iteration 830 Loss Train: 0.5738224983215332\n",
      "Epoch: 5 \t\t\t Iteration 831 Loss Train: 0.386951744556427\n",
      "Epoch: 5 \t\t\t Iteration 832 Loss Train: 0.24375712871551514\n",
      "Epoch: 5 \t\t\t Iteration 833 Loss Train: 0.468997061252594\n",
      "Epoch: 5 \t\t\t Iteration 834 Loss Train: 0.2601509392261505\n",
      "Epoch: 5 \t\t\t Iteration 835 Loss Train: 0.3588648736476898\n",
      "Epoch: 5 \t\t\t Iteration 836 Loss Train: 0.33608102798461914\n",
      "Epoch: 5 \t\t\t Iteration 837 Loss Train: 0.5882212519645691\n",
      "Epoch: 5 \t\t\t Iteration 838 Loss Train: 0.4520206153392792\n",
      "Epoch: 5 \t\t\t Iteration 839 Loss Train: 0.4743970036506653\n",
      "Epoch: 5 \t\t\t Iteration 840 Loss Train: 0.41579604148864746\n",
      "Epoch: 5 \t\t\t Iteration 841 Loss Train: 0.4972260594367981\n",
      "Epoch: 5 \t\t\t Iteration 842 Loss Train: 0.243051216006279\n",
      "Epoch: 5 \t\t\t Iteration 843 Loss Train: 0.2964915335178375\n",
      "Epoch: 5 \t\t\t Iteration 844 Loss Train: 0.5040301084518433\n",
      "Epoch: 5 \t\t\t Iteration 845 Loss Train: 0.5137771368026733\n",
      "Epoch: 5 \t\t\t Iteration 846 Loss Train: 0.33292776346206665\n",
      "Epoch: 5 \t\t\t Iteration 847 Loss Train: 0.33784857392311096\n",
      "Epoch: 5 \t\t\t Iteration 848 Loss Train: 0.4096296429634094\n",
      "Epoch: 5 \t\t\t Iteration 849 Loss Train: 0.30866265296936035\n",
      "Epoch: 5 \t\t\t Iteration 850 Loss Train: 0.291390597820282\n",
      "Epoch: 5 \t\t\t Iteration 851 Loss Train: 0.445314884185791\n",
      "Epoch: 5 \t\t\t Iteration 852 Loss Train: 0.39806121587753296\n",
      "Epoch: 5 \t\t\t Iteration 853 Loss Train: 0.40566033124923706\n",
      "Epoch: 5 \t\t\t Iteration 854 Loss Train: 0.43899279832839966\n",
      "Epoch: 5 \t\t\t Iteration 855 Loss Train: 0.3221425414085388\n",
      "Epoch: 5 \t\t\t Iteration 856 Loss Train: 0.48903149366378784\n",
      "Epoch: 5 \t\t\t Iteration 857 Loss Train: 0.3261064291000366\n",
      "Epoch: 5 \t\t\t Iteration 858 Loss Train: 0.3038330674171448\n",
      "Epoch: 5 \t\t\t Iteration 859 Loss Train: 0.3069606423377991\n",
      "Epoch: 5 \t\t\t Iteration 860 Loss Train: 0.4068440794944763\n",
      "Epoch: 5 \t\t\t Iteration 861 Loss Train: 0.28672853112220764\n",
      "Epoch: 5 \t\t\t Iteration 862 Loss Train: 0.30732113122940063\n",
      "Epoch: 5 \t\t\t Iteration 863 Loss Train: 0.4261413812637329\n",
      "Epoch: 5 \t\t\t Iteration 864 Loss Train: 0.734562873840332\n",
      "Epoch: 5 \t\t\t Iteration 865 Loss Train: 0.43854403495788574\n",
      "Epoch: 5 \t\t\t Iteration 866 Loss Train: 0.25965404510498047\n",
      "Epoch: 5 \t\t\t Iteration 867 Loss Train: 0.3876257836818695\n",
      "Epoch: 5 \t\t\t Iteration 868 Loss Train: 0.6453150510787964\n",
      "Epoch: 5 \t\t\t Iteration 869 Loss Train: 0.44807910919189453\n",
      "Epoch: 5 \t\t\t Iteration 870 Loss Train: 0.45612576603889465\n",
      "Epoch: 5 \t\t\t Iteration 871 Loss Train: 0.43813589215278625\n",
      "Epoch: 5 \t\t\t Iteration 872 Loss Train: 0.38574427366256714\n",
      "Epoch: 5 \t\t\t Iteration 873 Loss Train: 0.5358399152755737\n",
      "Epoch: 5 \t\t\t Iteration 874 Loss Train: 0.5564121007919312\n",
      "Epoch: 5 \t\t\t Iteration 875 Loss Train: 0.36880117654800415\n",
      "Epoch: 5 \t\t\t Iteration 876 Loss Train: 0.29452797770500183\n",
      "Epoch: 5 \t\t\t Iteration 877 Loss Train: 0.4931403398513794\n",
      "Epoch: 5 \t\t\t Iteration 878 Loss Train: 0.39019346237182617\n",
      "Epoch: 5 \t\t\t Iteration 879 Loss Train: 0.5126467347145081\n",
      "Epoch: 5 \t\t\t Iteration 880 Loss Train: 0.6164841651916504\n",
      "Epoch: 5 \t\t\t Iteration 881 Loss Train: 0.4548042416572571\n",
      "Epoch: 5 \t\t\t Iteration 882 Loss Train: 0.46612513065338135\n",
      "Epoch: 5 \t\t\t Iteration 883 Loss Train: 0.3882825970649719\n",
      "Epoch: 5 \t\t\t Iteration 884 Loss Train: 0.6600439548492432\n",
      "Epoch: 5 \t\t\t Iteration 885 Loss Train: 0.46084558963775635\n",
      "Epoch: 5 \t\t\t Iteration 886 Loss Train: 0.29795265197753906\n",
      "Epoch: 5 \t\t\t Iteration 887 Loss Train: 0.33988720178604126\n",
      "Epoch: 5 \t\t\t Iteration 888 Loss Train: 0.37323033809661865\n",
      "Epoch: 5 \t\t\t Iteration 889 Loss Train: 0.3506450653076172\n",
      "Epoch: 5 \t\t\t Iteration 890 Loss Train: 0.7577120065689087\n",
      "Epoch: 5 \t\t\t Iteration 891 Loss Train: 0.42084169387817383\n",
      "Epoch: 5 \t\t\t Iteration 892 Loss Train: 0.4519495666027069\n",
      "Epoch: 5 \t\t\t Iteration 893 Loss Train: 0.31127214431762695\n",
      "Epoch: 5 \t\t\t Iteration 894 Loss Train: 0.39722681045532227\n",
      "Epoch: 5 \t\t\t Iteration 895 Loss Train: 0.5751708149909973\n",
      "Epoch: 5 \t\t\t Iteration 896 Loss Train: 0.3016889989376068\n",
      "Epoch: 5 \t\t\t Iteration 897 Loss Train: 0.4535585939884186\n",
      "Epoch: 5 \t\t\t Iteration 898 Loss Train: 0.40027767419815063\n",
      "Epoch: 5 \t\t\t Iteration 899 Loss Train: 0.5287283658981323\n",
      "Epoch: 5 \t\t\t Iteration 900 Loss Train: 0.22699326276779175\n",
      "Epoch: 5 \t\t\t Iteration 901 Loss Train: 0.5706766843795776\n",
      "Epoch: 5 \t\t\t Iteration 902 Loss Train: 0.4254152774810791\n",
      "Epoch: 5 \t\t\t Iteration 903 Loss Train: 0.4505760073661804\n",
      "Epoch: 5 \t\t\t Iteration 904 Loss Train: 0.393903911113739\n",
      "Epoch: 5 \t\t\t Iteration 905 Loss Train: 0.40584036707878113\n",
      "Epoch: 5 \t\t\t Iteration 906 Loss Train: 0.27397096157073975\n",
      "Epoch: 5 \t\t\t Iteration 907 Loss Train: 0.5327972173690796\n",
      "Epoch: 5 \t\t\t Iteration 908 Loss Train: 0.4360833168029785\n",
      "Epoch: 5 \t\t\t Iteration 909 Loss Train: 0.357852578163147\n",
      "Epoch: 5 \t\t\t Iteration 910 Loss Train: 0.5920554399490356\n",
      "Epoch: 5 \t\t\t Iteration 911 Loss Train: 0.5269251465797424\n",
      "Epoch: 5 \t\t\t Iteration 912 Loss Train: 0.444288432598114\n",
      "Epoch: 5 \t\t\t Iteration 913 Loss Train: 0.33912408351898193\n",
      "Epoch: 5 \t\t\t Iteration 914 Loss Train: 0.3105900287628174\n",
      "Epoch: 5 \t\t\t Iteration 915 Loss Train: 0.4662303626537323\n",
      "Epoch: 5 \t\t\t Iteration 916 Loss Train: 0.41632306575775146\n",
      "Epoch: 5 \t\t\t Iteration 917 Loss Train: 0.47471800446510315\n",
      "Epoch: 5 \t\t\t Iteration 918 Loss Train: 0.39186131954193115\n",
      "Epoch: 5 \t\t\t Iteration 919 Loss Train: 0.39113032817840576\n",
      "Epoch: 5 \t\t\t Iteration 920 Loss Train: 0.4536550045013428\n",
      "Epoch: 5 \t\t\t Iteration 921 Loss Train: 0.4128868579864502\n",
      "Epoch: 5 \t\t\t Iteration 922 Loss Train: 0.2576615810394287\n",
      "Epoch: 5 \t\t\t Iteration 923 Loss Train: 0.5332529544830322\n",
      "Epoch: 5 \t\t\t Iteration 924 Loss Train: 0.46106719970703125\n",
      "Epoch: 5 \t\t\t Iteration 925 Loss Train: 0.4557684659957886\n",
      "Epoch: 5 \t\t\t Iteration 926 Loss Train: 0.3532046675682068\n",
      "Epoch: 5 \t\t\t Iteration 927 Loss Train: 0.4750896692276001\n",
      "Epoch: 5 \t\t\t Iteration 928 Loss Train: 0.5370245575904846\n",
      "Epoch: 5 \t\t\t Iteration 929 Loss Train: 0.4679844081401825\n",
      "Epoch: 5 \t\t\t Iteration 930 Loss Train: 0.3818117380142212\n",
      "Epoch: 5 \t\t\t Iteration 931 Loss Train: 0.4200020432472229\n",
      "Epoch: 5 \t\t\t Iteration 932 Loss Train: 0.32935449481010437\n",
      "Epoch: 5 \t\t\t Iteration 933 Loss Train: 0.346221387386322\n",
      "Epoch: 5 \t\t\t Iteration 934 Loss Train: 0.5396007299423218\n",
      "Epoch: 5 \t\t\t Iteration 935 Loss Train: 0.36461979150772095\n",
      "Epoch: 5 \t\t\t Iteration 936 Loss Train: 0.5656845569610596\n",
      "Epoch: 5 \t\t\t Iteration 937 Loss Train: 0.3602529466152191\n",
      "Epoch: 5 \t\t\t Iteration 938 Loss Train: 0.5524851679801941\n",
      "Epoch: 5 \t\t\t Iteration 939 Loss Train: 0.41238510608673096\n",
      "Epoch: 5 \t\t\t Iteration 940 Loss Train: 0.43636173009872437\n",
      "Epoch: 5 \t\t\t Iteration 941 Loss Train: 0.3558496832847595\n",
      "Epoch: 5 \t\t\t Iteration 942 Loss Train: 0.448739618062973\n",
      "Epoch: 5 \t\t\t Iteration 943 Loss Train: 0.39028483629226685\n",
      "Epoch: 5 \t\t\t Iteration 944 Loss Train: 0.34263694286346436\n",
      "Epoch: 5 \t\t\t Iteration 945 Loss Train: 0.38743454217910767\n",
      "Epoch: 5 \t\t\t Iteration 946 Loss Train: 0.32736557722091675\n",
      "Epoch: 5 \t\t\t Iteration 947 Loss Train: 0.2360498607158661\n",
      "Epoch: 5 \t\t\t Iteration 948 Loss Train: 0.1294674277305603\n",
      "Epoch: 5 \t\t\t Iteration 949 Loss Train: 0.2983021140098572\n",
      "Epoch: 5 \t\t\t Iteration 950 Loss Train: 0.42372196912765503\n",
      "Epoch: 5 \t\t\t Iteration 951 Loss Train: 0.4940154552459717\n",
      "Epoch: 5 \t\t\t Iteration 952 Loss Train: 0.3946080207824707\n",
      "Epoch: 5 \t\t\t Iteration 953 Loss Train: 0.2474563717842102\n",
      "Epoch: 5 \t\t\t Iteration 954 Loss Train: 0.5315776467323303\n",
      "Epoch: 5 \t\t\t Iteration 955 Loss Train: 0.4614109396934509\n",
      "Epoch: 5 \t\t\t Iteration 956 Loss Train: 0.3533783555030823\n",
      "Epoch: 5 \t\t\t Iteration 957 Loss Train: 0.490234375\n",
      "Epoch: 5 \t\t\t Iteration 958 Loss Train: 0.5127265453338623\n",
      "Epoch: 5 \t\t\t Iteration 959 Loss Train: 0.4890725016593933\n",
      "Epoch: 5 \t\t\t Iteration 960 Loss Train: 0.43226251006126404\n",
      "Epoch: 5 \t\t\t Iteration 961 Loss Train: 0.7863653898239136\n",
      "Epoch: 5 \t\t\t Iteration 962 Loss Train: 0.35744139552116394\n",
      "Epoch: 5 \t\t\t Iteration 963 Loss Train: 0.4203428626060486\n",
      "Epoch: 5 \t\t\t Iteration 964 Loss Train: 0.39390286803245544\n",
      "Epoch: 5 \t\t\t Iteration 965 Loss Train: 0.4591100215911865\n",
      "Epoch: 5 \t\t\t Iteration 966 Loss Train: 0.3727450966835022\n",
      "Epoch: 5 \t\t\t Iteration 967 Loss Train: 0.23419465124607086\n",
      "Epoch: 5 \t\t\t Iteration 968 Loss Train: 0.3753111660480499\n",
      "Epoch: 5 \t\t\t Iteration 969 Loss Train: 0.5603840351104736\n",
      "Epoch: 5 \t\t\t Iteration 970 Loss Train: 0.2814023196697235\n",
      "Epoch: 5 \t\t\t Iteration 971 Loss Train: 0.4305456876754761\n",
      "Epoch: 5 \t\t\t Iteration 972 Loss Train: 0.6418382525444031\n",
      "Epoch: 5 \t\t\t Iteration 973 Loss Train: 0.5558065176010132\n",
      "Epoch: 5 \t\t\t Iteration 974 Loss Train: 0.4081009030342102\n",
      "Epoch: 5 \t\t\t Iteration 975 Loss Train: 0.49194204807281494\n",
      "Epoch: 5 \t\t\t Iteration 976 Loss Train: 0.4091148376464844\n",
      "Epoch: 5 \t\t\t Iteration 977 Loss Train: 0.43946632742881775\n",
      "Epoch: 5 \t\t\t Iteration 978 Loss Train: 0.5283395051956177\n",
      "Epoch: 5 \t\t\t Iteration 979 Loss Train: 0.4507032334804535\n",
      "Epoch: 5 \t\t\t Iteration 980 Loss Train: 0.33907055854797363\n",
      "Epoch: 5 \t\t\t Iteration 981 Loss Train: 0.20069894194602966\n",
      "Epoch: 5 \t\t\t Iteration 982 Loss Train: 0.41968876123428345\n",
      "Epoch: 5 \t\t\t Iteration 983 Loss Train: 0.3898243308067322\n",
      "Epoch: 5 \t\t\t Iteration 984 Loss Train: 0.541911244392395\n",
      "Epoch: 5 \t\t\t Iteration 985 Loss Train: 0.4240154027938843\n",
      "Epoch: 5 \t\t\t Iteration 986 Loss Train: 0.5065503120422363\n",
      "Epoch: 5 \t\t\t Iteration 987 Loss Train: 0.4624156653881073\n",
      "Epoch: 5 \t\t\t Iteration 988 Loss Train: 0.25897300243377686\n",
      "Epoch: 5 \t\t\t Iteration 989 Loss Train: 0.3318275511264801\n",
      "Epoch: 5 \t\t\t Iteration 990 Loss Train: 0.3096679449081421\n",
      "Epoch: 5 \t\t\t Iteration 991 Loss Train: 0.3259596824645996\n",
      "Epoch: 5 \t\t\t Iteration 992 Loss Train: 0.46808677911758423\n",
      "Epoch: 5 \t\t\t Iteration 993 Loss Train: 0.4034981429576874\n",
      "Epoch: 5 \t\t\t Iteration 994 Loss Train: 0.33289632201194763\n",
      "Epoch: 5 \t\t\t Iteration 995 Loss Train: 0.318792462348938\n",
      "Epoch: 5 \t\t\t Iteration 996 Loss Train: 0.6954982280731201\n",
      "Epoch: 5 \t\t\t Iteration 997 Loss Train: 0.43182796239852905\n",
      "Epoch: 5 \t\t\t Iteration 998 Loss Train: 0.4522309899330139\n",
      "Epoch: 5 \t\t\t Iteration 999 Loss Train: 0.6671208739280701\n",
      "Epoch: 5 \t\t\t Iteration 1000 Loss Train: 0.44433724880218506\n",
      "Epoch: 5 \t\t\t Iteration 1001 Loss Train: 0.48816192150115967\n",
      "Epoch: 5 \t\t\t Iteration 1002 Loss Train: 0.5033761858940125\n",
      "Epoch: 5 \t\t\t Iteration 1003 Loss Train: 0.2115265429019928\n",
      "Epoch: 5 \t\t\t Iteration 1004 Loss Train: 0.310683012008667\n",
      "Epoch: 5 \t\t\t Iteration 1005 Loss Train: 0.5528661608695984\n",
      "Epoch: 5 \t\t\t Iteration 1006 Loss Train: 0.29007476568222046\n",
      "Epoch: 5 \t\t\t Iteration 1007 Loss Train: 0.4605443477630615\n",
      "Epoch: 5 \t\t\t Iteration 1008 Loss Train: 0.39152050018310547\n",
      "Epoch: 5 \t\t\t Iteration 1009 Loss Train: 0.369406521320343\n",
      "Epoch: 5 \t\t\t Iteration 1010 Loss Train: 0.4869796931743622\n",
      "Epoch: 5 \t\t\t Iteration 1011 Loss Train: 0.47728967666625977\n",
      "Epoch: 5 \t\t\t Iteration 1012 Loss Train: 0.4730614125728607\n",
      "Epoch: 5 \t\t\t Iteration 1013 Loss Train: 0.2552196979522705\n",
      "Epoch: 5 \t\t\t Iteration 1014 Loss Train: 0.5034210085868835\n",
      "Epoch: 5 \t\t\t Iteration 1015 Loss Train: 0.343976765871048\n",
      "Epoch: 5 \t\t\t Iteration 1016 Loss Train: 0.3879432678222656\n",
      "Epoch: 5 \t\t\t Iteration 1017 Loss Train: 0.5295224189758301\n",
      "Epoch: 5 \t\t\t Iteration 1018 Loss Train: 0.5450572371482849\n",
      "Epoch: 5 \t\t\t Iteration 1019 Loss Train: 0.3102395534515381\n",
      "Epoch: 5 \t\t\t Iteration 1020 Loss Train: 0.4483937919139862\n",
      "Epoch: 5 \t\t\t Iteration 1021 Loss Train: 0.4250895082950592\n",
      "Epoch: 5 \t\t\t Iteration 1022 Loss Train: 0.5775481462478638\n",
      "Epoch: 5 \t\t\t Iteration 1023 Loss Train: 0.3820868730545044\n",
      "Epoch: 5 \t\t\t Iteration 1024 Loss Train: 0.23321306705474854\n",
      "Epoch: 5 \t\t\t Iteration 1025 Loss Train: 0.34588557481765747\n",
      "Epoch: 5 \t\t\t Iteration 1026 Loss Train: 0.3962898850440979\n",
      "Epoch: 5 \t\t\t Iteration 1027 Loss Train: 0.5189605951309204\n",
      "Epoch: 5 \t\t\t Iteration 1028 Loss Train: 0.3197466731071472\n",
      "Epoch: 5 \t\t\t Iteration 1029 Loss Train: 0.11962009966373444\n",
      "Epoch: 5 \t\t\t Iteration 1030 Loss Train: 0.46177077293395996\n",
      "Epoch: 5 \t\t\t Iteration 1031 Loss Train: 0.3078038990497589\n",
      "Epoch: 5 \t\t\t Iteration 1032 Loss Train: 0.5086199045181274\n",
      "Epoch: 5 \t\t\t Iteration 1033 Loss Train: 0.4139387011528015\n",
      "Epoch: 5 \t\t\t Iteration 1034 Loss Train: 0.45528319478034973\n",
      "Epoch: 5 \t\t\t Iteration 1035 Loss Train: 0.35983818769454956\n",
      "Epoch: 5 \t\t\t Iteration 1036 Loss Train: 0.23883342742919922\n",
      "Epoch: 5 \t\t\t Iteration 1037 Loss Train: 0.22211547195911407\n",
      "Epoch: 5 \t\t\t Iteration 1038 Loss Train: 0.4551883935928345\n",
      "Epoch: 5 \t\t\t Iteration 1039 Loss Train: 0.3667736053466797\n",
      "Epoch: 5 \t\t\t Iteration 1040 Loss Train: 0.3928537368774414\n",
      "Epoch: 5 \t\t\t Iteration 1041 Loss Train: 0.568416178226471\n",
      "Epoch: 5 \t\t\t Iteration 1042 Loss Train: 0.48490017652511597\n",
      "Epoch: 5 \t\t\t Iteration 1043 Loss Train: 0.6010087728500366\n",
      "Epoch: 5 \t\t\t Iteration 1044 Loss Train: 0.38899827003479004\n",
      "Epoch: 5 \t\t\t Iteration 1045 Loss Train: 0.37836477160453796\n",
      "Epoch: 5 \t\t\t Iteration 1046 Loss Train: 0.3334587812423706\n",
      "Epoch: 5 \t\t\t Iteration 1047 Loss Train: 0.28276538848876953\n",
      "Epoch: 5 \t\t\t Iteration 1048 Loss Train: 0.6038984656333923\n",
      "Epoch: 5 \t\t\t Iteration 1049 Loss Train: 0.3616085946559906\n",
      "Epoch: 5 \t\t\t Iteration 1050 Loss Train: 0.4925227165222168\n",
      "Epoch: 5 \t\t\t Iteration 1051 Loss Train: 0.3656589090824127\n",
      "Epoch: 5 \t\t\t Iteration 1052 Loss Train: 0.5349351167678833\n",
      "Epoch: 5 \t\t\t Iteration 1053 Loss Train: 0.44915077090263367\n",
      "Epoch: 5 \t\t\t Iteration 1054 Loss Train: 0.34428584575653076\n",
      "Epoch: 5 \t\t\t Iteration 1055 Loss Train: 0.5246676802635193\n",
      "Epoch: 5 \t\t\t Iteration 1056 Loss Train: 0.3170148730278015\n",
      "Epoch: 5 \t\t\t Iteration 1057 Loss Train: 0.4112618565559387\n",
      "Epoch: 5 \t\t\t Iteration 1058 Loss Train: 0.5258944034576416\n",
      "Epoch: 5 \t\t\t Iteration 1059 Loss Train: 0.41341930627822876\n",
      "Epoch: 5 \t\t\t Iteration 1060 Loss Train: 0.3686901330947876\n",
      "Epoch: 5 \t\t\t Iteration 1061 Loss Train: 0.22278305888175964\n",
      "Epoch: 5 \t\t\t Iteration 1062 Loss Train: 0.48880407214164734\n",
      "Epoch: 5 \t\t\t Iteration 1063 Loss Train: 0.47794610261917114\n",
      "Epoch: 5 \t\t\t Iteration 1064 Loss Train: 0.2832096815109253\n",
      "Epoch: 5 \t\t\t Iteration 1065 Loss Train: 0.4350201487541199\n",
      "Epoch: 5 \t\t\t Iteration 1066 Loss Train: 0.5606940388679504\n",
      "Epoch: 5 \t\t\t Iteration 1067 Loss Train: 0.4260311424732208\n",
      "Epoch: 5 \t\t\t Iteration 1068 Loss Train: 0.4176170527935028\n",
      "Epoch: 5 \t\t\t Iteration 1069 Loss Train: 0.29329177737236023\n",
      "Epoch: 5 \t\t\t Iteration 1070 Loss Train: 0.3269941806793213\n",
      "Epoch: 5 \t\t\t Iteration 1071 Loss Train: 0.19862520694732666\n",
      "Epoch: 5 \t\t\t Iteration 1072 Loss Train: 0.4011036455631256\n",
      "Epoch: 5 \t\t\t Iteration 1073 Loss Train: 0.4002823531627655\n",
      "Epoch: 5 \t\t\t Iteration 1074 Loss Train: 0.6732775568962097\n",
      "Epoch: 5 \t\t\t Iteration 1075 Loss Train: 0.26830559968948364\n",
      "Epoch: 5 \t\t\t Iteration 1076 Loss Train: 0.34417012333869934\n",
      "Epoch: 5 \t\t\t Iteration 1077 Loss Train: 0.3426397442817688\n",
      "Epoch: 5 \t\t\t Iteration 1078 Loss Train: 0.39747166633605957\n",
      "Epoch: 5 \t\t\t Iteration 1079 Loss Train: 0.3742312490940094\n",
      "Epoch: 5 \t\t\t Iteration 1080 Loss Train: 0.2629707157611847\n",
      "Epoch: 5 \t\t\t Iteration 1081 Loss Train: 0.3120938539505005\n",
      "Epoch: 5 \t\t\t Iteration 1082 Loss Train: 0.265296995639801\n",
      "Epoch: 5 \t\t\t Iteration 1083 Loss Train: 0.4915660321712494\n",
      "Epoch: 5 \t\t\t Iteration 1084 Loss Train: 0.3671686053276062\n",
      "Epoch: 5 \t\t\t Iteration 1085 Loss Train: 0.5095165967941284\n",
      "Epoch: 5 \t\t\t Iteration 1086 Loss Train: 0.4862556457519531\n",
      "Epoch: 5 \t\t\t Iteration 1087 Loss Train: 0.4870806336402893\n",
      "Epoch: 5 \t\t\t Iteration 1088 Loss Train: 0.572290301322937\n",
      "Epoch: 5 \t\t\t Iteration 1089 Loss Train: 0.5368350744247437\n",
      "Epoch: 5 \t\t\t Iteration 1090 Loss Train: 0.4290391206741333\n",
      "Epoch: 5 \t\t\t Iteration 1091 Loss Train: 0.412075936794281\n",
      "Epoch: 5 \t\t\t Iteration 1092 Loss Train: 0.4105651080608368\n",
      "Epoch: 5 \t\t\t Iteration 1093 Loss Train: 0.44412344694137573\n",
      "Epoch: 5 \t\t\t Iteration 1094 Loss Train: 0.48466381430625916\n",
      "Epoch: 5 \t\t\t Iteration 1095 Loss Train: 0.3047841489315033\n",
      "Epoch: 5 \t\t\t Iteration 1096 Loss Train: 0.3780669867992401\n",
      "Epoch: 5 \t\t\t Iteration 1097 Loss Train: 0.7935419678688049\n",
      "Epoch: 5 \t\t\t Iteration 1098 Loss Train: 0.4911954700946808\n",
      "Epoch: 5 \t\t\t Iteration 1099 Loss Train: 0.39960506558418274\n",
      "Epoch: 5 \t\t\t Iteration 1100 Loss Train: 0.41667640209198\n",
      "Epoch: 5 \t\t\t Iteration 1101 Loss Train: 0.2814379334449768\n",
      "Epoch: 5 \t\t\t Iteration 1102 Loss Train: 0.32492294907569885\n",
      "Epoch: 5 \t\t\t Iteration 1103 Loss Train: 0.4486422836780548\n",
      "Epoch: 5 \t\t\t Iteration 1104 Loss Train: 0.4190991520881653\n",
      "Epoch: 5 \t\t\t Iteration 1105 Loss Train: 0.39960065484046936\n",
      "Epoch: 5 \t\t\t Iteration 1106 Loss Train: 0.3876168131828308\n",
      "Epoch: 5 \t\t\t Iteration 1107 Loss Train: 0.4081125855445862\n",
      "Epoch: 5 \t\t\t Iteration 1108 Loss Train: 0.22620829939842224\n",
      "Epoch: 5 \t\t\t Iteration 1109 Loss Train: 0.49355289340019226\n",
      "Epoch: 5 \t\t\t Iteration 1110 Loss Train: 0.4064635634422302\n",
      "Epoch: 5 \t\t\t Iteration 1111 Loss Train: 0.5303032398223877\n",
      "Epoch: 5 \t\t\t Iteration 1112 Loss Train: 0.3132418692111969\n",
      "Epoch: 5 \t\t\t Iteration 1113 Loss Train: 0.45030778646469116\n",
      "Epoch: 5 \t\t\t Iteration 1114 Loss Train: 0.31925347447395325\n",
      "Epoch: 5 \t\t\t Iteration 1115 Loss Train: 0.3417823314666748\n",
      "Epoch: 5 \t\t\t Iteration 1116 Loss Train: 0.4836385250091553\n",
      "Epoch: 5 \t\t\t Iteration 1117 Loss Train: 0.45035579800605774\n",
      "Epoch: 5 \t\t\t Iteration 1118 Loss Train: 0.33568453788757324\n",
      "Epoch: 5 \t\t\t Iteration 1119 Loss Train: 0.3552880883216858\n",
      "Epoch: 5 \t\t\t Iteration 1120 Loss Train: 0.4017893671989441\n",
      "Epoch: 5 \t\t\t Iteration 1121 Loss Train: 0.37368083000183105\n",
      "Epoch: 5 \t\t\t Iteration 1122 Loss Train: 0.45323485136032104\n",
      "Epoch: 5 \t\t\t Iteration 1123 Loss Train: 0.45124298334121704\n",
      "Epoch: 5 \t\t\t Iteration 1124 Loss Train: 0.5006729960441589\n",
      "Epoch: 5 \t\t\t Iteration 1125 Loss Train: 0.4334358870983124\n",
      "Epoch: 5 \t\t\t Iteration 1126 Loss Train: 0.43936818838119507\n",
      "Epoch: 5 \t\t\t Iteration 1127 Loss Train: 0.4142327308654785\n",
      "Epoch: 5 \t\t\t Iteration 1128 Loss Train: 0.2700410485267639\n",
      "Epoch: 5 \t\t\t Iteration 1129 Loss Train: 0.5971851944923401\n",
      "Epoch: 5 \t\t\t Iteration 1130 Loss Train: 0.25166642665863037\n",
      "Epoch: 5 \t\t\t Iteration 1131 Loss Train: 0.4747259020805359\n",
      "Epoch: 5 \t\t\t Iteration 1132 Loss Train: 0.373735249042511\n",
      "Epoch: 5 \t\t\t Iteration 1133 Loss Train: 0.5096384286880493\n",
      "Epoch: 5 \t\t\t Iteration 1134 Loss Train: 0.3071793019771576\n",
      "Epoch: 5 \t\t\t Iteration 1135 Loss Train: 0.2600509524345398\n",
      "Epoch: 5 \t\t\t Iteration 1136 Loss Train: 0.566364049911499\n",
      "Epoch: 5 \t\t\t Iteration 1137 Loss Train: 0.40651869773864746\n",
      "Epoch: 5 \t\t\t Iteration 1138 Loss Train: 0.32124871015548706\n",
      "Epoch: 5 \t\t\t Iteration 1139 Loss Train: 0.3346138000488281\n",
      "Epoch: 5 \t\t\t Iteration 1140 Loss Train: 0.464974582195282\n",
      "Epoch: 5 \t\t\t Iteration 1141 Loss Train: 0.5500711798667908\n",
      "Epoch: 5 \t\t\t Iteration 1142 Loss Train: 0.3656008541584015\n",
      "Epoch: 5 \t\t\t Iteration 1143 Loss Train: 0.3728049397468567\n",
      "Epoch: 5 \t\t\t Iteration 1144 Loss Train: 0.4981013834476471\n",
      "Epoch: 5 \t\t\t Iteration 1145 Loss Train: 0.48693835735321045\n",
      "Epoch: 5 \t\t\t Iteration 1146 Loss Train: 0.45110851526260376\n",
      "Epoch: 5 \t\t\t Iteration 1147 Loss Train: 0.3967314660549164\n",
      "Epoch: 5 \t\t\t Iteration 1148 Loss Train: 0.3669397532939911\n",
      "Epoch: 5 \t\t\t Iteration 1149 Loss Train: 0.36176371574401855\n",
      "Epoch: 5 \t\t\t Iteration 1150 Loss Train: 0.3562466502189636\n",
      "Epoch: 5 \t\t\t Iteration 1151 Loss Train: 0.5470749735832214\n",
      "Epoch: 5 \t\t\t Iteration 1152 Loss Train: 0.5068049430847168\n",
      "Epoch: 5 \t\t\t Iteration 1153 Loss Train: 0.30030930042266846\n",
      "Epoch: 5 \t\t\t Iteration 1154 Loss Train: 0.5482057929039001\n",
      "Epoch: 5 \t\t\t Iteration 1155 Loss Train: 0.3655378222465515\n",
      "Epoch: 5 \t\t\t Iteration 1156 Loss Train: 0.4087105095386505\n",
      "Epoch: 5 \t\t\t Iteration 1157 Loss Train: 0.404498428106308\n",
      "Epoch: 5 \t\t\t Iteration 1158 Loss Train: 0.3555881977081299\n",
      "Epoch: 5 \t\t\t Iteration 1159 Loss Train: 0.3199297785758972\n",
      "Epoch: 5 \t\t\t Iteration 1160 Loss Train: 0.32152941823005676\n",
      "Epoch: 5 \t\t\t Iteration 1161 Loss Train: 0.22572538256645203\n",
      "Epoch: 5 \t\t\t Iteration 1162 Loss Train: 0.2537851631641388\n",
      "Epoch: 5 \t\t\t Iteration 1163 Loss Train: 0.16730915009975433\n",
      "Epoch: 5 \t\t\t Iteration 1164 Loss Train: 0.6262874007225037\n",
      "Epoch: 5 \t\t\t Iteration 1165 Loss Train: 0.42245763540267944\n",
      "Epoch: 5 \t\t\t Iteration 1166 Loss Train: 0.3333331048488617\n",
      "Epoch: 5 \t\t\t Iteration 1167 Loss Train: 0.3003493547439575\n",
      "Epoch: 5 \t\t\t Iteration 1168 Loss Train: 0.4296516478061676\n",
      "Epoch: 5 \t\t\t Iteration 1169 Loss Train: 0.3709624409675598\n",
      "Epoch: 5 \t\t\t Iteration 1170 Loss Train: 0.5494670867919922\n",
      "Epoch: 5 \t\t\t Iteration 1171 Loss Train: 0.3471771478652954\n",
      "Epoch: 5 \t\t\t Iteration 1172 Loss Train: 0.42508354783058167\n",
      "Epoch: 5 \t\t\t Iteration 1173 Loss Train: 0.40027880668640137\n",
      "Epoch: 5 \t\t\t Iteration 1174 Loss Train: 0.32220256328582764\n",
      "Epoch: 5 \t\t\t Iteration 1175 Loss Train: 0.3913796544075012\n",
      "Epoch: 5 \t\t\t Iteration 1176 Loss Train: 0.4512975215911865\n",
      "Epoch: 5 \t\t\t Iteration 1177 Loss Train: 0.5839220881462097\n",
      "Epoch: 5 \t\t\t Iteration 1178 Loss Train: 0.43978187441825867\n",
      "Epoch: 5 \t\t\t Iteration 1179 Loss Train: 0.34906336665153503\n",
      "Epoch: 5 \t\t\t Iteration 1180 Loss Train: 0.3211250603199005\n",
      "Epoch: 5 \t\t\t Iteration 1181 Loss Train: 0.6911756992340088\n",
      "Epoch: 5 \t\t\t Iteration 1182 Loss Train: 0.3816410303115845\n",
      "Epoch: 5 \t\t\t Iteration 1183 Loss Train: 0.4832213521003723\n",
      "Epoch: 5 \t\t\t Iteration 1184 Loss Train: 0.4662954807281494\n",
      "Epoch: 5 \t\t\t Iteration 1185 Loss Train: 0.5212103128433228\n",
      "Epoch: 5 \t\t\t Iteration 1186 Loss Train: 0.2774035930633545\n",
      "Epoch: 5 \t\t\t Iteration 1187 Loss Train: 0.328033983707428\n",
      "Epoch: 5 \t\t\t Iteration 1188 Loss Train: 0.43519580364227295\n",
      "Epoch: 5 \t\t\t Iteration 1189 Loss Train: 0.44418224692344666\n",
      "Epoch: 5 \t\t\t Iteration 1190 Loss Train: 0.36687347292900085\n",
      "Epoch: 5 \t\t\t Iteration 1191 Loss Train: 0.3418408930301666\n",
      "Epoch: 5 \t\t\t Iteration 1192 Loss Train: 0.354377806186676\n",
      "Epoch: 5 \t\t\t Iteration 1193 Loss Train: 0.4104290008544922\n",
      "Epoch: 5 \t\t\t Iteration 1194 Loss Train: 0.4619174599647522\n",
      "Epoch: 5 \t\t\t Iteration 1195 Loss Train: 0.42925384640693665\n",
      "Epoch: 5 \t\t\t Iteration 1196 Loss Train: 0.6793091893196106\n",
      "Epoch: 5 \t\t\t Iteration 1197 Loss Train: 0.37331074476242065\n",
      "Epoch: 5 \t\t\t Iteration 1198 Loss Train: 0.5624266266822815\n",
      "Epoch: 5 \t\t\t Iteration 1199 Loss Train: 0.33203500509262085\n",
      "Epoch: 5 \t\t\t Iteration 1200 Loss Train: 0.3848726749420166\n",
      "Epoch: 5 \t\t\t Iteration 1201 Loss Train: 0.516115128993988\n",
      "Epoch: 5 \t\t\t Iteration 1202 Loss Train: 0.2680063545703888\n",
      "Epoch: 5 \t\t\t Iteration 1203 Loss Train: 0.41391605138778687\n",
      "Epoch: 5 \t\t\t Iteration 1204 Loss Train: 0.51372230052948\n",
      "Epoch: 5 \t\t\t Iteration 1205 Loss Train: 0.4222586154937744\n",
      "Epoch: 5 \t\t\t Iteration 1206 Loss Train: 0.5042548775672913\n",
      "Epoch: 5 \t\t\t Iteration 1207 Loss Train: 0.4977354407310486\n",
      "Epoch: 5 \t\t\t Iteration 1208 Loss Train: 0.28986841440200806\n",
      "Epoch: 5 \t\t\t Iteration 1209 Loss Train: 0.31961965560913086\n",
      "Epoch: 5 \t\t\t Iteration 1210 Loss Train: 0.2581191062927246\n",
      "Epoch: 5 \t\t\t Iteration 1211 Loss Train: 0.6191189289093018\n",
      "Epoch: 5 \t\t\t Iteration 1212 Loss Train: 0.4225143492221832\n",
      "Epoch: 5 \t\t\t Iteration 1213 Loss Train: 0.3953426778316498\n",
      "Epoch: 5 \t\t\t Iteration 1214 Loss Train: 0.4654877781867981\n",
      "Epoch: 5 \t\t\t Iteration 1215 Loss Train: 0.3591424822807312\n",
      "Epoch: 5 \t\t\t Iteration 1216 Loss Train: 0.2785959541797638\n",
      "Epoch: 5 \t\t\t Iteration 1217 Loss Train: 0.5768392086029053\n",
      "Epoch: 5 \t\t\t Iteration 1218 Loss Train: 0.5267030596733093\n",
      "Epoch: 5 \t\t\t Iteration 1219 Loss Train: 0.3073985278606415\n",
      "Epoch: 5 \t\t\t Iteration 1220 Loss Train: 0.5429109334945679\n",
      "Epoch: 5 \t\t\t Iteration 1221 Loss Train: 0.2533409297466278\n",
      "Epoch: 5 \t\t\t Iteration 1222 Loss Train: 0.26028794050216675\n",
      "Epoch: 5 \t\t\t Iteration 1223 Loss Train: 0.41975271701812744\n",
      "Epoch: 5 \t\t\t Iteration 1224 Loss Train: 0.35375308990478516\n",
      "Epoch: 5 \t\t\t Iteration 1225 Loss Train: 0.31209683418273926\n",
      "Epoch: 5 \t\t\t Iteration 1226 Loss Train: 0.46464890241622925\n",
      "Epoch: 5 \t\t\t Iteration 1227 Loss Train: 0.42471593618392944\n",
      "Epoch: 5 \t\t\t Iteration 1228 Loss Train: 0.43668225407600403\n",
      "Epoch: 5 \t\t\t Iteration 1229 Loss Train: 0.5310577750205994\n",
      "Epoch: 5 \t\t\t Iteration 1230 Loss Train: 0.33125776052474976\n",
      "Epoch: 5 \t\t\t Iteration 1231 Loss Train: 0.3324700593948364\n",
      "Epoch: 5 \t\t\t Iteration 1232 Loss Train: 0.4481585919857025\n",
      "Epoch: 5 \t\t\t Iteration 1233 Loss Train: 0.29567816853523254\n",
      "Epoch: 5 \t\t\t Iteration 1234 Loss Train: 0.49619412422180176\n",
      "Epoch: 5 \t\t\t Iteration 1235 Loss Train: 0.3511647880077362\n",
      "Epoch: 5 \t\t\t Iteration 1236 Loss Train: 0.3629119396209717\n",
      "Epoch: 5 \t\t\t Iteration 1237 Loss Train: 0.37285351753234863\n",
      "Epoch: 5 \t\t\t Iteration 1238 Loss Train: 0.6805886626243591\n",
      "Epoch: 5 \t\t\t Iteration 1239 Loss Train: 0.5158305168151855\n",
      "Epoch: 5 \t\t\t Iteration 1240 Loss Train: 0.4572533965110779\n",
      "Epoch: 5 \t\t\t Iteration 1241 Loss Train: 0.36102649569511414\n",
      "Epoch: 5 \t\t\t Iteration 1242 Loss Train: 0.4214724898338318\n",
      "Epoch: 5 \t\t\t Iteration 1243 Loss Train: 0.3849011957645416\n",
      "Epoch: 5 \t\t\t Iteration 1244 Loss Train: 0.49457448720932007\n",
      "Epoch: 5 \t\t\t Iteration 1245 Loss Train: 0.19302788376808167\n",
      "Epoch: 5 \t\t\t Iteration 1246 Loss Train: 0.4623016119003296\n",
      "Epoch: 5 \t\t\t Iteration 1247 Loss Train: 0.3443290889263153\n",
      "Epoch: 5 \t\t\t Iteration 1248 Loss Train: 0.2075875699520111\n",
      "Epoch: 5 \t\t\t Iteration 1249 Loss Train: 0.6758370399475098\n",
      "Epoch: 5 \t\t\t Iteration 1250 Loss Train: 0.40571165084838867\n",
      "Epoch: 5 \t\t\t Iteration 1251 Loss Train: 0.3732919692993164\n",
      "Epoch: 5 \t\t\t Iteration 1252 Loss Train: 0.4729236364364624\n",
      "Epoch: 5 \t\t\t Iteration 1253 Loss Train: 0.38201022148132324\n",
      "Epoch: 5 \t\t\t Iteration 1254 Loss Train: 0.43797460198402405\n",
      "Epoch: 5 \t\t\t Iteration 1255 Loss Train: 0.26618897914886475\n",
      "Epoch: 5 \t\t\t Iteration 1256 Loss Train: 0.3755647540092468\n",
      "Epoch: 5 \t\t\t Iteration 1257 Loss Train: 0.3991818428039551\n",
      "Epoch: 5 \t\t\t Iteration 1258 Loss Train: 0.3533654808998108\n",
      "Epoch: 5 \t\t\t Iteration 1259 Loss Train: 0.47558557987213135\n",
      "Epoch: 5 \t\t\t Iteration 1260 Loss Train: 0.37966153025627136\n",
      "Epoch: 5 \t\t\t Iteration 1261 Loss Train: 0.15926861763000488\n",
      "Epoch: 5 \t\t\t Iteration 1262 Loss Train: 0.2564403712749481\n",
      "Epoch: 5 \t\t\t Iteration 1263 Loss Train: 0.40319740772247314\n",
      "Epoch: 5 \t\t\t Iteration 1264 Loss Train: 0.42387521266937256\n",
      "Epoch: 5 \t\t\t Iteration 1265 Loss Train: 0.46633097529411316\n",
      "Epoch: 5 \t\t\t Iteration 1266 Loss Train: 0.5482158064842224\n",
      "Epoch: 5 \t\t\t Iteration 1267 Loss Train: 0.4014102518558502\n",
      "Epoch: 5 \t\t\t Iteration 1268 Loss Train: 0.33118095993995667\n",
      "Epoch: 5 \t\t\t Iteration 1269 Loss Train: 0.37000757455825806\n",
      "Epoch: 5 \t\t\t Iteration 1270 Loss Train: 0.45434093475341797\n",
      "Epoch: 5 \t\t\t Iteration 1271 Loss Train: 0.3467535376548767\n",
      "Epoch: 5 \t\t\t Iteration 1272 Loss Train: 0.2528790831565857\n",
      "Epoch: 5 \t\t\t Iteration 1273 Loss Train: 0.35393548011779785\n",
      "Epoch: 5 \t\t\t Iteration 1274 Loss Train: 0.3327163755893707\n",
      "Epoch: 5 \t\t\t Iteration 1275 Loss Train: 0.33321821689605713\n",
      "Epoch: 5 \t\t\t Iteration 1276 Loss Train: 0.4460856318473816\n",
      "Epoch: 5 \t\t\t Iteration 1277 Loss Train: 0.33893388509750366\n",
      "Epoch: 5 \t\t\t Iteration 1278 Loss Train: 0.3299787938594818\n",
      "Epoch: 5 \t\t\t Iteration 1279 Loss Train: 0.3557299077510834\n",
      "Epoch: 5 \t\t\t Iteration 1280 Loss Train: 0.3671175241470337\n",
      "Epoch: 5 \t\t\t Iteration 1281 Loss Train: 0.394732266664505\n",
      "Epoch: 5 \t\t\t Iteration 1282 Loss Train: 0.22316110134124756\n",
      "Epoch: 5 \t\t\t Iteration 1283 Loss Train: 0.3004816174507141\n",
      "Epoch: 5 \t\t\t Iteration 1284 Loss Train: 0.6265963315963745\n",
      "Epoch: 5 \t\t\t Iteration 1285 Loss Train: 0.38469523191452026\n",
      "Epoch: 5 \t\t\t Iteration 1286 Loss Train: 0.5138435363769531\n",
      "Epoch: 5 \t\t\t Iteration 1287 Loss Train: 0.42160564661026\n",
      "Epoch: 5 \t\t\t Iteration 1288 Loss Train: 0.3644968569278717\n",
      "Epoch: 5 \t\t\t Iteration 1289 Loss Train: 0.3159129321575165\n",
      "Epoch: 5 \t\t\t Iteration 1290 Loss Train: 0.38706737756729126\n",
      "Epoch: 5 \t\t\t Iteration 1291 Loss Train: 0.5705273151397705\n",
      "Epoch: 5 \t\t\t Iteration 1292 Loss Train: 0.33487266302108765\n",
      "Epoch: 5 \t\t\t Iteration 1293 Loss Train: 0.4183587431907654\n",
      "Epoch: 5 \t\t\t Iteration 1294 Loss Train: 0.35158854722976685\n",
      "Epoch: 5 \t\t\t Iteration 1295 Loss Train: 0.39501988887786865\n",
      "Epoch: 5 \t\t\t Iteration 1296 Loss Train: 0.3210393190383911\n",
      "Epoch: 5 \t\t\t Iteration 1297 Loss Train: 0.4332424998283386\n",
      "Epoch: 5 \t\t\t Iteration 1298 Loss Train: 0.43158990144729614\n",
      "Epoch: 5 \t\t\t Iteration 1299 Loss Train: 0.7001282572746277\n",
      "Epoch: 5 \t\t\t Iteration 1300 Loss Train: 0.47960537672042847\n",
      "Epoch: 5 \t\t\t Iteration 1301 Loss Train: 0.45319807529449463\n",
      "Epoch: 5 \t\t\t Iteration 1302 Loss Train: 0.544249415397644\n",
      "Epoch: 5 \t\t\t Iteration 1303 Loss Train: 0.38480669260025024\n",
      "Epoch: 5 \t\t\t Iteration 1304 Loss Train: 0.28284114599227905\n",
      "Epoch: 5 \t\t\t Iteration 1305 Loss Train: 0.2871582508087158\n",
      "Epoch: 5 \t\t\t Iteration 1306 Loss Train: 0.32118701934814453\n",
      "Epoch: 5 \t\t\t Iteration 1307 Loss Train: 0.373961865901947\n",
      "Epoch: 5 \t\t\t Iteration 1308 Loss Train: 0.3561766743659973\n",
      "Epoch: 5 \t\t\t Iteration 1309 Loss Train: 0.5375422239303589\n",
      "Epoch: 5 \t\t\t Iteration 1310 Loss Train: 0.5530901551246643\n",
      "Epoch: 5 \t\t\t Iteration 1311 Loss Train: 0.39304476976394653\n",
      "Epoch: 5 \t\t\t Iteration 1312 Loss Train: 0.31643372774124146\n",
      "Epoch: 5 \t\t\t Iteration 1313 Loss Train: 0.7208642363548279\n",
      "Epoch: 5 \t\t\t Iteration 1314 Loss Train: 0.42467647790908813\n",
      "Epoch: 5 \t\t\t Iteration 1315 Loss Train: 0.3898226320743561\n",
      "Epoch: 5 \t\t\t Iteration 1316 Loss Train: 0.46107515692710876\n",
      "Epoch: 5 \t\t\t Iteration 1317 Loss Train: 0.3246416747570038\n",
      "Epoch: 5 \t\t\t Iteration 1318 Loss Train: 0.5378570556640625\n",
      "Epoch: 5 \t\t\t Iteration 1319 Loss Train: 0.5173227787017822\n",
      "Epoch: 5 \t\t\t Iteration 1320 Loss Train: 0.3350653052330017\n",
      "Epoch: 5 \t\t\t Iteration 1321 Loss Train: 0.3927454352378845\n",
      "Epoch: 5 \t\t\t Iteration 1322 Loss Train: 0.4283841848373413\n",
      "Epoch: 5 \t\t\t Iteration 1323 Loss Train: 0.2739781439304352\n",
      "Epoch: 5 \t\t\t Iteration 1324 Loss Train: 0.43946120142936707\n",
      "Epoch: 5 \t\t\t Iteration 1325 Loss Train: 0.5741186141967773\n",
      "Epoch: 5 \t\t\t Iteration 1326 Loss Train: 0.43739211559295654\n",
      "Epoch: 5 \t\t\t Iteration 1327 Loss Train: 0.45256608724594116\n",
      "Epoch: 5 \t\t\t Iteration 1328 Loss Train: 0.476203590631485\n",
      "Epoch: 5 \t\t\t Iteration 1329 Loss Train: 0.43847909569740295\n",
      "Epoch: 5 \t\t\t Iteration 1330 Loss Train: 0.3716232478618622\n",
      "Epoch: 5 \t\t\t Iteration 1331 Loss Train: 0.41908547282218933\n",
      "Epoch: 5 \t\t\t Iteration 1332 Loss Train: 0.4791785776615143\n",
      "Epoch: 5 \t\t\t Iteration 1333 Loss Train: 0.45938822627067566\n",
      "Epoch: 5 \t\t\t Iteration 1334 Loss Train: 0.42525172233581543\n",
      "Epoch: 5 \t\t\t Iteration 1335 Loss Train: 0.3416021168231964\n",
      "Epoch: 5 \t\t\t Iteration 1336 Loss Train: 0.1733642816543579\n",
      "Epoch: 5 \t\t\t Iteration 1337 Loss Train: 0.7033405303955078\n",
      "Epoch: 5 \t\t\t Iteration 1338 Loss Train: 0.4610676169395447\n",
      "Epoch: 5 \t\t\t Iteration 1339 Loss Train: 0.4267137050628662\n",
      "Epoch: 5 \t\t\t Iteration 1340 Loss Train: 0.5422025918960571\n",
      "Epoch: 5 \t\t\t Iteration 1341 Loss Train: 0.40963292121887207\n",
      "Epoch: 5 \t\t\t Iteration 1342 Loss Train: 0.36426037549972534\n",
      "Epoch: 5 \t\t\t Iteration 1343 Loss Train: 0.32289624214172363\n",
      "Epoch: 5 \t\t\t Iteration 1344 Loss Train: 0.18959859013557434\n",
      "Epoch: 5 \t\t\t Iteration 1345 Loss Train: 0.3645598888397217\n",
      "Epoch: 5 \t\t\t Iteration 1346 Loss Train: 0.4967796206474304\n",
      "Epoch: 5 \t\t\t Iteration 1347 Loss Train: 0.501052975654602\n",
      "Epoch: 5 \t\t\t Iteration 1348 Loss Train: 0.5366078615188599\n",
      "Epoch: 5 \t\t\t Iteration 1349 Loss Train: 0.4578016698360443\n",
      "Epoch: 5 \t\t\t Iteration 1350 Loss Train: 0.26709240674972534\n",
      "Epoch: 5 \t\t\t Iteration 1351 Loss Train: 0.45111432671546936\n",
      "Epoch: 5 \t\t\t Iteration 1352 Loss Train: 0.2818678915500641\n",
      "Epoch: 5 \t\t\t Iteration 1353 Loss Train: 0.4561077654361725\n",
      "Epoch: 5 \t\t\t Iteration 1354 Loss Train: 0.5074585676193237\n",
      "Epoch: 5 \t\t\t Iteration 1355 Loss Train: 0.50141441822052\n",
      "Epoch: 5 \t\t\t Iteration 1356 Loss Train: 0.46981942653656006\n",
      "Epoch: 5 \t\t\t Iteration 1357 Loss Train: 0.3681296408176422\n",
      "Epoch: 5 \t\t\t Iteration 1358 Loss Train: 0.45131805539131165\n",
      "Epoch: 5 \t\t\t Iteration 1359 Loss Train: 0.43937748670578003\n",
      "Epoch: 5 \t\t\t Iteration 1360 Loss Train: 0.4553341567516327\n",
      "Epoch: 5 \t\t\t Iteration 1361 Loss Train: 0.31565549969673157\n",
      "Epoch: 5 \t\t\t Iteration 1362 Loss Train: 0.5226031541824341\n",
      "Epoch: 5 \t\t\t Iteration 1363 Loss Train: 0.23055729269981384\n",
      "Epoch: 5 \t\t\t Iteration 1364 Loss Train: 0.2974448800086975\n",
      "Epoch: 5 \t\t\t Iteration 1365 Loss Train: 0.23079349100589752\n",
      "Epoch: 5 \t\t\t Iteration 1366 Loss Train: 0.7291895747184753\n",
      "Epoch: 5 \t\t\t Iteration 1367 Loss Train: 0.38759392499923706\n",
      "Epoch: 5 \t\t\t Iteration 1368 Loss Train: 0.36725497245788574\n",
      "Epoch: 5 \t\t\t Iteration 1369 Loss Train: 0.39509475231170654\n",
      "Epoch: 5 \t\t\t Iteration 1370 Loss Train: 0.43643784523010254\n",
      "Epoch: 5 \t\t\t Iteration 1371 Loss Train: 0.37983161211013794\n",
      "Epoch: 5 \t\t\t Iteration 1372 Loss Train: 0.41195112466812134\n",
      "Epoch: 5 \t\t\t Iteration 1373 Loss Train: 0.382148802280426\n",
      "Epoch: 5 \t\t\t Iteration 1374 Loss Train: 0.2899838089942932\n",
      "Epoch: 5 \t\t\t Iteration 1375 Loss Train: 0.24523961544036865\n",
      "Epoch: 5 \t\t\t Iteration 1376 Loss Train: 0.718928337097168\n",
      "Epoch: 5 \t\t\t Iteration 1377 Loss Train: 0.3689256012439728\n",
      "Epoch: 5 \t\t\t Iteration 1378 Loss Train: 0.3614703416824341\n",
      "Epoch: 5 \t\t\t Iteration 1379 Loss Train: 0.28901147842407227\n",
      "Epoch: 5 \t\t\t Iteration 1380 Loss Train: 0.3287178874015808\n",
      "Epoch: 5 \t\t\t Iteration 1381 Loss Train: 0.4589337706565857\n",
      "Epoch: 5 \t\t\t Iteration 1382 Loss Train: 0.25569233298301697\n",
      "Epoch: 5 \t\t\t Iteration 1383 Loss Train: 0.6564992666244507\n",
      "Epoch: 5 \t\t\t Iteration 1384 Loss Train: 0.3045909106731415\n",
      "Epoch: 5 \t\t\t Iteration 1385 Loss Train: 0.41304636001586914\n",
      "Epoch: 5 \t\t\t Iteration 1386 Loss Train: 0.4272744655609131\n",
      "Epoch: 5 \t\t\t Iteration 1387 Loss Train: 0.6067979335784912\n",
      "Epoch: 5 \t\t\t Iteration 1388 Loss Train: 0.37793147563934326\n",
      "Epoch: 5 \t\t\t Iteration 1389 Loss Train: 0.35548627376556396\n",
      "Epoch: 5 \t\t\t Iteration 1390 Loss Train: 0.29365217685699463\n",
      "Epoch: 5 \t\t\t Iteration 1391 Loss Train: 0.4106575548648834\n",
      "Epoch: 5 \t\t\t Iteration 1392 Loss Train: 0.3434522747993469\n",
      "Epoch: 5 \t\t\t Iteration 1393 Loss Train: 0.3305944800376892\n",
      "Epoch: 5 \t\t\t Iteration 1394 Loss Train: 0.2891431748867035\n",
      "Epoch: 5 \t\t\t Iteration 1395 Loss Train: 0.7044979929924011\n",
      "Epoch: 5 / 5 \t\t\t Training Loss:0.4160074582450279\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.8)\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    for data,label in enumerate(train_loader):\n",
    "        txt,y = label\n",
    "#         print(txt.shape)\n",
    "#         print(y.unsqueeze(-1).size())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        output = model(txt.float().to(device))\n",
    "#         print(output.size())\n",
    "#         break\n",
    "        loss = criterion(output.squeeze(-1).float().to(device), y.float().to(device)) #pass the output tensor to the criterion function\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() \n",
    "        print(f'Epoch: {i+1} \\t\\t\\t Iteration {data+1} Loss Train: {loss.item()}')\n",
    "    print(f'Epoch: {i+1} / {epochs} \\t\\t\\t Training Loss:{train_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "w7ZsYhWTDTSL",
   "metadata": {
    "id": "w7ZsYhWTDTSL"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"trainedAtepochs.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c75d3",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "_2zdJ9hy9x2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2zdJ9hy9x2S",
    "outputId": "111e3ab7-f795-49ba-b4a2-e2a859c2ac9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss:0.8649061182166922\n",
      "Correct Predictions: 9284/11158\n",
      "Accuracy: 83.20487542570353 %\n",
      "F1 Score:  0.2506607316733899\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "correct, total = 0,0\n",
    "preds=[]\n",
    "targets=[]\n",
    "model.load_state_dict(torch.load(\"trainedAtepochs.pth\"))\n",
    "for data,label in enumerate(test_loader):\n",
    "    txt,y = label\n",
    "    output = model(txt.float().to(device))\n",
    "    output=torch.sigmoid(output)\n",
    "    for o,l in zip(torch.argmax(output,axis = 1),y.float()):\n",
    "        o=o.round()\n",
    "#         print(o)\n",
    "#         print(l)\n",
    "        preds.append(int(o))\n",
    "        targets.append(int(l))\n",
    "        if o == l:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    output = output.squeeze(-1).float() \n",
    "    loss = criterion(output,y.float().to(device))\n",
    "    test_loss += loss.item()\n",
    "\n",
    "print(f'Testing Loss:{test_loss/len(test_loader)}')\n",
    "print(f'Correct Predictions: {correct}/{total}')\n",
    "print(f'Accuracy:', correct/total*100,'%')\n",
    "print('F1 Score: ',computeF1Score(preds,targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy9xNep-9yGH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xy9xNep-9yGH",
    "outputId": "30db8cb2-491e-4d4f-9dff-30b62a87aa6d"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"486pt\" height=\"809pt\"\n",
       " viewBox=\"0.00 0.00 486.00 809.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 805)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-805 482,-805 482,4 -4,4\"/>\n",
       "<!-- 139955452516752 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139955452516752</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"274,-31 209,-31 209,0 274,0 274,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (32, 1)</text>\n",
       "</g>\n",
       "<!-- 139955446477584 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139955446477584</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"292,-86 191,-86 191,-67 292,-67 292,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446477584&#45;&gt;139955452516752 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>139955446477584&#45;&gt;139955452516752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.5,-66.79C241.5,-60.07 241.5,-50.4 241.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"245,-41.19 241.5,-31.19 238,-41.19 245,-41.19\"/>\n",
       "</g>\n",
       "<!-- 139955446477536 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139955446477536</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"176,-141 75,-141 75,-122 176,-122 176,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446477536&#45;&gt;139955446477584 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139955446477536&#45;&gt;139955446477584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.14,-121.98C162.8,-113.46 191.75,-100.23 213.24,-90.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.88,-93.51 222.52,-86.17 211.97,-87.14 214.88,-93.51\"/>\n",
       "</g>\n",
       "<!-- 139955447979184 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139955447979184</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"170,-207 81,-207 81,-177 170,-177 170,-207\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-195\" font-family=\"monospace\" font-size=\"10.00\">linear5.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139955447979184&#45;&gt;139955446477536 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139955447979184&#45;&gt;139955446477536</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.5,-176.84C125.5,-169.21 125.5,-159.7 125.5,-151.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129,-151.27 125.5,-141.27 122,-151.27 129,-151.27\"/>\n",
       "</g>\n",
       "<!-- 139955446479264 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139955446479264</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"289,-141 194,-141 194,-122 289,-122 289,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"241.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446479264&#45;&gt;139955446477584 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139955446479264&#45;&gt;139955446477584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M241.5,-121.75C241.5,-114.8 241.5,-104.85 241.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"245,-96.09 241.5,-86.09 238,-96.09 245,-96.09\"/>\n",
       "</g>\n",
       "<!-- 139955446477104 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139955446477104</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"290,-201.5 189,-201.5 189,-182.5 290,-182.5 290,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"239.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446477104&#45;&gt;139955446479264 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139955446477104&#45;&gt;139955446479264</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.8,-182.37C240.07,-174.25 240.5,-161.81 240.85,-151.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.36,-151.28 241.2,-141.17 237.36,-151.04 244.36,-151.28\"/>\n",
       "</g>\n",
       "<!-- 139955446474080 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139955446474080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-267.5 50,-267.5 50,-248.5 151,-248.5 151,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446474080&#45;&gt;139955446477104 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139955446474080&#45;&gt;139955446477104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M119.25,-248.37C142.98,-237.44 184.07,-218.52 211.52,-205.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"212.99,-209.06 220.61,-201.7 210.06,-202.7 212.99,-209.06\"/>\n",
       "</g>\n",
       "<!-- 139955447978864 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139955447978864</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"145,-339 56,-339 56,-309 145,-309 145,-339\"/>\n",
       "<text text-anchor=\"middle\" x=\"100.5\" y=\"-327\" font-family=\"monospace\" font-size=\"10.00\">linear4.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"100.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\"> (20)</text>\n",
       "</g>\n",
       "<!-- 139955447978864&#45;&gt;139955446474080 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139955447978864&#45;&gt;139955446474080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.5,-308.8C100.5,-299.7 100.5,-287.79 100.5,-277.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"104,-277.84 100.5,-267.84 97,-277.84 104,-277.84\"/>\n",
       "</g>\n",
       "<!-- 139955446474848 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139955446474848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"264,-267.5 169,-267.5 169,-248.5 264,-248.5 264,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"216.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446474848&#45;&gt;139955446477104 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139955446474848&#45;&gt;139955446477104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219.6,-248.37C222.98,-238.97 228.48,-223.67 232.84,-211.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.21,-212.5 236.3,-201.91 229.62,-210.13 236.21,-212.5\"/>\n",
       "</g>\n",
       "<!-- 139955446480512 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139955446480512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"265,-333.5 164,-333.5 164,-314.5 265,-314.5 265,-333.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"214.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446480512&#45;&gt;139955446474848 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139955446480512&#45;&gt;139955446474848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.77,-314.37C215.06,-305.16 215.52,-290.29 215.9,-278.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"219.41,-278.01 216.22,-267.91 212.41,-277.79 219.41,-278.01\"/>\n",
       "</g>\n",
       "<!-- 139955446474416 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139955446474416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"126,-399.5 25,-399.5 25,-380.5 126,-380.5 126,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446474416&#45;&gt;139955446480512 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139955446474416&#45;&gt;139955446480512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.25,-380.37C117.98,-369.44 159.07,-350.52 186.52,-337.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.99,-341.06 195.61,-333.7 185.06,-334.7 187.99,-341.06\"/>\n",
       "</g>\n",
       "<!-- 139955454603200 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>139955454603200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"120,-471 31,-471 31,-441 120,-441 120,-471\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\">linear3.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"75.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\"> (50)</text>\n",
       "</g>\n",
       "<!-- 139955454603200&#45;&gt;139955446474416 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139955454603200&#45;&gt;139955446474416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M75.5,-440.8C75.5,-431.7 75.5,-419.79 75.5,-409.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79,-409.84 75.5,-399.84 72,-409.84 79,-409.84\"/>\n",
       "</g>\n",
       "<!-- 139955446477968 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>139955446477968</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"239,-399.5 144,-399.5 144,-380.5 239,-380.5 239,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446477968&#45;&gt;139955446480512 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139955446477968&#45;&gt;139955446480512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194.6,-380.37C197.98,-370.97 203.48,-355.67 207.84,-343.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"211.21,-344.5 211.3,-333.91 204.62,-342.13 211.21,-344.5\"/>\n",
       "</g>\n",
       "<!-- 139955446478592 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>139955446478592</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"240,-465.5 139,-465.5 139,-446.5 240,-446.5 240,-465.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446478592&#45;&gt;139955446477968 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>139955446478592&#45;&gt;139955446477968</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M189.77,-446.37C190.06,-437.16 190.52,-422.29 190.9,-410.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194.41,-410.01 191.22,-399.91 187.41,-409.79 194.41,-410.01\"/>\n",
       "</g>\n",
       "<!-- 139955446474224 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>139955446474224</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-531.5 0,-531.5 0,-512.5 101,-512.5 101,-531.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446474224&#45;&gt;139955446478592 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>139955446474224&#45;&gt;139955446478592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.25,-512.37C92.98,-501.44 134.07,-482.52 161.52,-469.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.99,-473.06 170.61,-465.7 160.06,-466.7 162.99,-473.06\"/>\n",
       "</g>\n",
       "<!-- 139955452745248 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>139955452745248</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"95,-603 6,-603 6,-573 95,-573 95,-603\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\">linear2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-580\" font-family=\"monospace\" font-size=\"10.00\"> (90)</text>\n",
       "</g>\n",
       "<!-- 139955452745248&#45;&gt;139955446474224 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>139955452745248&#45;&gt;139955446474224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-572.8C50.5,-563.7 50.5,-551.79 50.5,-541.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-541.84 50.5,-531.84 47,-541.84 54,-541.84\"/>\n",
       "</g>\n",
       "<!-- 139955446478016 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>139955446478016</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-531.5 119,-531.5 119,-512.5 214,-512.5 214,-531.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446478016&#45;&gt;139955446478592 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>139955446478016&#45;&gt;139955446478592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.6,-512.37C172.98,-502.97 178.48,-487.67 182.84,-475.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"186.21,-476.5 186.3,-465.91 179.62,-474.13 186.21,-476.5\"/>\n",
       "</g>\n",
       "<!-- 139955446474368 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>139955446474368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"215,-597.5 114,-597.5 114,-578.5 215,-578.5 215,-597.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-585.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446474368&#45;&gt;139955446478016 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>139955446474368&#45;&gt;139955446478016</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.77,-578.37C165.06,-569.16 165.52,-554.29 165.9,-542.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.41,-542.01 166.22,-531.91 162.41,-541.79 169.41,-542.01\"/>\n",
       "</g>\n",
       "<!-- 139955446474032 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>139955446474032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"111,-663.5 10,-663.5 10,-644.5 111,-644.5 111,-663.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446474032&#45;&gt;139955446474368 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>139955446474032&#45;&gt;139955446474368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74.53,-644.37C91.74,-633.78 121.16,-615.67 141.65,-603.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.68,-605.92 150.36,-597.7 140.01,-599.96 143.68,-605.92\"/>\n",
       "</g>\n",
       "<!-- 139955843549040 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>139955843549040</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"104,-735 15,-735 15,-705 104,-705 104,-735\"/>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-723\" font-family=\"monospace\" font-size=\"10.00\">linear1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"59.5\" y=\"-712\" font-family=\"monospace\" font-size=\"10.00\"> (250)</text>\n",
       "</g>\n",
       "<!-- 139955843549040&#45;&gt;139955446474032 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>139955843549040&#45;&gt;139955446474032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.72,-704.8C59.86,-695.7 60.05,-683.79 60.2,-673.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.71,-673.9 60.36,-663.84 56.71,-673.79 63.71,-673.9\"/>\n",
       "</g>\n",
       "<!-- 139955446476144 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>139955446476144</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"209,-663.5 132,-663.5 132,-644.5 209,-644.5 209,-663.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5\" y=\"-651.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446476144&#45;&gt;139955446474368 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>139955446476144&#45;&gt;139955446474368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M169.69,-644.37C168.82,-635.07 167.4,-619.98 166.27,-607.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.75,-607.53 165.33,-597.91 162.78,-608.19 169.75,-607.53\"/>\n",
       "</g>\n",
       "<!-- 139955446470000 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>139955446470000</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"223,-729.5 122,-729.5 122,-710.5 223,-710.5 223,-729.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-717.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446470000&#45;&gt;139955446476144 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>139955446470000&#45;&gt;139955446476144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.23,-710.37C171.94,-701.16 171.48,-686.29 171.1,-674.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.59,-673.79 170.78,-663.91 167.59,-674.01 174.59,-673.79\"/>\n",
       "</g>\n",
       "<!-- 139955843552320 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>139955843552320</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"223,-801 122,-801 122,-771 223,-771 223,-801\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-789\" font-family=\"monospace\" font-size=\"10.00\">linear1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-778\" font-family=\"monospace\" font-size=\"10.00\"> (250, 15)</text>\n",
       "</g>\n",
       "<!-- 139955843552320&#45;&gt;139955446470000 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>139955843552320&#45;&gt;139955446470000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.5,-770.8C172.5,-761.7 172.5,-749.79 172.5,-739.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176,-739.84 172.5,-729.84 169,-739.84 176,-739.84\"/>\n",
       "</g>\n",
       "<!-- 139955446478256 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>139955446478256</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"309,-531.5 232,-531.5 232,-512.5 309,-512.5 309,-531.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"270.5\" y=\"-519.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446478256&#45;&gt;139955446478592 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>139955446478256&#45;&gt;139955446478592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.58,-512.37C246.53,-502.06 224.49,-484.65 208.6,-472.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"210.53,-469.15 200.51,-465.7 206.19,-474.64 210.53,-469.15\"/>\n",
       "</g>\n",
       "<!-- 139955446469904 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>139955446469904</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"334,-597.5 233,-597.5 233,-578.5 334,-578.5 334,-597.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-585.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446469904&#45;&gt;139955446478256 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>139955446469904&#45;&gt;139955446478256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.75,-578.37C279.86,-569.07 276.79,-553.98 274.34,-541.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.73,-541.01 272.31,-531.91 270.87,-542.4 277.73,-541.01\"/>\n",
       "</g>\n",
       "<!-- 139956058212928 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>139956058212928</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"334,-669 233,-669 233,-639 334,-639 334,-669\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-657\" font-family=\"monospace\" font-size=\"10.00\">linear2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"283.5\" y=\"-646\" font-family=\"monospace\" font-size=\"10.00\"> (90, 250)</text>\n",
       "</g>\n",
       "<!-- 139956058212928&#45;&gt;139955446469904 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>139956058212928&#45;&gt;139955446469904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M283.5,-638.8C283.5,-629.7 283.5,-617.79 283.5,-607.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287,-607.84 283.5,-597.84 280,-607.84 287,-607.84\"/>\n",
       "</g>\n",
       "<!-- 139955446478112 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>139955446478112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"334,-399.5 257,-399.5 257,-380.5 334,-380.5 334,-399.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"295.5\" y=\"-387.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446478112&#45;&gt;139955446480512 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>139955446478112&#45;&gt;139955446480512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M284.58,-380.37C271.53,-370.06 249.49,-352.65 233.6,-340.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"235.53,-337.15 225.51,-333.7 231.19,-342.64 235.53,-337.15\"/>\n",
       "</g>\n",
       "<!-- 139955446476192 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>139955446476192</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"405,-465.5 304,-465.5 304,-446.5 405,-446.5 405,-465.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.5\" y=\"-453.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446476192&#45;&gt;139955446478112 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>139955446476192&#45;&gt;139955446478112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346.54,-446.37C337.36,-436.4 322.04,-419.79 310.58,-407.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"313.06,-404.89 303.71,-399.91 307.91,-409.63 313.06,-404.89\"/>\n",
       "</g>\n",
       "<!-- 139955451263344 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>139955451263344</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"428,-537 327,-537 327,-507 428,-507 428,-537\"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-525\" font-family=\"monospace\" font-size=\"10.00\">linear3.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\"> (50, 90)</text>\n",
       "</g>\n",
       "<!-- 139955451263344&#45;&gt;139955446476192 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>139955451263344&#45;&gt;139955446476192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.4,-506.8C369.06,-497.5 364.66,-485.27 361.06,-475.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"364.35,-474.07 357.68,-465.84 357.77,-476.44 364.35,-474.07\"/>\n",
       "</g>\n",
       "<!-- 139955446477008 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>139955446477008</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"359,-267.5 282,-267.5 282,-248.5 359,-248.5 359,-267.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"320.5\" y=\"-255.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446477008&#45;&gt;139955446477104 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>139955446477008&#45;&gt;139955446477104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M309.58,-248.37C296.53,-238.06 274.49,-220.65 258.6,-208.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.53,-205.15 250.51,-201.7 256.19,-210.64 260.53,-205.15\"/>\n",
       "</g>\n",
       "<!-- 139955446474176 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>139955446474176</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"430,-333.5 329,-333.5 329,-314.5 430,-314.5 430,-333.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"379.5\" y=\"-321.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446474176&#45;&gt;139955446477008 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>139955446474176&#45;&gt;139955446477008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M371.54,-314.37C362.36,-304.4 347.04,-287.79 335.58,-275.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"338.06,-272.89 328.71,-267.91 332.91,-277.63 338.06,-272.89\"/>\n",
       "</g>\n",
       "<!-- 139955447978784 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>139955447978784</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"453,-405 352,-405 352,-375 453,-375 453,-405\"/>\n",
       "<text text-anchor=\"middle\" x=\"402.5\" y=\"-393\" font-family=\"monospace\" font-size=\"10.00\">linear4.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"402.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\"> (20, 50)</text>\n",
       "</g>\n",
       "<!-- 139955447978784&#45;&gt;139955446474176 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>139955447978784&#45;&gt;139955446474176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M397.4,-374.8C394.06,-365.5 389.66,-353.27 386.06,-343.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"389.35,-342.07 382.68,-333.84 382.77,-344.44 389.35,-342.07\"/>\n",
       "</g>\n",
       "<!-- 139955446480176 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>139955446480176</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"413,-141 336,-141 336,-122 413,-122 413,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"374.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139955446480176&#45;&gt;139955446477584 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>139955446480176&#45;&gt;139955446477584</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M353.13,-121.98C331.35,-113.3 297.34,-99.75 272.58,-89.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"273.84,-86.62 263.26,-86.17 271.25,-93.12 273.84,-86.62\"/>\n",
       "</g>\n",
       "<!-- 139955446474128 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>139955446474128</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"455,-201.5 354,-201.5 354,-182.5 455,-182.5 455,-201.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"404.5\" y=\"-189.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139955446474128&#45;&gt;139955446480176 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>139955446474128&#45;&gt;139955446480176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M400.07,-182.37C395.73,-173.9 388.98,-160.74 383.51,-150.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"386.62,-148.47 378.94,-141.17 380.39,-151.66 386.62,-148.47\"/>\n",
       "</g>\n",
       "<!-- 139955447979104 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>139955447979104</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"478,-273 377,-273 377,-243 478,-243 478,-273\"/>\n",
       "<text text-anchor=\"middle\" x=\"427.5\" y=\"-261\" font-family=\"monospace\" font-size=\"10.00\">linear5.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"427.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (1, 20)</text>\n",
       "</g>\n",
       "<!-- 139955447979104&#45;&gt;139955446474128 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>139955447979104&#45;&gt;139955446474128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M422.4,-242.8C419.06,-233.5 414.66,-221.27 411.06,-211.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"414.35,-210.07 407.68,-201.84 407.77,-212.44 414.35,-210.07\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f49eaabf940>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "batch = next(iter(train_loader))\n",
    "out=model(batch[0].to(device))\n",
    "make_dot(out,params=dict(list(model.named_parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2f428",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note:** I have tried my best to provide accurate results in this notebook. However, these results may not be entirely accurate, and contributions or corrections are encouraged. Thank you!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
